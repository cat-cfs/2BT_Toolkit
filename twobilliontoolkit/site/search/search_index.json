{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Pages SpatialTransformer Database SpatialTransformer Datatracker SpatialTransformer Parameters SpatialTransformer Processor SpatialTransformer GeoAttachmentSeeker Logger NetworkTransfer RecordReviser RippleUnzipple Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. For full documentation visit mkdocs.org .","title":"TwoBillionToolkit Home"},{"location":"#pages","text":"SpatialTransformer Database SpatialTransformer Datatracker SpatialTransformer Parameters SpatialTransformer Processor SpatialTransformer GeoAttachmentSeeker Logger NetworkTransfer RecordReviser RippleUnzipple","title":"Pages"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. For full documentation visit mkdocs.org .","title":"Project layout"},{"location":"pages/GeoAttachmentSeeker/","text":"GeoAttachmentSeeker File: twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri January 05 08:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script identifies and processes attachment tables within the specified GDB, filtering out non-attachment tables. It then extracts attachment files from each table and exports them to a specified output directory. The attachments are exported to a directory structure organized by project IDs. Each attachment file is named with a prefix (e.g., \"ATT{attachment_id}_\") to distinguish them. Usage python path/to/geo_attachment_seeker.py gdb_path output_path find_attachments ( gdb_path , output_path ) Searches through the GDB and finds all relevant attachments. Parameters: gdb_path ( str ) \u2013 Path to input GDB. output_path ( str ) \u2013 Path to export the attachments to. Returns: dict ( dict ) \u2013 A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to. Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def find_attachments ( gdb_path : str , output_path : str ) -> dict : \"\"\" Searches through the GDB and finds all relevant attachments. Args: gdb_path (str): Path to input GDB. output_path (str): Path to export the attachments to. Returns: dict: A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to. \"\"\" # validate the gdb_path if not isinstance ( gdb_path , str ) or not gdb_path . strip (): raise ValueError ( f 'The provided gdb_path must be a non-empty string.' ) if not gdb_path . endswith ( '.gdb' ): raise ValueError ( f 'The provided gdb_path must be of type \".gdb\".' ) if not os . path . exists ( gdb_path ): raise ValueError ( f 'The provided gdb_path path does not exist.' ) # Set the arc environment arcpy . env . workspace = gdb_path # Work all attach tables attachment_dict = {} for table in arcpy . ListTables (): # Filter out non attachment tables if '__ATTACH' not in table : continue # Build the output project paths project_id = table . replace ( '__ATTACH' , '' ) output_project_path = os . path . abspath ( os . path . join ( output_path , project_id )) table_path = os . path . join ( gdb_path , table ) # Call the processing function process_attachment ( output_project_path , table_path ) # Add to the dictionary attachment_dict [ project_id ] = output_project_path # Return the attachement dictionary return attachment_dict main () The main function of the geo_attachment_seeker.py script Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def main (): # \"\"\" The main function of the geo_attachment_seeker.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = '' ) # Define command-line arguments parser . add_argument ( 'gdb_path' , help = 'Input GDB path' ) parser . add_argument ( 'output_path' , help = 'Where to export the attachments to' ) # Parse the command-line arguments args = parser . parse_args () # Access the values using the attribute notation gdb_path = args . gdb_path output_path = args . output_path # Call the function to perform the processing find_attachments ( gdb_path , output_path ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' ) process_attachment ( output_project_path , table_path ) Processes any attachments handed to it. Parameters: output_project_path ( str ) \u2013 The path where the attachments will be exported. table_path ( str ) \u2013 The path to a __ATTACH table in a GDB. Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def process_attachment ( output_project_path : str , table_path : str ) -> None : \"\"\" Processes any attachments handed to it. Args: output_project_path (str): The path where the attachments will be exported. table_path (str): The path to a __ATTACH table in a GDB. \"\"\" # Check if the directory exists, if not, create it if not os . path . exists ( output_project_path ): os . makedirs ( output_project_path ) # Extract the attachment files from each table # Credits: A Modified version of the method Andrea found at https://support.esri.com/en-us/knowledge-base/how-to-batch-export-attachments-from-a-feature-class-in-000011912 with arcpy . da . SearchCursor ( table_path , [ 'DATA' , 'ATT_NAME' , 'ATTACHMENTID' ]) as cursor : for attachment , att_name , attachment_id in cursor : filenum = f \"ATT { attachment_id } _\" filename = os . path . join ( output_project_path , filenum + att_name ) with open ( filename , 'wb' ) as file : file . write ( attachment . tobytes ())","title":"GeoAttachmentSeeker"},{"location":"pages/GeoAttachmentSeeker/#geoattachmentseeker","text":"File: twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri January 05 08:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script identifies and processes attachment tables within the specified GDB, filtering out non-attachment tables. It then extracts attachment files from each table and exports them to a specified output directory. The attachments are exported to a directory structure organized by project IDs. Each attachment file is named with a prefix (e.g., \"ATT{attachment_id}_\") to distinguish them. Usage python path/to/geo_attachment_seeker.py gdb_path output_path","title":"GeoAttachmentSeeker"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.find_attachments","text":"Searches through the GDB and finds all relevant attachments. Parameters: gdb_path ( str ) \u2013 Path to input GDB. output_path ( str ) \u2013 Path to export the attachments to. Returns: dict ( dict ) \u2013 A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to. Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def find_attachments ( gdb_path : str , output_path : str ) -> dict : \"\"\" Searches through the GDB and finds all relevant attachments. Args: gdb_path (str): Path to input GDB. output_path (str): Path to export the attachments to. Returns: dict: A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to. \"\"\" # validate the gdb_path if not isinstance ( gdb_path , str ) or not gdb_path . strip (): raise ValueError ( f 'The provided gdb_path must be a non-empty string.' ) if not gdb_path . endswith ( '.gdb' ): raise ValueError ( f 'The provided gdb_path must be of type \".gdb\".' ) if not os . path . exists ( gdb_path ): raise ValueError ( f 'The provided gdb_path path does not exist.' ) # Set the arc environment arcpy . env . workspace = gdb_path # Work all attach tables attachment_dict = {} for table in arcpy . ListTables (): # Filter out non attachment tables if '__ATTACH' not in table : continue # Build the output project paths project_id = table . replace ( '__ATTACH' , '' ) output_project_path = os . path . abspath ( os . path . join ( output_path , project_id )) table_path = os . path . join ( gdb_path , table ) # Call the processing function process_attachment ( output_project_path , table_path ) # Add to the dictionary attachment_dict [ project_id ] = output_project_path # Return the attachement dictionary return attachment_dict","title":"find_attachments"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.main","text":"The main function of the geo_attachment_seeker.py script Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def main (): # \"\"\" The main function of the geo_attachment_seeker.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = '' ) # Define command-line arguments parser . add_argument ( 'gdb_path' , help = 'Input GDB path' ) parser . add_argument ( 'output_path' , help = 'Where to export the attachments to' ) # Parse the command-line arguments args = parser . parse_args () # Access the values using the attribute notation gdb_path = args . gdb_path output_path = args . output_path # Call the function to perform the processing find_attachments ( gdb_path , output_path ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' )","title":"main"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.process_attachment","text":"Processes any attachments handed to it. Parameters: output_project_path ( str ) \u2013 The path where the attachments will be exported. table_path ( str ) \u2013 The path to a __ATTACH table in a GDB. Source code in twobilliontoolkit\\GeoAttachmentSeeker\\geo_attachment_seeker.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def process_attachment ( output_project_path : str , table_path : str ) -> None : \"\"\" Processes any attachments handed to it. Args: output_project_path (str): The path where the attachments will be exported. table_path (str): The path to a __ATTACH table in a GDB. \"\"\" # Check if the directory exists, if not, create it if not os . path . exists ( output_project_path ): os . makedirs ( output_project_path ) # Extract the attachment files from each table # Credits: A Modified version of the method Andrea found at https://support.esri.com/en-us/knowledge-base/how-to-batch-export-attachments-from-a-feature-class-in-000011912 with arcpy . da . SearchCursor ( table_path , [ 'DATA' , 'ATT_NAME' , 'ATTACHMENTID' ]) as cursor : for attachment , att_name , attachment_id in cursor : filenum = f \"ATT { attachment_id } _\" filename = os . path . join ( output_project_path , filenum + att_name ) with open ( filename , 'wb' ) as file : file . write ( attachment . tobytes ())","title":"process_attachment"},{"location":"pages/Logger/","text":"Logger File: twobilliontoolkit/Logger/logger.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Thur January 18 12:00:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The module holds the function that handles the logging for the package, the user may provide a log path so that each warning and error gets written to that, otherwise everything will be written to the terminal. Usage Not callable from the command-line generate_header ( file_path , ps_script = None ) Generate the header for the log file. Parameters: file_path ( str ) \u2013 Path to the log file. ps_script ( str , default: None ) \u2013 The path location of the script to run spatial transformer. Defaults to an empty string. Source code in twobilliontoolkit\\Logger\\logger.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def generate_header ( file_path : str , ps_script : str = None ): \"\"\" Generate the header for the log file. Args: file_path (str): Path to the log file. ps_script (str, optional): The path location of the script to run spatial transformer. Defaults to an empty string. \"\"\" try : with open ( file_path , 'a' ) as log_file : log_file . write ( \"___________________________________________________________ \\n \" ) log_file . write ( f \" { os . path . basename ( file_path ) } file header \\n \" ) log_file . write ( \"___________________________________________________________ \\n \" ) log_file . write ( f \"User: { os . getlogin () } \\n \" ) log_file . write ( \"Log Header Created: \" + datetime . now () . strftime ( \"%Y/%m/ %d %H:%M:%S\" + \" \\n \" )) log_file . write ( f \"Script Path: { ps_script } \\n \" ) log_file . write ( f \"twobilliontoolkit package version: { version ( 'twobilliontoolkit' ) } \\n \" ) log_file . write ( \"___________________________________________________________ \\n \" ) except Exception as error : print ( error ) log ( file_path = '' , type = Colors . ERROR , message = '' , suppress = False , ps_script = '' , project_id = '' ) Log messages with colored tags and timestamps. Parameters: file_path ( str , default: '' ) \u2013 Path to the log file. type ( str , default: ERROR ) \u2013 Color code for the log message type. message ( str , default: '' ) \u2013 The log message. suppress ( bool , default: False ) \u2013 Suppress warnings in the command line. ps_script ( str , default: '' ) \u2013 The path location of the script to run spatial transformer. project_id ( str , default: '' ) \u2013 A 2BT specific variable to include a project id to more easily correlate errors and fix them. Source code in twobilliontoolkit\\Logger\\logger.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def log ( file_path : str = '' , type : str = Colors . ERROR , message : str = '' , suppress : bool = False , ps_script : str = '' , project_id : str = '' ) -> None : \"\"\" Log messages with colored tags and timestamps. Args: file_path (str, optional): Path to the log file. type (str): Color code for the log message type. message (str): The log message. suppress (bool, optional): Suppress warnings in the command line. ps_script (str, optional): The path location of the script to run spatial transformer. project_id (str, optional): A 2BT specific variable to include a project id to more easily correlate errors and fix them. \"\"\" if project_id : project_id = f '- Project Spatial ID: { project_id } - ' # Set the tag and print to the console if type == Colors . INFO : tag = 'INFO' print ( f ' { type } [ { tag } ] { message }{ Colors . END } ' ) elif type == Colors . WARNING and not suppress : tag = 'WARNING' print ( f ' { type } [ { tag } ] { project_id }{ message }{ Colors . END } ' ) elif type == Colors . WARNING and suppress : tag = 'WARNING' elif type == Colors . ERROR : tag = 'ERROR' print ( f ' { type } [ { tag } ] { project_id }{ message }{ Colors . END } ' ) # If a file path is provided if file_path is not None : # Split the log files into seperate Warning and Error logs file_path = r 'C:\\LocalTwoBillionToolkit \\\\ ' + file_path [: - 4 ] + '_' + tag + '.txt' # Check if the directory exists, if not, create it directory = os . path . dirname ( file_path ) if not os . path . exists ( directory ): os . makedirs ( directory ) if not os . path . exists ( file_path ): generate_header ( file_path , ps_script ) try : # Open the file in append mode and append a log message with open ( file_path , 'a' ) as log_file : log_file . write ( f ' { datetime . now () . strftime ( \" %d /%m/%Y %H:%M:%S\" ) } [ { tag } ] { project_id }{ message } \\n\\n ' ) except PermissionError as e : print ( f \"Permission denied to write to file: { file_path } . Error: { e } \" )","title":"Logger"},{"location":"pages/Logger/#logger","text":"File: twobilliontoolkit/Logger/logger.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Thur January 18 12:00:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The module holds the function that handles the logging for the package, the user may provide a log path so that each warning and error gets written to that, otherwise everything will be written to the terminal. Usage Not callable from the command-line","title":"Logger"},{"location":"pages/Logger/#twobilliontoolkit.Logger.logger.generate_header","text":"Generate the header for the log file. Parameters: file_path ( str ) \u2013 Path to the log file. ps_script ( str , default: None ) \u2013 The path location of the script to run spatial transformer. Defaults to an empty string. Source code in twobilliontoolkit\\Logger\\logger.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def generate_header ( file_path : str , ps_script : str = None ): \"\"\" Generate the header for the log file. Args: file_path (str): Path to the log file. ps_script (str, optional): The path location of the script to run spatial transformer. Defaults to an empty string. \"\"\" try : with open ( file_path , 'a' ) as log_file : log_file . write ( \"___________________________________________________________ \\n \" ) log_file . write ( f \" { os . path . basename ( file_path ) } file header \\n \" ) log_file . write ( \"___________________________________________________________ \\n \" ) log_file . write ( f \"User: { os . getlogin () } \\n \" ) log_file . write ( \"Log Header Created: \" + datetime . now () . strftime ( \"%Y/%m/ %d %H:%M:%S\" + \" \\n \" )) log_file . write ( f \"Script Path: { ps_script } \\n \" ) log_file . write ( f \"twobilliontoolkit package version: { version ( 'twobilliontoolkit' ) } \\n \" ) log_file . write ( \"___________________________________________________________ \\n \" ) except Exception as error : print ( error )","title":"generate_header"},{"location":"pages/Logger/#twobilliontoolkit.Logger.logger.log","text":"Log messages with colored tags and timestamps. Parameters: file_path ( str , default: '' ) \u2013 Path to the log file. type ( str , default: ERROR ) \u2013 Color code for the log message type. message ( str , default: '' ) \u2013 The log message. suppress ( bool , default: False ) \u2013 Suppress warnings in the command line. ps_script ( str , default: '' ) \u2013 The path location of the script to run spatial transformer. project_id ( str , default: '' ) \u2013 A 2BT specific variable to include a project id to more easily correlate errors and fix them. Source code in twobilliontoolkit\\Logger\\logger.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def log ( file_path : str = '' , type : str = Colors . ERROR , message : str = '' , suppress : bool = False , ps_script : str = '' , project_id : str = '' ) -> None : \"\"\" Log messages with colored tags and timestamps. Args: file_path (str, optional): Path to the log file. type (str): Color code for the log message type. message (str): The log message. suppress (bool, optional): Suppress warnings in the command line. ps_script (str, optional): The path location of the script to run spatial transformer. project_id (str, optional): A 2BT specific variable to include a project id to more easily correlate errors and fix them. \"\"\" if project_id : project_id = f '- Project Spatial ID: { project_id } - ' # Set the tag and print to the console if type == Colors . INFO : tag = 'INFO' print ( f ' { type } [ { tag } ] { message }{ Colors . END } ' ) elif type == Colors . WARNING and not suppress : tag = 'WARNING' print ( f ' { type } [ { tag } ] { project_id }{ message }{ Colors . END } ' ) elif type == Colors . WARNING and suppress : tag = 'WARNING' elif type == Colors . ERROR : tag = 'ERROR' print ( f ' { type } [ { tag } ] { project_id }{ message }{ Colors . END } ' ) # If a file path is provided if file_path is not None : # Split the log files into seperate Warning and Error logs file_path = r 'C:\\LocalTwoBillionToolkit \\\\ ' + file_path [: - 4 ] + '_' + tag + '.txt' # Check if the directory exists, if not, create it directory = os . path . dirname ( file_path ) if not os . path . exists ( directory ): os . makedirs ( directory ) if not os . path . exists ( file_path ): generate_header ( file_path , ps_script ) try : # Open the file in append mode and append a log message with open ( file_path , 'a' ) as log_file : log_file . write ( f ' { datetime . now () . strftime ( \" %d /%m/%Y %H:%M:%S\" ) } [ { tag } ] { project_id }{ message } \\n\\n ' ) except PermissionError as e : print ( f \"Permission denied to write to file: { file_path } . Error: { e } \" )","title":"log"},{"location":"pages/NetworkTransfer/","text":"NetworkTransfer File: twobilliontoolkit/NetworkTransfer/network_transfer.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri March 29 02:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script will transfer all files or any files specified from a source directory to a destination. It's main purpose will be used to transfer files from the local computers to a network drive with specific focus on the two billion trees toolkit processing. Usage python path/to/network_transfer.py [--files [...list of files...]] main () The main function of the network_transfer.py script Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def main (): \"\"\" The main function of the network_transfer.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = \"Transfer files from local to network location\" ) parser . add_argument ( \"local_path\" , type = str , help = \"Path to the local directory\" ) parser . add_argument ( \"network_path\" , type = str , help = \"Path to the network directory\" ) parser . add_argument ( \"--files\" , nargs = \"*\" , help = \"Optional. All of the files to transfer to the destination.\" ) args = parser . parse_args () files = args . files or None # Transfer files _ = transfer ( args . local_path , args . network_path , files ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' ) merge_directories ( src_dir , dst_dir , log_path ) Merge source directory into destination directory. Parameters: src_dir ( str ) \u2013 Path to the source directory. dst_dir ( str ) \u2013 Path to the destination directory. log_path ( str ) \u2013 path to the log file. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def merge_directories ( src_dir , dst_dir , log_path ): \"\"\" Merge source directory into destination directory. Args: src_dir (str): Path to the source directory. dst_dir (str): Path to the destination directory. log_path (str): path to the log file. \"\"\" try : if not os . path . exists ( dst_dir ): shutil . copytree ( src_dir , dst_dir ) # Iterate over files and subdirectories in the source directory for item in os . listdir ( src_dir ): src_item = os . path . join ( src_dir , item ) dst_item = os . path . join ( dst_dir , item ) # If the item is a file, copy it to the destination directory if os . path . isfile ( src_item ): shutil . copy2 ( src_item , dst_item ) # If the item is a directory, recursively merge it with the corresponding directory in the destination elif os . path . isdir ( src_item ): merge_directories ( src_item , dst_item , log_path ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to merge the { src_dir } to { dst_dir } : { error } ' ) merge_gdbs ( src_gdb , dest_gdb , log_path = None ) Merge source Geodatabase into destination Geodatabase. Parameters: src_gdb ( str ) \u2013 Path to the source Geodatabase. dest_gdb ( str ) \u2013 Path to the destination Geodatabase. log_path ( str , default: None ) \u2013 path to the log file. Return (bool): success flag of the operation. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def merge_gdbs ( src_gdb : str , dest_gdb : str , log_path : str = None ) -> bool : \"\"\" Merge source Geodatabase into destination Geodatabase. Args: src_gdb (str): Path to the source Geodatabase. dest_gdb (str): Path to the destination Geodatabase. log_path (str): path to the log file. Return: (bool): success flag of the operation. \"\"\" # Set the workplace for the source geodatabase arcpy . env . workspace = src_gdb try : # Copy the whole GDB if it does not exist if not arcpy . Exists ( dest_gdb ): if not os . path . exists ( os . path . dirname ( dest_gdb )): os . mkdir ( os . path . dirname ( dest_gdb )) # Copy over the whole gdb arcpy . management . Copy ( src_gdb , dest_gdb ) log ( None , Colors . INFO , f 'Copy to { dest_gdb } has completed.' ) return True except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to copy the geodatabase to { dest_gdb } : { error } ' ) return False # Get a list of feature classes in the source geodatabase feature_classes = arcpy . ListFeatureClasses () for feature_class in feature_classes : try : # Skip if already exists in destination if arcpy . Exists ( os . path . join ( dest_gdb , feature_class )): continue # Copy over the specified feature arcpy . management . Copy ( os . path . join ( src_gdb , feature_class ), os . path . join ( dest_gdb , feature_class ) ) log ( None , Colors . INFO , f 'Merging to { dest_gdb } has completed.' ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to merge the geodatabase to { dest_gdb } : { error } ' ) return False return True transfer ( local_path , network_path , list_files = None , log_path = None ) Transfer files from local directory to network directory. Parameters: local_path ( str ) \u2013 Path to the local directory. network_path ( str ) \u2013 Path to the network directory. list_files ( list , default: None ) \u2013 Optional. A provided list of files to transfew instead of all. log_path ( str , default: None ) \u2013 path to the log file. Return (bool): success flag of the operation. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def transfer ( local_path : str , network_path : str , list_files : list [ str ] = None , log_path : str = None ) -> bool : \"\"\" Transfer files from local directory to network directory. Args: local_path (str): Path to the local directory. network_path (str): Path to the network directory. list_files (list): Optional. A provided list of files to transfew instead of all. log_path (str): path to the log file. Return: (bool): success flag of the operation. \"\"\" try : if list_files is None : # Get list of files and directories in the local directory items = os . listdir ( local_path ) else : items = list_files # Iterate over files in the local directory for item in items : # Build full paths for source and destination src_path = os . path . join ( local_path , item ) dest_path = os . path . join ( network_path , item ) # Skip processing files that do not exist in the source directory if not os . path . exists ( src_path ): continue # Transfer files or directories if os . path . isdir ( src_path ): # Merge Geodatabases if destination exists if item . endswith ( \".gdb\" ): success = merge_gdbs ( src_path , dest_path , log_path ) if not success : return False else : merge_directories ( src_path , dest_path , log_path ) else : if item . endswith ( \".txt\" ) and os . path . exists ( dest_path ): # Append text files if destination file exists with open ( dest_path , \"a\" ) as dest_file : with open ( src_path , \"r\" ) as src_file : shutil . copyfileobj ( src_file , dest_file ) else : shutil . copy2 ( src_path , dest_path ) # preserves metadata log ( None , Colors . INFO , f 'The transfer of files has been completed' ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while transferring files from { local_path } to { network_path } : { error } ' ) return False return True","title":"NetworkTransfer"},{"location":"pages/NetworkTransfer/#networktransfer","text":"File: twobilliontoolkit/NetworkTransfer/network_transfer.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri March 29 02:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script will transfer all files or any files specified from a source directory to a destination. It's main purpose will be used to transfer files from the local computers to a network drive with specific focus on the two billion trees toolkit processing. Usage python path/to/network_transfer.py [--files [...list of files...]]","title":"NetworkTransfer"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.main","text":"The main function of the network_transfer.py script Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def main (): \"\"\" The main function of the network_transfer.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = \"Transfer files from local to network location\" ) parser . add_argument ( \"local_path\" , type = str , help = \"Path to the local directory\" ) parser . add_argument ( \"network_path\" , type = str , help = \"Path to the network directory\" ) parser . add_argument ( \"--files\" , nargs = \"*\" , help = \"Optional. All of the files to transfer to the destination.\" ) args = parser . parse_args () files = args . files or None # Transfer files _ = transfer ( args . local_path , args . network_path , files ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' )","title":"main"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.merge_directories","text":"Merge source directory into destination directory. Parameters: src_dir ( str ) \u2013 Path to the source directory. dst_dir ( str ) \u2013 Path to the destination directory. log_path ( str ) \u2013 path to the log file. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def merge_directories ( src_dir , dst_dir , log_path ): \"\"\" Merge source directory into destination directory. Args: src_dir (str): Path to the source directory. dst_dir (str): Path to the destination directory. log_path (str): path to the log file. \"\"\" try : if not os . path . exists ( dst_dir ): shutil . copytree ( src_dir , dst_dir ) # Iterate over files and subdirectories in the source directory for item in os . listdir ( src_dir ): src_item = os . path . join ( src_dir , item ) dst_item = os . path . join ( dst_dir , item ) # If the item is a file, copy it to the destination directory if os . path . isfile ( src_item ): shutil . copy2 ( src_item , dst_item ) # If the item is a directory, recursively merge it with the corresponding directory in the destination elif os . path . isdir ( src_item ): merge_directories ( src_item , dst_item , log_path ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to merge the { src_dir } to { dst_dir } : { error } ' )","title":"merge_directories"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.merge_gdbs","text":"Merge source Geodatabase into destination Geodatabase. Parameters: src_gdb ( str ) \u2013 Path to the source Geodatabase. dest_gdb ( str ) \u2013 Path to the destination Geodatabase. log_path ( str , default: None ) \u2013 path to the log file. Return (bool): success flag of the operation. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def merge_gdbs ( src_gdb : str , dest_gdb : str , log_path : str = None ) -> bool : \"\"\" Merge source Geodatabase into destination Geodatabase. Args: src_gdb (str): Path to the source Geodatabase. dest_gdb (str): Path to the destination Geodatabase. log_path (str): path to the log file. Return: (bool): success flag of the operation. \"\"\" # Set the workplace for the source geodatabase arcpy . env . workspace = src_gdb try : # Copy the whole GDB if it does not exist if not arcpy . Exists ( dest_gdb ): if not os . path . exists ( os . path . dirname ( dest_gdb )): os . mkdir ( os . path . dirname ( dest_gdb )) # Copy over the whole gdb arcpy . management . Copy ( src_gdb , dest_gdb ) log ( None , Colors . INFO , f 'Copy to { dest_gdb } has completed.' ) return True except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to copy the geodatabase to { dest_gdb } : { error } ' ) return False # Get a list of feature classes in the source geodatabase feature_classes = arcpy . ListFeatureClasses () for feature_class in feature_classes : try : # Skip if already exists in destination if arcpy . Exists ( os . path . join ( dest_gdb , feature_class )): continue # Copy over the specified feature arcpy . management . Copy ( os . path . join ( src_gdb , feature_class ), os . path . join ( dest_gdb , feature_class ) ) log ( None , Colors . INFO , f 'Merging to { dest_gdb } has completed.' ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while trying to merge the geodatabase to { dest_gdb } : { error } ' ) return False return True","title":"merge_gdbs"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.transfer","text":"Transfer files from local directory to network directory. Parameters: local_path ( str ) \u2013 Path to the local directory. network_path ( str ) \u2013 Path to the network directory. list_files ( list , default: None ) \u2013 Optional. A provided list of files to transfew instead of all. log_path ( str , default: None ) \u2013 path to the log file. Return (bool): success flag of the operation. Source code in twobilliontoolkit\\NetworkTransfer\\network_transfer.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def transfer ( local_path : str , network_path : str , list_files : list [ str ] = None , log_path : str = None ) -> bool : \"\"\" Transfer files from local directory to network directory. Args: local_path (str): Path to the local directory. network_path (str): Path to the network directory. list_files (list): Optional. A provided list of files to transfew instead of all. log_path (str): path to the log file. Return: (bool): success flag of the operation. \"\"\" try : if list_files is None : # Get list of files and directories in the local directory items = os . listdir ( local_path ) else : items = list_files # Iterate over files in the local directory for item in items : # Build full paths for source and destination src_path = os . path . join ( local_path , item ) dest_path = os . path . join ( network_path , item ) # Skip processing files that do not exist in the source directory if not os . path . exists ( src_path ): continue # Transfer files or directories if os . path . isdir ( src_path ): # Merge Geodatabases if destination exists if item . endswith ( \".gdb\" ): success = merge_gdbs ( src_path , dest_path , log_path ) if not success : return False else : merge_directories ( src_path , dest_path , log_path ) else : if item . endswith ( \".txt\" ) and os . path . exists ( dest_path ): # Append text files if destination file exists with open ( dest_path , \"a\" ) as dest_file : with open ( src_path , \"r\" ) as src_file : shutil . copyfileobj ( src_file , dest_file ) else : shutil . copy2 ( src_path , dest_path ) # preserves metadata log ( None , Colors . INFO , f 'The transfer of files has been completed' ) except Exception as error : log ( log_path , Colors . ERROR , f 'An error has been caught while transferring files from { local_path } to { network_path } : { error } ' ) return False return True","title":"transfer"},{"location":"pages/RecordReviser/","text":"RecordReviser File: twobilliontoolkit/RecordReviser/record_reviser.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Wed January 17 10:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script will be used to revise any records that have been created in the 2BT Spatial Tools. It provides a graphical user interface (GUI) for viewing and updating records in the Data Tracker. The GUI allows users to make changes to various fields, including 'project_spatial_id', 'project_number', 'in_raw_gdb', 'absolute_path', and 'entry_type'. Additionally, the script supports the creation of duplicate records when updating 'project_number', ensuring data integrity. Changes made through the GUI can be committed to either the Data Tracker Excel file or a Postgres DB table. The script also offers functionality to update associated Raw Data GDB layers and attachment folders. Usage python path/to/record_reviser.py --gdb /path/to/geodatabase --load [datatracker/database] --save [datatracker/database] --datatracker /path/to/datatracker.xlsx --changes \"{key: {field: newvalue, field2: newvalue2...}, key2: {field: newfield}...}\" DataTableApp Bases: QWidget Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class DataTableApp ( QWidget ): def __init__ ( self , data : Datatracker2BT , gdb : str = None , filter : dict = None ) -> None : \"\"\" Initialize the DataTableApp with the provided data. Args: data (Datatracker2BT): An instance of the Datatracker2BT class. gdb (str, optional): The path to the gdb that changes will be made to if applicable. filter (dict, optional): The dictionary of filters for the display data. \"\"\" super () . __init__ () # Columns that are not editable and the key self . columns_noedit = [ 'project_spatial_id' , 'created_at' ] self . columns_to_add = [ 'project_spatial_id' , 'project_number' , 'in_raw_gdb' , 'absolute_file_path' , 'entry_type' ] self . key = 'project_spatial_id' # Store the original and current dataframes self . filter = filter self . refresh_data ( data ) self . gdb = gdb # Initialize the user interface self . init_ui () def init_ui ( self ) -> None : \"\"\" Initialize the user interface components. \"\"\" # Create the main layout self . layout = QVBoxLayout () # Create the table and populate it self . table = QTableWidget () self . populate_table () # Enable sorting for the columns self . table . setSortingEnabled ( True ) # Create a QHBoxLayout for buttons button_layout = QHBoxLayout () # Create Save and Reset buttons self . edit_button = QPushButton ( 'Save' ) self . edit_button . clicked . connect ( self . save_changes ) self . reset_button = QPushButton ( 'Reset' ) self . reset_button . clicked . connect ( self . reset_changes ) # Add buttons to the button layout button_layout . addWidget ( self . edit_button ) button_layout . addWidget ( self . reset_button ) # Add the table and button layout to the main layout self . layout . addWidget ( self . table ) self . layout . addLayout ( button_layout ) # Set the main layout for the widget self . setLayout ( self . layout ) self . setGeometry ( 100 , 100 , 800 , 600 ) self . setWindowTitle ( 'Record Reviser' ) script_dir = os . path . dirname ( os . path . abspath ( __file__ )) icon_path = os . path . join ( script_dir , 'revision.png' ) self . setWindowIcon ( QIcon ( icon_path )) # Credit: https://www.flaticon.com/free-icons/revision self . setWindowFlags ( Qt . WindowStaysOnTopHint ) self . show () def refresh_data ( self , data : Datatracker2BT ) -> None : \"\"\" Refresh the data in the application. Args: data (Datatracker2BT): The new data to be displayed. \"\"\" # Update the data, original and current dataframe self . data = data formatted_data = self . format_data ( data ) self . original_dataframe = formatted_data [ formatted_data [ 'dropped' ] != True ] conditions = [] # combined_condition = # Filter out the data if there was a filter given if self . filter is not None : for key , value in self . filter . items (): if key == \"created_at\" : date = value . date () condition = ( ( self . original_dataframe . created_at . dt . date == date ) | ( pd . isna ( self . original_dataframe . created_at )) ) else : condition = ( self . original_dataframe [ key ] == value ) conditions . append ( condition ) # Combine all conditions using & (and) operator combined_condition = conditions [ 0 ] for condition in conditions [ 1 :]: combined_condition &= condition # Always add the condition for project_spatial_id combined_condition |= self . original_dataframe . project_spatial_id . isin ( session_added_entries ) # Apply the combined condition to the DataFrame self . original_dataframe = self . original_dataframe [ combined_condition ] # Make a working copy of the original dataframe self . dataframe = self . original_dataframe . copy () def format_data ( self , data : Datatracker2BT ) -> pd . DataFrame : \"\"\" Format the raw data into a pandas DataFrame. Args: data (Datatracker2BT): Raw data to be formatted. Returns: pd.DataFrame: A formatted pandas DataFrame. \"\"\" # Convert raw data to a DataFrame and rename index column dataframe = pd . DataFrame . from_dict ( data . data_dict , orient = 'index' ) . reset_index () dataframe . rename ( columns = { 'index' : self . key }, inplace = True ) # Sort the DataFrame by the 'self.key' column alphabetically dataframe_sorted = dataframe . sort_values ( by = self . key ) return dataframe_sorted def populate_table ( self ) -> None : \"\"\" Populate the table with data from the dataframe. \"\"\" # Filter the dataframe to include only the columns_to_add dataframe_filtered = self . dataframe [ self . columns_to_add ] # Set the number of columns and rows in the table self . table . setColumnCount ( len ( dataframe_filtered . columns )) self . table . setRowCount ( len ( dataframe_filtered )) # Set headers in the table headers = [ str ( header ) for header in dataframe_filtered . columns ] self . table . setHorizontalHeaderLabels ( headers ) # Populate each cell in the table with corresponding data for i in range ( len ( dataframe_filtered . index )): for j in range ( len ( dataframe_filtered . columns )): item = QTableWidgetItem ( str ( dataframe_filtered . iloc [ i , j ])) # Set flags for non-editable columns if dataframe_filtered . columns [ j ] in self . columns_noedit : item . setFlags ( item . flags () & ~ Qt . ItemIsEditable | Qt . ItemIsSelectable ) self . table . setItem ( i , j , item ) # Resize columns to fit the content self . table . resizeColumnsToContents () # Connect the itemChanged signal to a custom slot (function) self . table . itemChanged . connect ( self . item_changed ) def item_changed ( self , item : QTableWidgetItem ) -> None : \"\"\" Handle changes in the table items. Args: item (QTableWidgetItem): The changed item in the table. \"\"\" # Get the project_spatial_id of the row changed row = item . row () project_spatial_id = self . table . item ( row , 0 ) . text () # Get the column name from the horizontal header column_name = self . table . horizontalHeaderItem ( item . column ()) . text () # Fetch the original value using project_spatial_id original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Highlight the cell if the value is different if item . text () != original_value : item . setForeground ( QColor ( 'red' )) else : item . setForeground ( QColor ( 'black' )) def save_changes ( self ) -> None : \"\"\" Save the changes made in the GUI. \"\"\" # Dictionary to store changes made in the GUI changes_dict = {} # Iterate over rows to identify changes for row in range ( len ( self . dataframe . index )): project_spatial_id = self . table . item ( row , 0 ) . text () row_changes = {} for column in range ( len ( self . dataframe . columns )): item = self . table . item ( row , column ) if item is not None : edited_value = item . text () column_name = self . table . horizontalHeaderItem ( column ) . text () original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Record changes if the value is different if edited_value != original_value : row_changes [ column_name ] = edited_value if row_changes : changes_dict [ project_spatial_id ] = row_changes # Log the changes log ( None , Colors . INFO , f 'The changes made in the GUI were: { changes_dict } ' ) # Update the original data with the changes for project_spatial_id , changes in changes_dict . items (): for column , value in changes . items (): # Explicitly convert 'value' to the appropriate data type if self . original_dataframe [ column ] . dtype == bool : if value in [ 'True' , 'False' ]: value = bool ( value ) else : log ( self . data . log_path , Colors . ERROR , f 'The value { value } must be a bool in { column } ' ) return elif self . original_dataframe [ column ] . dtype == int : value = int ( value ) elif self . original_dataframe [ column ] . dtype == float : value = float ( value ) self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column ] = value # Update the records in the original data class update_records ( self . data , changes_dict , self . gdb ) # Refresh the data being put into the table and reset the table to the current state self . refresh_data ( self . data ) self . reset_changes () def reset_changes ( self ) -> None : \"\"\" Reset the data in the table to its original state. \"\"\" # Disconnect the itemChanged signal temporarily self . table . itemChanged . disconnect ( self . item_changed ) # Clear the table self . table . clear () # Reset the dataframe to the original state self . dataframe = self . original_dataframe . copy () # Reset table sort so no confusion between dataframe and table occurs self . table . sortByColumn ( 0 , Qt . AscendingOrder ) # Repopulate the table with the original data self . populate_table () # Reconnect the itemChanged signal self . table . itemChanged . connect ( self . item_changed ) # Print a message or perform any other necessary actions log ( None , Colors . INFO , f 'GUI data has been reset to original state' ) __init__ ( data , gdb = None , filter = None ) Initialize the DataTableApp with the provided data. Parameters: data ( Datatracker2BT ) \u2013 An instance of the Datatracker2BT class. gdb ( str , default: None ) \u2013 The path to the gdb that changes will be made to if applicable. filter ( dict , default: None ) \u2013 The dictionary of filters for the display data. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , data : Datatracker2BT , gdb : str = None , filter : dict = None ) -> None : \"\"\" Initialize the DataTableApp with the provided data. Args: data (Datatracker2BT): An instance of the Datatracker2BT class. gdb (str, optional): The path to the gdb that changes will be made to if applicable. filter (dict, optional): The dictionary of filters for the display data. \"\"\" super () . __init__ () # Columns that are not editable and the key self . columns_noedit = [ 'project_spatial_id' , 'created_at' ] self . columns_to_add = [ 'project_spatial_id' , 'project_number' , 'in_raw_gdb' , 'absolute_file_path' , 'entry_type' ] self . key = 'project_spatial_id' # Store the original and current dataframes self . filter = filter self . refresh_data ( data ) self . gdb = gdb # Initialize the user interface self . init_ui () format_data ( data ) Format the raw data into a pandas DataFrame. Parameters: data ( Datatracker2BT ) \u2013 Raw data to be formatted. Returns: DataFrame \u2013 pd.DataFrame: A formatted pandas DataFrame. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def format_data ( self , data : Datatracker2BT ) -> pd . DataFrame : \"\"\" Format the raw data into a pandas DataFrame. Args: data (Datatracker2BT): Raw data to be formatted. Returns: pd.DataFrame: A formatted pandas DataFrame. \"\"\" # Convert raw data to a DataFrame and rename index column dataframe = pd . DataFrame . from_dict ( data . data_dict , orient = 'index' ) . reset_index () dataframe . rename ( columns = { 'index' : self . key }, inplace = True ) # Sort the DataFrame by the 'self.key' column alphabetically dataframe_sorted = dataframe . sort_values ( by = self . key ) return dataframe_sorted init_ui () Initialize the user interface components. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def init_ui ( self ) -> None : \"\"\" Initialize the user interface components. \"\"\" # Create the main layout self . layout = QVBoxLayout () # Create the table and populate it self . table = QTableWidget () self . populate_table () # Enable sorting for the columns self . table . setSortingEnabled ( True ) # Create a QHBoxLayout for buttons button_layout = QHBoxLayout () # Create Save and Reset buttons self . edit_button = QPushButton ( 'Save' ) self . edit_button . clicked . connect ( self . save_changes ) self . reset_button = QPushButton ( 'Reset' ) self . reset_button . clicked . connect ( self . reset_changes ) # Add buttons to the button layout button_layout . addWidget ( self . edit_button ) button_layout . addWidget ( self . reset_button ) # Add the table and button layout to the main layout self . layout . addWidget ( self . table ) self . layout . addLayout ( button_layout ) # Set the main layout for the widget self . setLayout ( self . layout ) self . setGeometry ( 100 , 100 , 800 , 600 ) self . setWindowTitle ( 'Record Reviser' ) script_dir = os . path . dirname ( os . path . abspath ( __file__ )) icon_path = os . path . join ( script_dir , 'revision.png' ) self . setWindowIcon ( QIcon ( icon_path )) # Credit: https://www.flaticon.com/free-icons/revision self . setWindowFlags ( Qt . WindowStaysOnTopHint ) self . show () item_changed ( item ) Handle changes in the table items. Parameters: item ( QTableWidgetItem ) \u2013 The changed item in the table. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def item_changed ( self , item : QTableWidgetItem ) -> None : \"\"\" Handle changes in the table items. Args: item (QTableWidgetItem): The changed item in the table. \"\"\" # Get the project_spatial_id of the row changed row = item . row () project_spatial_id = self . table . item ( row , 0 ) . text () # Get the column name from the horizontal header column_name = self . table . horizontalHeaderItem ( item . column ()) . text () # Fetch the original value using project_spatial_id original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Highlight the cell if the value is different if item . text () != original_value : item . setForeground ( QColor ( 'red' )) else : item . setForeground ( QColor ( 'black' )) populate_table () Populate the table with data from the dataframe. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def populate_table ( self ) -> None : \"\"\" Populate the table with data from the dataframe. \"\"\" # Filter the dataframe to include only the columns_to_add dataframe_filtered = self . dataframe [ self . columns_to_add ] # Set the number of columns and rows in the table self . table . setColumnCount ( len ( dataframe_filtered . columns )) self . table . setRowCount ( len ( dataframe_filtered )) # Set headers in the table headers = [ str ( header ) for header in dataframe_filtered . columns ] self . table . setHorizontalHeaderLabels ( headers ) # Populate each cell in the table with corresponding data for i in range ( len ( dataframe_filtered . index )): for j in range ( len ( dataframe_filtered . columns )): item = QTableWidgetItem ( str ( dataframe_filtered . iloc [ i , j ])) # Set flags for non-editable columns if dataframe_filtered . columns [ j ] in self . columns_noedit : item . setFlags ( item . flags () & ~ Qt . ItemIsEditable | Qt . ItemIsSelectable ) self . table . setItem ( i , j , item ) # Resize columns to fit the content self . table . resizeColumnsToContents () # Connect the itemChanged signal to a custom slot (function) self . table . itemChanged . connect ( self . item_changed ) refresh_data ( data ) Refresh the data in the application. Parameters: data ( Datatracker2BT ) \u2013 The new data to be displayed. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def refresh_data ( self , data : Datatracker2BT ) -> None : \"\"\" Refresh the data in the application. Args: data (Datatracker2BT): The new data to be displayed. \"\"\" # Update the data, original and current dataframe self . data = data formatted_data = self . format_data ( data ) self . original_dataframe = formatted_data [ formatted_data [ 'dropped' ] != True ] conditions = [] # combined_condition = # Filter out the data if there was a filter given if self . filter is not None : for key , value in self . filter . items (): if key == \"created_at\" : date = value . date () condition = ( ( self . original_dataframe . created_at . dt . date == date ) | ( pd . isna ( self . original_dataframe . created_at )) ) else : condition = ( self . original_dataframe [ key ] == value ) conditions . append ( condition ) # Combine all conditions using & (and) operator combined_condition = conditions [ 0 ] for condition in conditions [ 1 :]: combined_condition &= condition # Always add the condition for project_spatial_id combined_condition |= self . original_dataframe . project_spatial_id . isin ( session_added_entries ) # Apply the combined condition to the DataFrame self . original_dataframe = self . original_dataframe [ combined_condition ] # Make a working copy of the original dataframe self . dataframe = self . original_dataframe . copy () reset_changes () Reset the data in the table to its original state. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def reset_changes ( self ) -> None : \"\"\" Reset the data in the table to its original state. \"\"\" # Disconnect the itemChanged signal temporarily self . table . itemChanged . disconnect ( self . item_changed ) # Clear the table self . table . clear () # Reset the dataframe to the original state self . dataframe = self . original_dataframe . copy () # Reset table sort so no confusion between dataframe and table occurs self . table . sortByColumn ( 0 , Qt . AscendingOrder ) # Repopulate the table with the original data self . populate_table () # Reconnect the itemChanged signal self . table . itemChanged . connect ( self . item_changed ) # Print a message or perform any other necessary actions log ( None , Colors . INFO , f 'GUI data has been reset to original state' ) save_changes () Save the changes made in the GUI. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def save_changes ( self ) -> None : \"\"\" Save the changes made in the GUI. \"\"\" # Dictionary to store changes made in the GUI changes_dict = {} # Iterate over rows to identify changes for row in range ( len ( self . dataframe . index )): project_spatial_id = self . table . item ( row , 0 ) . text () row_changes = {} for column in range ( len ( self . dataframe . columns )): item = self . table . item ( row , column ) if item is not None : edited_value = item . text () column_name = self . table . horizontalHeaderItem ( column ) . text () original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Record changes if the value is different if edited_value != original_value : row_changes [ column_name ] = edited_value if row_changes : changes_dict [ project_spatial_id ] = row_changes # Log the changes log ( None , Colors . INFO , f 'The changes made in the GUI were: { changes_dict } ' ) # Update the original data with the changes for project_spatial_id , changes in changes_dict . items (): for column , value in changes . items (): # Explicitly convert 'value' to the appropriate data type if self . original_dataframe [ column ] . dtype == bool : if value in [ 'True' , 'False' ]: value = bool ( value ) else : log ( self . data . log_path , Colors . ERROR , f 'The value { value } must be a bool in { column } ' ) return elif self . original_dataframe [ column ] . dtype == int : value = int ( value ) elif self . original_dataframe [ column ] . dtype == float : value = float ( value ) self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column ] = value # Update the records in the original data class update_records ( self . data , changes_dict , self . gdb ) # Refresh the data being put into the table and reset the table to the current state self . refresh_data ( self . data ) self . reset_changes () call_record_reviser ( data , gdb = None , filter = None , changes = None ) Handles calling the record reviser parts so the tool can be used outside of command-line as well. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. gdb ( str , default: None ) \u2013 The geodatabase path. If provided, updates are applied to the geodatabase. filter ( dict , default: None ) \u2013 A dictionary containing filters for the data to be displayed. changes ( str , default: None ) \u2013 A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def call_record_reviser ( data : Datatracker2BT , gdb : str = None , filter : dict = None , changes : str = None ) -> None : \"\"\" Handles calling the record reviser parts so the tool can be used outside of command-line as well. Args: data (Datatracker2BT): An instance of Datatracker2BT. gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase. filter (dict, optional): A dictionary containing filters for the data to be displayed. changes (str, optional): A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit. \"\"\" if changes : try : # Parse the changes argument and update records changes_dict = ast . literal_eval ( changes ) update_records ( data , changes_dict , gdb ) except ( ValueError , SyntaxError ) as e : log ( None , Colors . INFO , f 'Error parsing changes argument: { e } ' ) else : # If no changes dict is provided, open a PyQt application for data visualization try : app = QApplication ([]) window = DataTableApp ( data , gdb , filter ) app . exec_ () except RuntimeWarning as error : log ( None , Colors . INFO , error ) create_duplicate ( data , project_spatial_id , new_project_number ) Create a duplicate entry in the data for a given project with a new project number. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. project_spatial_id ( str ) \u2013 The unique identifier of the project to duplicate. new_project_number ( str ) \u2013 The new project number for the duplicated entry. Returns: str ( str ) \u2013 The spatial identifier of the newly created duplicate entry. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def create_duplicate ( data : Datatracker2BT , project_spatial_id : str , new_project_number : str ) -> str : \"\"\" Create a duplicate entry in the data for a given project with a new project number. Args: data (Datatracker2BT): An instance of Datatracker2BT. project_spatial_id (str): The unique identifier of the project to duplicate. new_project_number (str): The new project number for the duplicated entry. Returns: str: The spatial identifier of the newly created duplicate entry. \"\"\" # Retrieve data for the project to duplicate entry_to_duplicate = data . get_data ( project_spatial_id ) # Create a new spatial identifier for the duplicated project new_project_spatial_id = data . create_project_spatial_id ( new_project_number ) # Add the duplicated entry with the new project number to the data data . add_data ( project_spatial_id = new_project_spatial_id , project_number = new_project_number , dropped = entry_to_duplicate . get ( 'dropped' ), raw_data_path = entry_to_duplicate . get ( 'raw_data_path' ), raw_gdb_path = entry_to_duplicate . get ( 'raw_gdb_path' ), absolute_file_path = entry_to_duplicate . get ( 'absolute_file_path' ), in_raw_gdb = entry_to_duplicate . get ( 'in_raw_gdb' ), contains_pdf = entry_to_duplicate . get ( 'contains_pdf' ), contains_image = entry_to_duplicate . get ( 'contains_image' ), extracted_attachments_path = entry_to_duplicate . get ( 'extracted_attachments_path' ), editor_tracking_enabled = entry_to_duplicate . get ( 'editor_tracking_enabled' ), processed = entry_to_duplicate . get ( 'processed' ), entry_type = entry_to_duplicate . get ( 'entry_type' ), ) # Add new project spatial id's to be included in the table session_added_entries . append ( new_project_spatial_id ) # Set the 'dropped' attribute to True for the original project data . set_data ( project_spatial_id , dropped = True ) return new_project_spatial_id main () The main function of the record_reviser.py script Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def main (): \"\"\" The main function of the record_reviser.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = '' ) # Define command-line arguments parser . add_argument ( '--gdb' , required = True , default = '' , help = 'The new location or where an exsiting Geodatabase is located' ) parser . add_argument ( '--load' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to load from (datatracker or database)' ) parser . add_argument ( '--save' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to save to (datatracker or database)' ) parser . add_argument ( '--datatracker' , required = False , default = None , help = 'The new location or where an exsiting data tracker is located' ) parser . add_argument ( '--changes' , required = False , default = None , help = 'The changes that you want to update, in form \"{project_spaital_id: {field: newvalue, field2:newvalue2...}, project_spatial_id: {field: newfield}...\"' ) # Parse the command-line arguments args = parser . parse_args () # Access the values using the attribute notation load_from = args . load save_to = args . save datatracker_path = args . datatracker gdb_path = args . gdb changes = args . changes # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if ( load_from == 'datatracker' or save_to == 'datatracker' ) and datatracker_path == None : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker_path must be specified.\" ) elif ( load_from == 'datatracker' or save_to == 'datatracker' ) and datatracker_path != None : if not isinstance ( datatracker_path , str ) or not datatracker_path . strip (): raise ValueError ( f 'datatracker_path: { datatracker_path } must be a non-empty string.' ) if not datatracker_path . endswith ( '.xlsx' ): raise ValueError ( f 'datatracker_path: { datatracker_path } must be of type .xlsx.' ) if not os . path . exists ( datatracker_path ): raise ValueError ( f 'datatracker_path: { datatracker_path } path does not exist.' ) # Create the logfile path log_path = gdb_path . replace ( '.gdb' , f \" { datetime . datetime . now () . strftime ( '%Y-%m- %d ' ) } .txt\" ) # Create an instance of the Datatracker2BT class data = Datatracker2BT ( datatracker_path , load_from , save_to , log_path ) # Call the handler function call_record_reviser ( data = data , gdb = gdb_path , changes = changes or None ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' ) update_records ( data , changes_dict , gdb = None ) Update records in the data based on the changes provided in the dictionary. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. changes_dict ( dict ) \u2013 A dictionary containing changes for each project. gdb ( str , default: None ) \u2013 The geodatabase path. If provided, updates are applied to the geodatabase. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def update_records ( data : Datatracker2BT , changes_dict : dict , gdb : str = None ) -> None : \"\"\" Update records in the data based on the changes provided in the dictionary. Args: data (Datatracker2BT): An instance of Datatracker2BT. changes_dict (dict): A dictionary containing changes for each project. gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase. \"\"\" for project_spatial_id , value in changes_dict . items (): # Check if the current change updated the project number new_project_number = value . get ( 'project_number' ) if new_project_number : # Check if the project number entered was valid data . database_connection . connect ( data . database_parameters ) found = data . database_connection . read ( schema = data . database_connection . schema , table = 'project_number' , condition = f \"project_number=' { new_project_number } '\" ) data . database_connection . disconnect () if not found : print ( f 'Project number { new_project_number } is not a valid project number in the database. Skipping changing this...' ) continue # Duplicate the project with the new project number old_project_spatial_id = 'proj_' + project_spatial_id project_spatial_id = create_duplicate ( data , project_spatial_id , new_project_number ) if gdb and data . get_data ( project_spatial_id ) . get ( 'in_raw_gdb' ) == True : # If geodatabase is provided, rename the corresponding entries arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ), 'proj_' + project_spatial_id ) # Rename attachments path if it exists attachments_path = str ( data . get_data ( project_spatial_id ) . get ( 'extracted_attachments_path' )) if attachments_path not in [ 'nan' , None , 'None' ]: # update the attachment path new_attachments_path = attachments_path . split ( 'proj_' )[ 0 ] + 'proj_' + project_spatial_id # Rename the path to the attachments os . rename ( attachments_path , new_attachments_path ) # Set the corresponding attachments path data . set_data ( project_spatial_id , extracted_attachments_path = new_attachments_path ) # Also need to update the linked geodatabase things arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ) + '__ATTACH' , 'proj_' + project_spatial_id + '__ATTACH' ) arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ) + '__ATTACHREL' , 'proj_' + project_spatial_id + '__ATTACHREL' ) # Update other attributes in the data data . set_data ( project_spatial_id , in_raw_gdb = value . get ( 'in_raw_gdb' ), absolute_file_path = value . get ( 'absolute_file_path' ), entry_type = value . get ( 'entry_type' ) ) # Save the updated data data . save_data ( update = True )","title":"RecordReviser"},{"location":"pages/RecordReviser/#recordreviser","text":"File: twobilliontoolkit/RecordReviser/record_reviser.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Wed January 17 10:30:00 PST 2024 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The script will be used to revise any records that have been created in the 2BT Spatial Tools. It provides a graphical user interface (GUI) for viewing and updating records in the Data Tracker. The GUI allows users to make changes to various fields, including 'project_spatial_id', 'project_number', 'in_raw_gdb', 'absolute_path', and 'entry_type'. Additionally, the script supports the creation of duplicate records when updating 'project_number', ensuring data integrity. Changes made through the GUI can be committed to either the Data Tracker Excel file or a Postgres DB table. The script also offers functionality to update associated Raw Data GDB layers and attachment folders. Usage python path/to/record_reviser.py --gdb /path/to/geodatabase --load [datatracker/database] --save [datatracker/database] --datatracker /path/to/datatracker.xlsx --changes \"{key: {field: newvalue, field2: newvalue2...}, key2: {field: newfield}...}\"","title":"RecordReviser"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp","text":"Bases: QWidget Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 class DataTableApp ( QWidget ): def __init__ ( self , data : Datatracker2BT , gdb : str = None , filter : dict = None ) -> None : \"\"\" Initialize the DataTableApp with the provided data. Args: data (Datatracker2BT): An instance of the Datatracker2BT class. gdb (str, optional): The path to the gdb that changes will be made to if applicable. filter (dict, optional): The dictionary of filters for the display data. \"\"\" super () . __init__ () # Columns that are not editable and the key self . columns_noedit = [ 'project_spatial_id' , 'created_at' ] self . columns_to_add = [ 'project_spatial_id' , 'project_number' , 'in_raw_gdb' , 'absolute_file_path' , 'entry_type' ] self . key = 'project_spatial_id' # Store the original and current dataframes self . filter = filter self . refresh_data ( data ) self . gdb = gdb # Initialize the user interface self . init_ui () def init_ui ( self ) -> None : \"\"\" Initialize the user interface components. \"\"\" # Create the main layout self . layout = QVBoxLayout () # Create the table and populate it self . table = QTableWidget () self . populate_table () # Enable sorting for the columns self . table . setSortingEnabled ( True ) # Create a QHBoxLayout for buttons button_layout = QHBoxLayout () # Create Save and Reset buttons self . edit_button = QPushButton ( 'Save' ) self . edit_button . clicked . connect ( self . save_changes ) self . reset_button = QPushButton ( 'Reset' ) self . reset_button . clicked . connect ( self . reset_changes ) # Add buttons to the button layout button_layout . addWidget ( self . edit_button ) button_layout . addWidget ( self . reset_button ) # Add the table and button layout to the main layout self . layout . addWidget ( self . table ) self . layout . addLayout ( button_layout ) # Set the main layout for the widget self . setLayout ( self . layout ) self . setGeometry ( 100 , 100 , 800 , 600 ) self . setWindowTitle ( 'Record Reviser' ) script_dir = os . path . dirname ( os . path . abspath ( __file__ )) icon_path = os . path . join ( script_dir , 'revision.png' ) self . setWindowIcon ( QIcon ( icon_path )) # Credit: https://www.flaticon.com/free-icons/revision self . setWindowFlags ( Qt . WindowStaysOnTopHint ) self . show () def refresh_data ( self , data : Datatracker2BT ) -> None : \"\"\" Refresh the data in the application. Args: data (Datatracker2BT): The new data to be displayed. \"\"\" # Update the data, original and current dataframe self . data = data formatted_data = self . format_data ( data ) self . original_dataframe = formatted_data [ formatted_data [ 'dropped' ] != True ] conditions = [] # combined_condition = # Filter out the data if there was a filter given if self . filter is not None : for key , value in self . filter . items (): if key == \"created_at\" : date = value . date () condition = ( ( self . original_dataframe . created_at . dt . date == date ) | ( pd . isna ( self . original_dataframe . created_at )) ) else : condition = ( self . original_dataframe [ key ] == value ) conditions . append ( condition ) # Combine all conditions using & (and) operator combined_condition = conditions [ 0 ] for condition in conditions [ 1 :]: combined_condition &= condition # Always add the condition for project_spatial_id combined_condition |= self . original_dataframe . project_spatial_id . isin ( session_added_entries ) # Apply the combined condition to the DataFrame self . original_dataframe = self . original_dataframe [ combined_condition ] # Make a working copy of the original dataframe self . dataframe = self . original_dataframe . copy () def format_data ( self , data : Datatracker2BT ) -> pd . DataFrame : \"\"\" Format the raw data into a pandas DataFrame. Args: data (Datatracker2BT): Raw data to be formatted. Returns: pd.DataFrame: A formatted pandas DataFrame. \"\"\" # Convert raw data to a DataFrame and rename index column dataframe = pd . DataFrame . from_dict ( data . data_dict , orient = 'index' ) . reset_index () dataframe . rename ( columns = { 'index' : self . key }, inplace = True ) # Sort the DataFrame by the 'self.key' column alphabetically dataframe_sorted = dataframe . sort_values ( by = self . key ) return dataframe_sorted def populate_table ( self ) -> None : \"\"\" Populate the table with data from the dataframe. \"\"\" # Filter the dataframe to include only the columns_to_add dataframe_filtered = self . dataframe [ self . columns_to_add ] # Set the number of columns and rows in the table self . table . setColumnCount ( len ( dataframe_filtered . columns )) self . table . setRowCount ( len ( dataframe_filtered )) # Set headers in the table headers = [ str ( header ) for header in dataframe_filtered . columns ] self . table . setHorizontalHeaderLabels ( headers ) # Populate each cell in the table with corresponding data for i in range ( len ( dataframe_filtered . index )): for j in range ( len ( dataframe_filtered . columns )): item = QTableWidgetItem ( str ( dataframe_filtered . iloc [ i , j ])) # Set flags for non-editable columns if dataframe_filtered . columns [ j ] in self . columns_noedit : item . setFlags ( item . flags () & ~ Qt . ItemIsEditable | Qt . ItemIsSelectable ) self . table . setItem ( i , j , item ) # Resize columns to fit the content self . table . resizeColumnsToContents () # Connect the itemChanged signal to a custom slot (function) self . table . itemChanged . connect ( self . item_changed ) def item_changed ( self , item : QTableWidgetItem ) -> None : \"\"\" Handle changes in the table items. Args: item (QTableWidgetItem): The changed item in the table. \"\"\" # Get the project_spatial_id of the row changed row = item . row () project_spatial_id = self . table . item ( row , 0 ) . text () # Get the column name from the horizontal header column_name = self . table . horizontalHeaderItem ( item . column ()) . text () # Fetch the original value using project_spatial_id original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Highlight the cell if the value is different if item . text () != original_value : item . setForeground ( QColor ( 'red' )) else : item . setForeground ( QColor ( 'black' )) def save_changes ( self ) -> None : \"\"\" Save the changes made in the GUI. \"\"\" # Dictionary to store changes made in the GUI changes_dict = {} # Iterate over rows to identify changes for row in range ( len ( self . dataframe . index )): project_spatial_id = self . table . item ( row , 0 ) . text () row_changes = {} for column in range ( len ( self . dataframe . columns )): item = self . table . item ( row , column ) if item is not None : edited_value = item . text () column_name = self . table . horizontalHeaderItem ( column ) . text () original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Record changes if the value is different if edited_value != original_value : row_changes [ column_name ] = edited_value if row_changes : changes_dict [ project_spatial_id ] = row_changes # Log the changes log ( None , Colors . INFO , f 'The changes made in the GUI were: { changes_dict } ' ) # Update the original data with the changes for project_spatial_id , changes in changes_dict . items (): for column , value in changes . items (): # Explicitly convert 'value' to the appropriate data type if self . original_dataframe [ column ] . dtype == bool : if value in [ 'True' , 'False' ]: value = bool ( value ) else : log ( self . data . log_path , Colors . ERROR , f 'The value { value } must be a bool in { column } ' ) return elif self . original_dataframe [ column ] . dtype == int : value = int ( value ) elif self . original_dataframe [ column ] . dtype == float : value = float ( value ) self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column ] = value # Update the records in the original data class update_records ( self . data , changes_dict , self . gdb ) # Refresh the data being put into the table and reset the table to the current state self . refresh_data ( self . data ) self . reset_changes () def reset_changes ( self ) -> None : \"\"\" Reset the data in the table to its original state. \"\"\" # Disconnect the itemChanged signal temporarily self . table . itemChanged . disconnect ( self . item_changed ) # Clear the table self . table . clear () # Reset the dataframe to the original state self . dataframe = self . original_dataframe . copy () # Reset table sort so no confusion between dataframe and table occurs self . table . sortByColumn ( 0 , Qt . AscendingOrder ) # Repopulate the table with the original data self . populate_table () # Reconnect the itemChanged signal self . table . itemChanged . connect ( self . item_changed ) # Print a message or perform any other necessary actions log ( None , Colors . INFO , f 'GUI data has been reset to original state' )","title":"DataTableApp"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.__init__","text":"Initialize the DataTableApp with the provided data. Parameters: data ( Datatracker2BT ) \u2013 An instance of the Datatracker2BT class. gdb ( str , default: None ) \u2013 The path to the gdb that changes will be made to if applicable. filter ( dict , default: None ) \u2013 The dictionary of filters for the display data. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 def __init__ ( self , data : Datatracker2BT , gdb : str = None , filter : dict = None ) -> None : \"\"\" Initialize the DataTableApp with the provided data. Args: data (Datatracker2BT): An instance of the Datatracker2BT class. gdb (str, optional): The path to the gdb that changes will be made to if applicable. filter (dict, optional): The dictionary of filters for the display data. \"\"\" super () . __init__ () # Columns that are not editable and the key self . columns_noedit = [ 'project_spatial_id' , 'created_at' ] self . columns_to_add = [ 'project_spatial_id' , 'project_number' , 'in_raw_gdb' , 'absolute_file_path' , 'entry_type' ] self . key = 'project_spatial_id' # Store the original and current dataframes self . filter = filter self . refresh_data ( data ) self . gdb = gdb # Initialize the user interface self . init_ui ()","title":"__init__"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.format_data","text":"Format the raw data into a pandas DataFrame. Parameters: data ( Datatracker2BT ) \u2013 Raw data to be formatted. Returns: DataFrame \u2013 pd.DataFrame: A formatted pandas DataFrame. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 def format_data ( self , data : Datatracker2BT ) -> pd . DataFrame : \"\"\" Format the raw data into a pandas DataFrame. Args: data (Datatracker2BT): Raw data to be formatted. Returns: pd.DataFrame: A formatted pandas DataFrame. \"\"\" # Convert raw data to a DataFrame and rename index column dataframe = pd . DataFrame . from_dict ( data . data_dict , orient = 'index' ) . reset_index () dataframe . rename ( columns = { 'index' : self . key }, inplace = True ) # Sort the DataFrame by the 'self.key' column alphabetically dataframe_sorted = dataframe . sort_values ( by = self . key ) return dataframe_sorted","title":"format_data"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.init_ui","text":"Initialize the user interface components. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 def init_ui ( self ) -> None : \"\"\" Initialize the user interface components. \"\"\" # Create the main layout self . layout = QVBoxLayout () # Create the table and populate it self . table = QTableWidget () self . populate_table () # Enable sorting for the columns self . table . setSortingEnabled ( True ) # Create a QHBoxLayout for buttons button_layout = QHBoxLayout () # Create Save and Reset buttons self . edit_button = QPushButton ( 'Save' ) self . edit_button . clicked . connect ( self . save_changes ) self . reset_button = QPushButton ( 'Reset' ) self . reset_button . clicked . connect ( self . reset_changes ) # Add buttons to the button layout button_layout . addWidget ( self . edit_button ) button_layout . addWidget ( self . reset_button ) # Add the table and button layout to the main layout self . layout . addWidget ( self . table ) self . layout . addLayout ( button_layout ) # Set the main layout for the widget self . setLayout ( self . layout ) self . setGeometry ( 100 , 100 , 800 , 600 ) self . setWindowTitle ( 'Record Reviser' ) script_dir = os . path . dirname ( os . path . abspath ( __file__ )) icon_path = os . path . join ( script_dir , 'revision.png' ) self . setWindowIcon ( QIcon ( icon_path )) # Credit: https://www.flaticon.com/free-icons/revision self . setWindowFlags ( Qt . WindowStaysOnTopHint ) self . show ()","title":"init_ui"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.item_changed","text":"Handle changes in the table items. Parameters: item ( QTableWidgetItem ) \u2013 The changed item in the table. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def item_changed ( self , item : QTableWidgetItem ) -> None : \"\"\" Handle changes in the table items. Args: item (QTableWidgetItem): The changed item in the table. \"\"\" # Get the project_spatial_id of the row changed row = item . row () project_spatial_id = self . table . item ( row , 0 ) . text () # Get the column name from the horizontal header column_name = self . table . horizontalHeaderItem ( item . column ()) . text () # Fetch the original value using project_spatial_id original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Highlight the cell if the value is different if item . text () != original_value : item . setForeground ( QColor ( 'red' )) else : item . setForeground ( QColor ( 'black' ))","title":"item_changed"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.populate_table","text":"Populate the table with data from the dataframe. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def populate_table ( self ) -> None : \"\"\" Populate the table with data from the dataframe. \"\"\" # Filter the dataframe to include only the columns_to_add dataframe_filtered = self . dataframe [ self . columns_to_add ] # Set the number of columns and rows in the table self . table . setColumnCount ( len ( dataframe_filtered . columns )) self . table . setRowCount ( len ( dataframe_filtered )) # Set headers in the table headers = [ str ( header ) for header in dataframe_filtered . columns ] self . table . setHorizontalHeaderLabels ( headers ) # Populate each cell in the table with corresponding data for i in range ( len ( dataframe_filtered . index )): for j in range ( len ( dataframe_filtered . columns )): item = QTableWidgetItem ( str ( dataframe_filtered . iloc [ i , j ])) # Set flags for non-editable columns if dataframe_filtered . columns [ j ] in self . columns_noedit : item . setFlags ( item . flags () & ~ Qt . ItemIsEditable | Qt . ItemIsSelectable ) self . table . setItem ( i , j , item ) # Resize columns to fit the content self . table . resizeColumnsToContents () # Connect the itemChanged signal to a custom slot (function) self . table . itemChanged . connect ( self . item_changed )","title":"populate_table"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.refresh_data","text":"Refresh the data in the application. Parameters: data ( Datatracker2BT ) \u2013 The new data to be displayed. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def refresh_data ( self , data : Datatracker2BT ) -> None : \"\"\" Refresh the data in the application. Args: data (Datatracker2BT): The new data to be displayed. \"\"\" # Update the data, original and current dataframe self . data = data formatted_data = self . format_data ( data ) self . original_dataframe = formatted_data [ formatted_data [ 'dropped' ] != True ] conditions = [] # combined_condition = # Filter out the data if there was a filter given if self . filter is not None : for key , value in self . filter . items (): if key == \"created_at\" : date = value . date () condition = ( ( self . original_dataframe . created_at . dt . date == date ) | ( pd . isna ( self . original_dataframe . created_at )) ) else : condition = ( self . original_dataframe [ key ] == value ) conditions . append ( condition ) # Combine all conditions using & (and) operator combined_condition = conditions [ 0 ] for condition in conditions [ 1 :]: combined_condition &= condition # Always add the condition for project_spatial_id combined_condition |= self . original_dataframe . project_spatial_id . isin ( session_added_entries ) # Apply the combined condition to the DataFrame self . original_dataframe = self . original_dataframe [ combined_condition ] # Make a working copy of the original dataframe self . dataframe = self . original_dataframe . copy ()","title":"refresh_data"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.reset_changes","text":"Reset the data in the table to its original state. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 def reset_changes ( self ) -> None : \"\"\" Reset the data in the table to its original state. \"\"\" # Disconnect the itemChanged signal temporarily self . table . itemChanged . disconnect ( self . item_changed ) # Clear the table self . table . clear () # Reset the dataframe to the original state self . dataframe = self . original_dataframe . copy () # Reset table sort so no confusion between dataframe and table occurs self . table . sortByColumn ( 0 , Qt . AscendingOrder ) # Repopulate the table with the original data self . populate_table () # Reconnect the itemChanged signal self . table . itemChanged . connect ( self . item_changed ) # Print a message or perform any other necessary actions log ( None , Colors . INFO , f 'GUI data has been reset to original state' )","title":"reset_changes"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.save_changes","text":"Save the changes made in the GUI. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 def save_changes ( self ) -> None : \"\"\" Save the changes made in the GUI. \"\"\" # Dictionary to store changes made in the GUI changes_dict = {} # Iterate over rows to identify changes for row in range ( len ( self . dataframe . index )): project_spatial_id = self . table . item ( row , 0 ) . text () row_changes = {} for column in range ( len ( self . dataframe . columns )): item = self . table . item ( row , column ) if item is not None : edited_value = item . text () column_name = self . table . horizontalHeaderItem ( column ) . text () original_value = str ( self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column_name ] . values [ 0 ]) # Record changes if the value is different if edited_value != original_value : row_changes [ column_name ] = edited_value if row_changes : changes_dict [ project_spatial_id ] = row_changes # Log the changes log ( None , Colors . INFO , f 'The changes made in the GUI were: { changes_dict } ' ) # Update the original data with the changes for project_spatial_id , changes in changes_dict . items (): for column , value in changes . items (): # Explicitly convert 'value' to the appropriate data type if self . original_dataframe [ column ] . dtype == bool : if value in [ 'True' , 'False' ]: value = bool ( value ) else : log ( self . data . log_path , Colors . ERROR , f 'The value { value } must be a bool in { column } ' ) return elif self . original_dataframe [ column ] . dtype == int : value = int ( value ) elif self . original_dataframe [ column ] . dtype == float : value = float ( value ) self . original_dataframe . loc [ self . original_dataframe [ self . key ] == project_spatial_id , column ] = value # Update the records in the original data class update_records ( self . data , changes_dict , self . gdb ) # Refresh the data being put into the table and reset the table to the current state self . refresh_data ( self . data ) self . reset_changes ()","title":"save_changes"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.call_record_reviser","text":"Handles calling the record reviser parts so the tool can be used outside of command-line as well. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. gdb ( str , default: None ) \u2013 The geodatabase path. If provided, updates are applied to the geodatabase. filter ( dict , default: None ) \u2013 A dictionary containing filters for the data to be displayed. changes ( str , default: None ) \u2013 A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 def call_record_reviser ( data : Datatracker2BT , gdb : str = None , filter : dict = None , changes : str = None ) -> None : \"\"\" Handles calling the record reviser parts so the tool can be used outside of command-line as well. Args: data (Datatracker2BT): An instance of Datatracker2BT. gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase. filter (dict, optional): A dictionary containing filters for the data to be displayed. changes (str, optional): A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit. \"\"\" if changes : try : # Parse the changes argument and update records changes_dict = ast . literal_eval ( changes ) update_records ( data , changes_dict , gdb ) except ( ValueError , SyntaxError ) as e : log ( None , Colors . INFO , f 'Error parsing changes argument: { e } ' ) else : # If no changes dict is provided, open a PyQt application for data visualization try : app = QApplication ([]) window = DataTableApp ( data , gdb , filter ) app . exec_ () except RuntimeWarning as error : log ( None , Colors . INFO , error )","title":"call_record_reviser"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.create_duplicate","text":"Create a duplicate entry in the data for a given project with a new project number. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. project_spatial_id ( str ) \u2013 The unique identifier of the project to duplicate. new_project_number ( str ) \u2013 The new project number for the duplicated entry. Returns: str ( str ) \u2013 The spatial identifier of the newly created duplicate entry. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 def create_duplicate ( data : Datatracker2BT , project_spatial_id : str , new_project_number : str ) -> str : \"\"\" Create a duplicate entry in the data for a given project with a new project number. Args: data (Datatracker2BT): An instance of Datatracker2BT. project_spatial_id (str): The unique identifier of the project to duplicate. new_project_number (str): The new project number for the duplicated entry. Returns: str: The spatial identifier of the newly created duplicate entry. \"\"\" # Retrieve data for the project to duplicate entry_to_duplicate = data . get_data ( project_spatial_id ) # Create a new spatial identifier for the duplicated project new_project_spatial_id = data . create_project_spatial_id ( new_project_number ) # Add the duplicated entry with the new project number to the data data . add_data ( project_spatial_id = new_project_spatial_id , project_number = new_project_number , dropped = entry_to_duplicate . get ( 'dropped' ), raw_data_path = entry_to_duplicate . get ( 'raw_data_path' ), raw_gdb_path = entry_to_duplicate . get ( 'raw_gdb_path' ), absolute_file_path = entry_to_duplicate . get ( 'absolute_file_path' ), in_raw_gdb = entry_to_duplicate . get ( 'in_raw_gdb' ), contains_pdf = entry_to_duplicate . get ( 'contains_pdf' ), contains_image = entry_to_duplicate . get ( 'contains_image' ), extracted_attachments_path = entry_to_duplicate . get ( 'extracted_attachments_path' ), editor_tracking_enabled = entry_to_duplicate . get ( 'editor_tracking_enabled' ), processed = entry_to_duplicate . get ( 'processed' ), entry_type = entry_to_duplicate . get ( 'entry_type' ), ) # Add new project spatial id's to be included in the table session_added_entries . append ( new_project_spatial_id ) # Set the 'dropped' attribute to True for the original project data . set_data ( project_spatial_id , dropped = True ) return new_project_spatial_id","title":"create_duplicate"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.main","text":"The main function of the record_reviser.py script Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 def main (): \"\"\" The main function of the record_reviser.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = '' ) # Define command-line arguments parser . add_argument ( '--gdb' , required = True , default = '' , help = 'The new location or where an exsiting Geodatabase is located' ) parser . add_argument ( '--load' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to load from (datatracker or database)' ) parser . add_argument ( '--save' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to save to (datatracker or database)' ) parser . add_argument ( '--datatracker' , required = False , default = None , help = 'The new location or where an exsiting data tracker is located' ) parser . add_argument ( '--changes' , required = False , default = None , help = 'The changes that you want to update, in form \"{project_spaital_id: {field: newvalue, field2:newvalue2...}, project_spatial_id: {field: newfield}...\"' ) # Parse the command-line arguments args = parser . parse_args () # Access the values using the attribute notation load_from = args . load save_to = args . save datatracker_path = args . datatracker gdb_path = args . gdb changes = args . changes # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if ( load_from == 'datatracker' or save_to == 'datatracker' ) and datatracker_path == None : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker_path must be specified.\" ) elif ( load_from == 'datatracker' or save_to == 'datatracker' ) and datatracker_path != None : if not isinstance ( datatracker_path , str ) or not datatracker_path . strip (): raise ValueError ( f 'datatracker_path: { datatracker_path } must be a non-empty string.' ) if not datatracker_path . endswith ( '.xlsx' ): raise ValueError ( f 'datatracker_path: { datatracker_path } must be of type .xlsx.' ) if not os . path . exists ( datatracker_path ): raise ValueError ( f 'datatracker_path: { datatracker_path } path does not exist.' ) # Create the logfile path log_path = gdb_path . replace ( '.gdb' , f \" { datetime . datetime . now () . strftime ( '%Y-%m- %d ' ) } .txt\" ) # Create an instance of the Datatracker2BT class data = Datatracker2BT ( datatracker_path , load_from , save_to , log_path ) # Call the handler function call_record_reviser ( data = data , gdb = gdb_path , changes = changes or None ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' )","title":"main"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.update_records","text":"Update records in the data based on the changes provided in the dictionary. Parameters: data ( Datatracker2BT ) \u2013 An instance of Datatracker2BT. changes_dict ( dict ) \u2013 A dictionary containing changes for each project. gdb ( str , default: None ) \u2013 The geodatabase path. If provided, updates are applied to the geodatabase. Source code in twobilliontoolkit\\RecordReviser\\record_reviser.py 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 def update_records ( data : Datatracker2BT , changes_dict : dict , gdb : str = None ) -> None : \"\"\" Update records in the data based on the changes provided in the dictionary. Args: data (Datatracker2BT): An instance of Datatracker2BT. changes_dict (dict): A dictionary containing changes for each project. gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase. \"\"\" for project_spatial_id , value in changes_dict . items (): # Check if the current change updated the project number new_project_number = value . get ( 'project_number' ) if new_project_number : # Check if the project number entered was valid data . database_connection . connect ( data . database_parameters ) found = data . database_connection . read ( schema = data . database_connection . schema , table = 'project_number' , condition = f \"project_number=' { new_project_number } '\" ) data . database_connection . disconnect () if not found : print ( f 'Project number { new_project_number } is not a valid project number in the database. Skipping changing this...' ) continue # Duplicate the project with the new project number old_project_spatial_id = 'proj_' + project_spatial_id project_spatial_id = create_duplicate ( data , project_spatial_id , new_project_number ) if gdb and data . get_data ( project_spatial_id ) . get ( 'in_raw_gdb' ) == True : # If geodatabase is provided, rename the corresponding entries arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ), 'proj_' + project_spatial_id ) # Rename attachments path if it exists attachments_path = str ( data . get_data ( project_spatial_id ) . get ( 'extracted_attachments_path' )) if attachments_path not in [ 'nan' , None , 'None' ]: # update the attachment path new_attachments_path = attachments_path . split ( 'proj_' )[ 0 ] + 'proj_' + project_spatial_id # Rename the path to the attachments os . rename ( attachments_path , new_attachments_path ) # Set the corresponding attachments path data . set_data ( project_spatial_id , extracted_attachments_path = new_attachments_path ) # Also need to update the linked geodatabase things arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ) + '__ATTACH' , 'proj_' + project_spatial_id + '__ATTACH' ) arcpy . management . Rename ( os . path . join ( gdb , old_project_spatial_id ) + '__ATTACHREL' , 'proj_' + project_spatial_id + '__ATTACHREL' ) # Update other attributes in the data data . set_data ( project_spatial_id , in_raw_gdb = value . get ( 'in_raw_gdb' ), absolute_file_path = value . get ( 'absolute_file_path' ), entry_type = value . get ( 'entry_type' ) ) # Save the updated data data . save_data ( update = True )","title":"update_records"},{"location":"pages/RippleUnzipple/","text":"RippleUnzipple File: twobilliontoolkit/RippleUnzipple/ripple_unzipple.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri November 10 14:00:00 PST 2023 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description Recursively unzips all compressed folders in a given directory. Usage python ripple_unzipple.py input_path output_path [log_path] main () The main function of the ripple_unzipple.py script Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def main (): \"\"\" The main function of the ripple_unzipple.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) parser = argparse . ArgumentParser ( description = 'Recursively unzip all compressed folders in a given directory.' ) parser . add_argument ( '--input' , required = True , help = 'Path to the input directory or compressed file.' ) parser . add_argument ( '--output' , required = True , help = 'Path to the output directory.' ) parser . add_argument ( '--log' , default = '' , help = 'Path to the log file.' ) args = parser . parse_args () try : ripple_unzip ( args . input , args . output , args . log ) except Exception as error : log ( args . log , Colors . ERROR , error ) exit ( 1 ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' ) recursive_unzip ( input_path , output_path , original_input_path , log_path = '' ) Recursively unzip .zip and .7z files in the input_path to the output_path. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to the output directory. original_input_path ( str ) \u2013 Path of the original input compress/directory. log_path ( str , default: '' ) \u2013 Path to the log file. Defaults to ''. Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def recursive_unzip ( input_path : str , output_path : str , original_input_path : str , log_path : str = '' ) -> None : \"\"\" Recursively unzip .zip and .7z files in the input_path to the output_path. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to the output directory. original_input_path (str): Path of the original input compress/directory. log_path (str, optional): Path to the log file. Defaults to ''. \"\"\" # Create output_path if it doesn't exist os . makedirs ( output_path , exist_ok = True ) # Iterate through the directory and unzip any compressed folders for root , dirs , files in os . walk ( input_path ): for file in files : # Get the file path of the input file_path = os . path . join ( root , file ) # Make sure that the original zip does not get touched if file_path == original_input_path : continue file_to_remove = '' if file . endswith (( \".zip\" , \".7z\" )): try : with ZipFile ( file_path , mode = 'r' ) if file . endswith ( \".zip\" ) else SevenZipFile ( file_path , mode = 'r' ) as archive_ref : # Get the path that the file will be extracted to extract_path = os . path . join ( output_path , os . path . splitext ( file_path )[ 0 ]) # unzip the file to the location archive_ref . extractall ( extract_path ) # Recursively call the function to check every file in the directory tree recursive_unzip ( extract_path , extract_path , original_input_path , log_path ) # Flag the compressed file to be removed file_to_remove = extract_path + '.zip' if file . endswith ( \".zip\" ) else extract_path + '.7z' except FileNotFoundError as error : error_message = f \"FileNotFoundError: { error . strerror } in ( { file_path } ) \\n\\n A common cause for this issue may be that the MAX_PATH_LENGTH for your machine's directory is surpassed. The compressed directory will be placed in the folder for you to extract manually. Please read the Configuration section in the README to resolve this issue.\" log ( log_path , Colors . ERROR , error_message ) continue except ( BadZipFile , Bad7zFile ) as error : error_message = f \"BadZipFile or Bad7ZFile: { error . strerror } with ( { file_path } ) \\n\\n Continuing the tool and placing file for manual extracting.\" log ( log_path , Colors . ERROR , error_message ) continue # Remove the original compressed file from the new output folder if os . path . exists ( file_to_remove ): os . remove ( file_to_remove ) ripple_unzip ( input_path , output_path , log_path = '' ) Unzip .zip and .7z files either for a directory or a compressed file. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to the output directory. log_path ( str , default: '' ) \u2013 Path to the log file. Defaults to ''. Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def ripple_unzip ( input_path : str , output_path : str , log_path : str = '' ) -> None : \"\"\" Unzip .zip and .7z files either for a directory or a compressed file. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to the output directory. log_path (str, optional): Path to the log file. Defaults to ''. \"\"\" try : # Check if the provided path exists if not os . path . exists ( input_path ): raise ValueError ( f \"ValueError: The specified path ( { input_path } ) does not exist\" ) # Handle different input extensions if os . path . isdir ( input_path ): # First copy the directory to the new location copy_tree ( input_path , output_path ) recursive_unzip ( output_path , output_path , log_path ) elif input_path . endswith (( \".zip\" , \".7z\" )): os . makedirs ( output_path , exist_ok = True ) with ZipFile ( input_path , mode = 'r' ) if input_path . endswith ( \".zip\" ) else SevenZipFile ( input_path , mode = 'r' ) as archive_ref : archive_ref . extractall ( output_path ) recursive_unzip ( output_path , output_path , input_path , log_path ) else : raise ValueError ( \"ValueError: Unsupported input type. Please provide a directory or a compressed file.\" ) except ValueError as error : log ( log_path , Colors . ERROR , error ) raise ValueError ( error ) except Exception as error : log ( log_path , Colors . ERROR , error ) raise Exception ( error )","title":"RippleUnzipple"},{"location":"pages/RippleUnzipple/#rippleunzipple","text":"File: twobilliontoolkit/RippleUnzipple/ripple_unzipple.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Fri November 10 14:00:00 PST 2023 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description Recursively unzips all compressed folders in a given directory. Usage python ripple_unzipple.py input_path output_path [log_path]","title":"RippleUnzipple"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.main","text":"The main function of the ripple_unzipple.py script Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 def main (): \"\"\" The main function of the ripple_unzipple.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , 'Tool is starting...' ) parser = argparse . ArgumentParser ( description = 'Recursively unzip all compressed folders in a given directory.' ) parser . add_argument ( '--input' , required = True , help = 'Path to the input directory or compressed file.' ) parser . add_argument ( '--output' , required = True , help = 'Path to the output directory.' ) parser . add_argument ( '--log' , default = '' , help = 'Path to the log file.' ) args = parser . parse_args () try : ripple_unzip ( args . input , args . output , args . log ) except Exception as error : log ( args . log , Colors . ERROR , error ) exit ( 1 ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , 'Tool has completed' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' )","title":"main"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.recursive_unzip","text":"Recursively unzip .zip and .7z files in the input_path to the output_path. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to the output directory. original_input_path ( str ) \u2013 Path of the original input compress/directory. log_path ( str , default: '' ) \u2013 Path to the log file. Defaults to ''. Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 def recursive_unzip ( input_path : str , output_path : str , original_input_path : str , log_path : str = '' ) -> None : \"\"\" Recursively unzip .zip and .7z files in the input_path to the output_path. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to the output directory. original_input_path (str): Path of the original input compress/directory. log_path (str, optional): Path to the log file. Defaults to ''. \"\"\" # Create output_path if it doesn't exist os . makedirs ( output_path , exist_ok = True ) # Iterate through the directory and unzip any compressed folders for root , dirs , files in os . walk ( input_path ): for file in files : # Get the file path of the input file_path = os . path . join ( root , file ) # Make sure that the original zip does not get touched if file_path == original_input_path : continue file_to_remove = '' if file . endswith (( \".zip\" , \".7z\" )): try : with ZipFile ( file_path , mode = 'r' ) if file . endswith ( \".zip\" ) else SevenZipFile ( file_path , mode = 'r' ) as archive_ref : # Get the path that the file will be extracted to extract_path = os . path . join ( output_path , os . path . splitext ( file_path )[ 0 ]) # unzip the file to the location archive_ref . extractall ( extract_path ) # Recursively call the function to check every file in the directory tree recursive_unzip ( extract_path , extract_path , original_input_path , log_path ) # Flag the compressed file to be removed file_to_remove = extract_path + '.zip' if file . endswith ( \".zip\" ) else extract_path + '.7z' except FileNotFoundError as error : error_message = f \"FileNotFoundError: { error . strerror } in ( { file_path } ) \\n\\n A common cause for this issue may be that the MAX_PATH_LENGTH for your machine's directory is surpassed. The compressed directory will be placed in the folder for you to extract manually. Please read the Configuration section in the README to resolve this issue.\" log ( log_path , Colors . ERROR , error_message ) continue except ( BadZipFile , Bad7zFile ) as error : error_message = f \"BadZipFile or Bad7ZFile: { error . strerror } with ( { file_path } ) \\n\\n Continuing the tool and placing file for manual extracting.\" log ( log_path , Colors . ERROR , error_message ) continue # Remove the original compressed file from the new output folder if os . path . exists ( file_to_remove ): os . remove ( file_to_remove )","title":"recursive_unzip"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.ripple_unzip","text":"Unzip .zip and .7z files either for a directory or a compressed file. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to the output directory. log_path ( str , default: '' ) \u2013 Path to the log file. Defaults to ''. Source code in twobilliontoolkit\\RippleUnzipple\\ripple_unzipple.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def ripple_unzip ( input_path : str , output_path : str , log_path : str = '' ) -> None : \"\"\" Unzip .zip and .7z files either for a directory or a compressed file. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to the output directory. log_path (str, optional): Path to the log file. Defaults to ''. \"\"\" try : # Check if the provided path exists if not os . path . exists ( input_path ): raise ValueError ( f \"ValueError: The specified path ( { input_path } ) does not exist\" ) # Handle different input extensions if os . path . isdir ( input_path ): # First copy the directory to the new location copy_tree ( input_path , output_path ) recursive_unzip ( output_path , output_path , log_path ) elif input_path . endswith (( \".zip\" , \".7z\" )): os . makedirs ( output_path , exist_ok = True ) with ZipFile ( input_path , mode = 'r' ) if input_path . endswith ( \".zip\" ) else SevenZipFile ( input_path , mode = 'r' ) as archive_ref : archive_ref . extractall ( output_path ) recursive_unzip ( output_path , output_path , input_path , log_path ) else : raise ValueError ( \"ValueError: Unsupported input type. Please provide a directory or a compressed file.\" ) except ValueError as error : log ( log_path , Colors . ERROR , error ) raise ValueError ( error ) except Exception as error : log ( log_path , Colors . ERROR , error ) raise Exception ( error )","title":"ripple_unzip"},{"location":"pages/SpatialTransformer/Database/","text":"SpatialTransformer.Database Database A class for interacting with a PostgreSQL database. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 class Database : \"\"\"A class for interacting with a PostgreSQL database.\"\"\" def __init__ ( self ) -> None : \"\"\"Initialize the Database instance.\"\"\" self . connection = None self . cursor = None self . schema = None self . table = None def connect ( self , params : dict [ str , str ]) -> None : \"\"\" Connect to the database. Args: params (dict): Dictionary containing database connection parameters. \"\"\" try : self . connection = psycopg2 . connect ( ** params ) self . cursor = self . connection . cursor () log ( None , Colors . INFO , 'Opened a connection to the database...' ) except Exception as error : raise Exception ( error ) def disconnect ( self ) -> None : \"\"\"Disconnect from the database.\"\"\" try : if self . connection is not None : self . connection . close () log ( None , Colors . INFO , 'Database connection closed.' ) except Exception as error : raise Exception ( error ) def get_params ( self , filename : str = None , section : str = 'postgresql' ) -> dict [ str , str ]: \"\"\" Get database connection parameters from a configuration file. Args: filename (str, optional): Path to the configuration file. section (str, optional): Section in the configuration file. Returns: dict: Dictionary containing database connection parameters. \"\"\" # Get the directory of the current script script_directory = os . path . dirname ( os . path . abspath ( __file__ )) # Join the script directory with the relative path to database.ini if filename == None : filename = os . path . join ( script_directory , 'database.ini' ) if not os . path . exists ( filename ): raise FileExistsError ( f 'The database.ini file is not in the correct place/does not exist in { script_directory } ' ) # create a parser parser = ConfigParser () # read config file parser . read ( filename ) # get section, default to postgresql db = {} if parser . has_section ( section ): params = parser . items ( section ) for param in params : if not param [ 1 ]: raise ValueError ( f 'The [ { param [ 0 ] } ] field in { filename } was not filled out.' ) if param [ 0 ] == 'schema' : self . schema = param [ 1 ] continue elif param [ 0 ] == 'table' : self . table = param [ 1 ] continue db [ param [ 0 ]] = param [ 1 ] else : raise Exception ( f 'Section { section } not found in the { filename } file' ) return db def execute ( self , query : str , values : list [ str ] = None ) -> None : \"\"\" Execute a SQL query. Args: query (str): SQL query string. values (list, optional): List of parameter values for the query. \"\"\" self . cursor . execute ( query , values ) self . connection . commit () def get_columns ( self , schema : str , table : str ) -> list [ str ]: \"\"\" Get the columns from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: list: List of strings that correspond to the table columns. \"\"\" query = f \"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = ' { schema + '.' + table } '\" self . execute ( query ) return [ row [ 0 ] for row in self . cursor . fetchall ()] def get_pkey ( self , schema : str , table : str ) -> str : \"\"\" Get the primary key from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: str: The name of the primary key in the schema/table provided. \"\"\" query = f \"SELECT * FROM information_schema.key_column_usage where table_schema = ' { schema } ' and constraint_name = CONCAT(' { table } ', '_pkey')\" self . execute ( query ) return self . cursor . fetchone () def create ( self , schema : str , table : str , columns : list [ str ], values : list [ str ]) -> None : \"\"\" Insert data into a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list): List of column names. values (list): List of values to be inserted. \"\"\" query = f \"INSERT INTO { schema + '.' + table } ( { ', ' . join ( columns ) } ) VALUES ( { ', ' . join ([ ' %s ' for _ in values ]) } )\" self . execute ( query , values ) def read ( self , schema : str , table : str , columns : list [ str ] = None , condition : str = None ) -> list [ tuple ]: \"\"\" Retrieve data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list, optional): List of column names to retrieve. Defaults to None (all columns). condition (str, optional): SQL condition to filter rows. Defaults to None. Returns: list: List of tuples containing the retrieved data. \"\"\" if columns is None : columns = [ '*' ] query = f \"SELECT { ', ' . join ( columns ) } FROM { schema + '.' + table } \" if condition is not None : query += f \" WHERE { condition } \" self . execute ( query ) return self . cursor . fetchall () def update ( self , schema : str , table : str , values_dict : dict [ str , str ], condition : str ) -> None : \"\"\" Update data in a table. Args: schema (str): Name of the database schema. table (str): Name of the table. values_dict (dict): Dictionary of column-value pairs to be updated. condition (str): SQL condition to filter rows to be updated. \"\"\" query = f \"UPDATE { schema + '.' + table } SET { ', ' . join ([ f ' { col } =%s' for col in values_dict . keys ()]) } WHERE { condition } \" values = list ( values_dict . values ()) self . execute ( query , values ) def delete ( self , schema : str , table : str , condition : str ) -> None : \"\"\" Delete data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. condition (str): SQL condition to filter rows to be deleted. \"\"\" query = f \"DELETE FROM { schema + '.' + table } WHERE { condition } \" self . execute ( query ) __init__ () Initialize the Database instance. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 17 18 19 20 21 22 def __init__ ( self ) -> None : \"\"\"Initialize the Database instance.\"\"\" self . connection = None self . cursor = None self . schema = None self . table = None connect ( params ) Connect to the database. Parameters: params ( dict ) \u2013 Dictionary containing database connection parameters. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def connect ( self , params : dict [ str , str ]) -> None : \"\"\" Connect to the database. Args: params (dict): Dictionary containing database connection parameters. \"\"\" try : self . connection = psycopg2 . connect ( ** params ) self . cursor = self . connection . cursor () log ( None , Colors . INFO , 'Opened a connection to the database...' ) except Exception as error : raise Exception ( error ) create ( schema , table , columns , values ) Insert data into a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. columns ( list ) \u2013 List of column names. values ( list ) \u2013 List of values to be inserted. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 136 137 138 139 140 141 142 143 144 145 146 147 def create ( self , schema : str , table : str , columns : list [ str ], values : list [ str ]) -> None : \"\"\" Insert data into a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list): List of column names. values (list): List of values to be inserted. \"\"\" query = f \"INSERT INTO { schema + '.' + table } ( { ', ' . join ( columns ) } ) VALUES ( { ', ' . join ([ ' %s ' for _ in values ]) } )\" self . execute ( query , values ) delete ( schema , table , condition ) Delete data from a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. condition ( str ) \u2013 SQL condition to filter rows to be deleted. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 190 191 192 193 194 195 196 197 198 199 200 def delete ( self , schema : str , table : str , condition : str ) -> None : \"\"\" Delete data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. condition (str): SQL condition to filter rows to be deleted. \"\"\" query = f \"DELETE FROM { schema + '.' + table } WHERE { condition } \" self . execute ( query ) disconnect () Disconnect from the database. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 38 39 40 41 42 43 44 45 def disconnect ( self ) -> None : \"\"\"Disconnect from the database.\"\"\" try : if self . connection is not None : self . connection . close () log ( None , Colors . INFO , 'Database connection closed.' ) except Exception as error : raise Exception ( error ) execute ( query , values = None ) Execute a SQL query. Parameters: query ( str ) \u2013 SQL query string. values ( list , default: None ) \u2013 List of parameter values for the query. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 95 96 97 98 99 100 101 102 103 104 def execute ( self , query : str , values : list [ str ] = None ) -> None : \"\"\" Execute a SQL query. Args: query (str): SQL query string. values (list, optional): List of parameter values for the query. \"\"\" self . cursor . execute ( query , values ) self . connection . commit () get_columns ( schema , table ) Get the columns from the table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. Returns: list ( list [ str ] ) \u2013 List of strings that correspond to the table columns. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_columns ( self , schema : str , table : str ) -> list [ str ]: \"\"\" Get the columns from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: list: List of strings that correspond to the table columns. \"\"\" query = f \"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = ' { schema + '.' + table } '\" self . execute ( query ) return [ row [ 0 ] for row in self . cursor . fetchall ()] get_params ( filename = None , section = 'postgresql' ) Get database connection parameters from a configuration file. Parameters: filename ( str , default: None ) \u2013 Path to the configuration file. section ( str , default: 'postgresql' ) \u2013 Section in the configuration file. Returns: dict ( dict [ str , str ] ) \u2013 Dictionary containing database connection parameters. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def get_params ( self , filename : str = None , section : str = 'postgresql' ) -> dict [ str , str ]: \"\"\" Get database connection parameters from a configuration file. Args: filename (str, optional): Path to the configuration file. section (str, optional): Section in the configuration file. Returns: dict: Dictionary containing database connection parameters. \"\"\" # Get the directory of the current script script_directory = os . path . dirname ( os . path . abspath ( __file__ )) # Join the script directory with the relative path to database.ini if filename == None : filename = os . path . join ( script_directory , 'database.ini' ) if not os . path . exists ( filename ): raise FileExistsError ( f 'The database.ini file is not in the correct place/does not exist in { script_directory } ' ) # create a parser parser = ConfigParser () # read config file parser . read ( filename ) # get section, default to postgresql db = {} if parser . has_section ( section ): params = parser . items ( section ) for param in params : if not param [ 1 ]: raise ValueError ( f 'The [ { param [ 0 ] } ] field in { filename } was not filled out.' ) if param [ 0 ] == 'schema' : self . schema = param [ 1 ] continue elif param [ 0 ] == 'table' : self . table = param [ 1 ] continue db [ param [ 0 ]] = param [ 1 ] else : raise Exception ( f 'Section { section } not found in the { filename } file' ) return db get_pkey ( schema , table ) Get the primary key from the table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. Returns: str ( str ) \u2013 The name of the primary key in the schema/table provided. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_pkey ( self , schema : str , table : str ) -> str : \"\"\" Get the primary key from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: str: The name of the primary key in the schema/table provided. \"\"\" query = f \"SELECT * FROM information_schema.key_column_usage where table_schema = ' { schema } ' and constraint_name = CONCAT(' { table } ', '_pkey')\" self . execute ( query ) return self . cursor . fetchone () read ( schema , table , columns = None , condition = None ) Retrieve data from a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. columns ( list , default: None ) \u2013 List of column names to retrieve. Defaults to None (all columns). condition ( str , default: None ) \u2013 SQL condition to filter rows. Defaults to None. Returns: list ( list [ tuple ] ) \u2013 List of tuples containing the retrieved data. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def read ( self , schema : str , table : str , columns : list [ str ] = None , condition : str = None ) -> list [ tuple ]: \"\"\" Retrieve data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list, optional): List of column names to retrieve. Defaults to None (all columns). condition (str, optional): SQL condition to filter rows. Defaults to None. Returns: list: List of tuples containing the retrieved data. \"\"\" if columns is None : columns = [ '*' ] query = f \"SELECT { ', ' . join ( columns ) } FROM { schema + '.' + table } \" if condition is not None : query += f \" WHERE { condition } \" self . execute ( query ) return self . cursor . fetchall () update ( schema , table , values_dict , condition ) Update data in a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. values_dict ( dict ) \u2013 Dictionary of column-value pairs to be updated. condition ( str ) \u2013 SQL condition to filter rows to be updated. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def update ( self , schema : str , table : str , values_dict : dict [ str , str ], condition : str ) -> None : \"\"\" Update data in a table. Args: schema (str): Name of the database schema. table (str): Name of the table. values_dict (dict): Dictionary of column-value pairs to be updated. condition (str): SQL condition to filter rows to be updated. \"\"\" query = f \"UPDATE { schema + '.' + table } SET { ', ' . join ([ f ' { col } =%s' for col in values_dict . keys ()]) } WHERE { condition } \" values = list ( values_dict . values ()) self . execute ( query , values )","title":"Database"},{"location":"pages/SpatialTransformer/Database/#spatialtransformerdatabase","text":"","title":"SpatialTransformer.Database"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database","text":"A class for interacting with a PostgreSQL database. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 class Database : \"\"\"A class for interacting with a PostgreSQL database.\"\"\" def __init__ ( self ) -> None : \"\"\"Initialize the Database instance.\"\"\" self . connection = None self . cursor = None self . schema = None self . table = None def connect ( self , params : dict [ str , str ]) -> None : \"\"\" Connect to the database. Args: params (dict): Dictionary containing database connection parameters. \"\"\" try : self . connection = psycopg2 . connect ( ** params ) self . cursor = self . connection . cursor () log ( None , Colors . INFO , 'Opened a connection to the database...' ) except Exception as error : raise Exception ( error ) def disconnect ( self ) -> None : \"\"\"Disconnect from the database.\"\"\" try : if self . connection is not None : self . connection . close () log ( None , Colors . INFO , 'Database connection closed.' ) except Exception as error : raise Exception ( error ) def get_params ( self , filename : str = None , section : str = 'postgresql' ) -> dict [ str , str ]: \"\"\" Get database connection parameters from a configuration file. Args: filename (str, optional): Path to the configuration file. section (str, optional): Section in the configuration file. Returns: dict: Dictionary containing database connection parameters. \"\"\" # Get the directory of the current script script_directory = os . path . dirname ( os . path . abspath ( __file__ )) # Join the script directory with the relative path to database.ini if filename == None : filename = os . path . join ( script_directory , 'database.ini' ) if not os . path . exists ( filename ): raise FileExistsError ( f 'The database.ini file is not in the correct place/does not exist in { script_directory } ' ) # create a parser parser = ConfigParser () # read config file parser . read ( filename ) # get section, default to postgresql db = {} if parser . has_section ( section ): params = parser . items ( section ) for param in params : if not param [ 1 ]: raise ValueError ( f 'The [ { param [ 0 ] } ] field in { filename } was not filled out.' ) if param [ 0 ] == 'schema' : self . schema = param [ 1 ] continue elif param [ 0 ] == 'table' : self . table = param [ 1 ] continue db [ param [ 0 ]] = param [ 1 ] else : raise Exception ( f 'Section { section } not found in the { filename } file' ) return db def execute ( self , query : str , values : list [ str ] = None ) -> None : \"\"\" Execute a SQL query. Args: query (str): SQL query string. values (list, optional): List of parameter values for the query. \"\"\" self . cursor . execute ( query , values ) self . connection . commit () def get_columns ( self , schema : str , table : str ) -> list [ str ]: \"\"\" Get the columns from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: list: List of strings that correspond to the table columns. \"\"\" query = f \"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = ' { schema + '.' + table } '\" self . execute ( query ) return [ row [ 0 ] for row in self . cursor . fetchall ()] def get_pkey ( self , schema : str , table : str ) -> str : \"\"\" Get the primary key from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: str: The name of the primary key in the schema/table provided. \"\"\" query = f \"SELECT * FROM information_schema.key_column_usage where table_schema = ' { schema } ' and constraint_name = CONCAT(' { table } ', '_pkey')\" self . execute ( query ) return self . cursor . fetchone () def create ( self , schema : str , table : str , columns : list [ str ], values : list [ str ]) -> None : \"\"\" Insert data into a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list): List of column names. values (list): List of values to be inserted. \"\"\" query = f \"INSERT INTO { schema + '.' + table } ( { ', ' . join ( columns ) } ) VALUES ( { ', ' . join ([ ' %s ' for _ in values ]) } )\" self . execute ( query , values ) def read ( self , schema : str , table : str , columns : list [ str ] = None , condition : str = None ) -> list [ tuple ]: \"\"\" Retrieve data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list, optional): List of column names to retrieve. Defaults to None (all columns). condition (str, optional): SQL condition to filter rows. Defaults to None. Returns: list: List of tuples containing the retrieved data. \"\"\" if columns is None : columns = [ '*' ] query = f \"SELECT { ', ' . join ( columns ) } FROM { schema + '.' + table } \" if condition is not None : query += f \" WHERE { condition } \" self . execute ( query ) return self . cursor . fetchall () def update ( self , schema : str , table : str , values_dict : dict [ str , str ], condition : str ) -> None : \"\"\" Update data in a table. Args: schema (str): Name of the database schema. table (str): Name of the table. values_dict (dict): Dictionary of column-value pairs to be updated. condition (str): SQL condition to filter rows to be updated. \"\"\" query = f \"UPDATE { schema + '.' + table } SET { ', ' . join ([ f ' { col } =%s' for col in values_dict . keys ()]) } WHERE { condition } \" values = list ( values_dict . values ()) self . execute ( query , values ) def delete ( self , schema : str , table : str , condition : str ) -> None : \"\"\" Delete data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. condition (str): SQL condition to filter rows to be deleted. \"\"\" query = f \"DELETE FROM { schema + '.' + table } WHERE { condition } \" self . execute ( query )","title":"Database"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.__init__","text":"Initialize the Database instance. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 17 18 19 20 21 22 def __init__ ( self ) -> None : \"\"\"Initialize the Database instance.\"\"\" self . connection = None self . cursor = None self . schema = None self . table = None","title":"__init__"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.connect","text":"Connect to the database. Parameters: params ( dict ) \u2013 Dictionary containing database connection parameters. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 24 25 26 27 28 29 30 31 32 33 34 35 36 def connect ( self , params : dict [ str , str ]) -> None : \"\"\" Connect to the database. Args: params (dict): Dictionary containing database connection parameters. \"\"\" try : self . connection = psycopg2 . connect ( ** params ) self . cursor = self . connection . cursor () log ( None , Colors . INFO , 'Opened a connection to the database...' ) except Exception as error : raise Exception ( error )","title":"connect"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.create","text":"Insert data into a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. columns ( list ) \u2013 List of column names. values ( list ) \u2013 List of values to be inserted. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 136 137 138 139 140 141 142 143 144 145 146 147 def create ( self , schema : str , table : str , columns : list [ str ], values : list [ str ]) -> None : \"\"\" Insert data into a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list): List of column names. values (list): List of values to be inserted. \"\"\" query = f \"INSERT INTO { schema + '.' + table } ( { ', ' . join ( columns ) } ) VALUES ( { ', ' . join ([ ' %s ' for _ in values ]) } )\" self . execute ( query , values )","title":"create"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.delete","text":"Delete data from a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. condition ( str ) \u2013 SQL condition to filter rows to be deleted. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 190 191 192 193 194 195 196 197 198 199 200 def delete ( self , schema : str , table : str , condition : str ) -> None : \"\"\" Delete data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. condition (str): SQL condition to filter rows to be deleted. \"\"\" query = f \"DELETE FROM { schema + '.' + table } WHERE { condition } \" self . execute ( query )","title":"delete"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.disconnect","text":"Disconnect from the database. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 38 39 40 41 42 43 44 45 def disconnect ( self ) -> None : \"\"\"Disconnect from the database.\"\"\" try : if self . connection is not None : self . connection . close () log ( None , Colors . INFO , 'Database connection closed.' ) except Exception as error : raise Exception ( error )","title":"disconnect"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.execute","text":"Execute a SQL query. Parameters: query ( str ) \u2013 SQL query string. values ( list , default: None ) \u2013 List of parameter values for the query. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 95 96 97 98 99 100 101 102 103 104 def execute ( self , query : str , values : list [ str ] = None ) -> None : \"\"\" Execute a SQL query. Args: query (str): SQL query string. values (list, optional): List of parameter values for the query. \"\"\" self . cursor . execute ( query , values ) self . connection . commit ()","title":"execute"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_columns","text":"Get the columns from the table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. Returns: list ( list [ str ] ) \u2013 List of strings that correspond to the table columns. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_columns ( self , schema : str , table : str ) -> list [ str ]: \"\"\" Get the columns from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: list: List of strings that correspond to the table columns. \"\"\" query = f \"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = ' { schema + '.' + table } '\" self . execute ( query ) return [ row [ 0 ] for row in self . cursor . fetchall ()]","title":"get_columns"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_params","text":"Get database connection parameters from a configuration file. Parameters: filename ( str , default: None ) \u2013 Path to the configuration file. section ( str , default: 'postgresql' ) \u2013 Section in the configuration file. Returns: dict ( dict [ str , str ] ) \u2013 Dictionary containing database connection parameters. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 def get_params ( self , filename : str = None , section : str = 'postgresql' ) -> dict [ str , str ]: \"\"\" Get database connection parameters from a configuration file. Args: filename (str, optional): Path to the configuration file. section (str, optional): Section in the configuration file. Returns: dict: Dictionary containing database connection parameters. \"\"\" # Get the directory of the current script script_directory = os . path . dirname ( os . path . abspath ( __file__ )) # Join the script directory with the relative path to database.ini if filename == None : filename = os . path . join ( script_directory , 'database.ini' ) if not os . path . exists ( filename ): raise FileExistsError ( f 'The database.ini file is not in the correct place/does not exist in { script_directory } ' ) # create a parser parser = ConfigParser () # read config file parser . read ( filename ) # get section, default to postgresql db = {} if parser . has_section ( section ): params = parser . items ( section ) for param in params : if not param [ 1 ]: raise ValueError ( f 'The [ { param [ 0 ] } ] field in { filename } was not filled out.' ) if param [ 0 ] == 'schema' : self . schema = param [ 1 ] continue elif param [ 0 ] == 'table' : self . table = param [ 1 ] continue db [ param [ 0 ]] = param [ 1 ] else : raise Exception ( f 'Section { section } not found in the { filename } file' ) return db","title":"get_params"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_pkey","text":"Get the primary key from the table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. Returns: str ( str ) \u2013 The name of the primary key in the schema/table provided. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def get_pkey ( self , schema : str , table : str ) -> str : \"\"\" Get the primary key from the table. Args: schema (str): Name of the database schema. table (str): Name of the table. Returns: str: The name of the primary key in the schema/table provided. \"\"\" query = f \"SELECT * FROM information_schema.key_column_usage where table_schema = ' { schema } ' and constraint_name = CONCAT(' { table } ', '_pkey')\" self . execute ( query ) return self . cursor . fetchone ()","title":"get_pkey"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.read","text":"Retrieve data from a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. columns ( list , default: None ) \u2013 List of column names to retrieve. Defaults to None (all columns). condition ( str , default: None ) \u2013 SQL condition to filter rows. Defaults to None. Returns: list ( list [ tuple ] ) \u2013 List of tuples containing the retrieved data. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def read ( self , schema : str , table : str , columns : list [ str ] = None , condition : str = None ) -> list [ tuple ]: \"\"\" Retrieve data from a table. Args: schema (str): Name of the database schema. table (str): Name of the table. columns (list, optional): List of column names to retrieve. Defaults to None (all columns). condition (str, optional): SQL condition to filter rows. Defaults to None. Returns: list: List of tuples containing the retrieved data. \"\"\" if columns is None : columns = [ '*' ] query = f \"SELECT { ', ' . join ( columns ) } FROM { schema + '.' + table } \" if condition is not None : query += f \" WHERE { condition } \" self . execute ( query ) return self . cursor . fetchall ()","title":"read"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.update","text":"Update data in a table. Parameters: schema ( str ) \u2013 Name of the database schema. table ( str ) \u2013 Name of the table. values_dict ( dict ) \u2013 Dictionary of column-value pairs to be updated. condition ( str ) \u2013 SQL condition to filter rows to be updated. Source code in twobilliontoolkit\\SpatialTransformer\\Database.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def update ( self , schema : str , table : str , values_dict : dict [ str , str ], condition : str ) -> None : \"\"\" Update data in a table. Args: schema (str): Name of the database schema. table (str): Name of the table. values_dict (dict): Dictionary of column-value pairs to be updated. condition (str): SQL condition to filter rows to be updated. \"\"\" query = f \"UPDATE { schema + '.' + table } SET { ', ' . join ([ f ' { col } =%s' for col in values_dict . keys ()]) } WHERE { condition } \" values = list ( values_dict . values ()) self . execute ( query , values )","title":"update"},{"location":"pages/SpatialTransformer/Datatracker/","text":"SpatialTransformer.Datatracker Datatracker Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class Datatracker : def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Datatracker class with input parameters to store data tracker information. Args: data_traker_path (str): Path to data tracker file. load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path (str, optional): Path to log file for recording errors. \"\"\" self . data_dict = {} self . datatracker = data_traker_path self . load_from = load_from self . save_to = save_to self . log_path = log_path if load_from == 'database' or save_to == 'database' : # Create database object self . database_connection = Database () # Read connection parameters from the configuration file self . database_parameters = self . database_connection . get_params () self . database_connection . connect ( self . database_parameters ) self . database_pkey = self . database_connection . get_pkey ( self . database_connection . schema , self . database_connection . table ) self . database_connection . disconnect () self . load_data () def add_data ( self , key : str , ** kwargs ) -> None : \"\"\" Adds project data to the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Additional keyword arguments for project data. \"\"\" self . data_dict [ key ] = kwargs def set_data ( self , key : str , ** kwargs ) -> None : \"\"\" Updates project data in the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Keyword arguments for updating project data. \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( key , {}) for pkey , pvalue in kwargs . items (): if pvalue is not None : project_data [ pkey ] = pvalue self . data_dict [ key ] = project_data def get_data ( self , key : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: key (str): Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ key ] def find_matching_data ( self , ** kwargs ) -> ( str , dict ): # type: ignore \"\"\" Search for a matching entry in the data based on given parameters. Args: **kwargs (any): Keyword arguments for finding a matching key. Returns: str: A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. \"\"\" return next ( ( ( key , data ) for key , data in self . data_dict . items () if all ( data . get ( field ) == value for field , value in kwargs . items ()) ), ( None , None ) ) def count_occurances ( self , field : str , value : str ) -> int : \"\"\" Count the occurrences of a specified field in the data object. Args: field (str): Name of the parameter to count occurrences. value (str): Value of the parameter to count occurrences. Returns: int: Number of occurrences of the specified parameter. \"\"\" return sum ( 1 for data in self . data_dict . values () if data . get ( field ) == value ) def load_data ( self ) -> None : \"\"\" Load data from an existing data tracker or a database connection into class. \"\"\" if self . load_from == 'database' : self . load_from_database () else : self . load_from_file () def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = self . database_connection . get_columns ( schema = self . database_connection . schema , table = self . database_connection . table ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ self . database_pkey ] = values self . database_connection . disconnect () def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker ) for index , row in data_df . iterrows (): pkey = row . index [ 0 ] data = row . drop ( pkey ) . to_dict () self . add_data ( key = row [ pkey ], ** data ) def save_data ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to data tracker or database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" if self . save_to == 'database' : self . save_to_database ( update ) else : self . save_to_file () def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) existing_keys = set ( row [ 0 ] for row in self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] )) for key , value in self . data_dict . items (): if key in existing_keys and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { field : value [ field ] for field in value }, condition = f \" { self . database_pkey } =' { key } '\" ) elif key not in existing_keys : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] + list ( value . keys ()), values = [ key ] + list ( value . values ()) ) self . database_connection . disconnect () def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" df = pd . DataFrame ( list ( self . data_dict . values ())) df . insert ( 0 , 'key' , self . data_dict . keys ()) if not df . empty : df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' ) __init__ ( data_traker_path , load_from = 'database' , save_to = 'database' , log_path = None ) Initializes the Datatracker class with input parameters to store data tracker information. Parameters: data_traker_path ( str ) \u2013 Path to data tracker file. load_from ( str , default: 'database' ) \u2013 Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to ( str , default: 'database' ) \u2013 Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path ( str , default: None ) \u2013 Path to log file for recording errors. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Datatracker class with input parameters to store data tracker information. Args: data_traker_path (str): Path to data tracker file. load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path (str, optional): Path to log file for recording errors. \"\"\" self . data_dict = {} self . datatracker = data_traker_path self . load_from = load_from self . save_to = save_to self . log_path = log_path if load_from == 'database' or save_to == 'database' : # Create database object self . database_connection = Database () # Read connection parameters from the configuration file self . database_parameters = self . database_connection . get_params () self . database_connection . connect ( self . database_parameters ) self . database_pkey = self . database_connection . get_pkey ( self . database_connection . schema , self . database_connection . table ) self . database_connection . disconnect () self . load_data () add_data ( key , ** kwargs ) Adds project data to the data tracker. Parameters: key ( str ) \u2013 Acts as key in dictionary. **kwargs ( any , default: {} ) \u2013 Additional keyword arguments for project data. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 43 44 45 46 47 48 49 50 51 def add_data ( self , key : str , ** kwargs ) -> None : \"\"\" Adds project data to the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Additional keyword arguments for project data. \"\"\" self . data_dict [ key ] = kwargs count_occurances ( field , value ) Count the occurrences of a specified field in the data object. Parameters: field ( str ) \u2013 Name of the parameter to count occurrences. value ( str ) \u2013 Value of the parameter to count occurrences. Returns: int ( int ) \u2013 Number of occurrences of the specified parameter. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def count_occurances ( self , field : str , value : str ) -> int : \"\"\" Count the occurrences of a specified field in the data object. Args: field (str): Name of the parameter to count occurrences. value (str): Value of the parameter to count occurrences. Returns: int: Number of occurrences of the specified parameter. \"\"\" return sum ( 1 for data in self . data_dict . values () if data . get ( field ) == value ) find_matching_data ( ** kwargs ) Search for a matching entry in the data based on given parameters. Parameters: **kwargs ( any , default: {} ) \u2013 Keyword arguments for finding a matching key. Returns: str ( ( str , dict ) ) \u2013 A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def find_matching_data ( self , ** kwargs ) -> ( str , dict ): # type: ignore \"\"\" Search for a matching entry in the data based on given parameters. Args: **kwargs (any): Keyword arguments for finding a matching key. Returns: str: A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. \"\"\" return next ( ( ( key , data ) for key , data in self . data_dict . items () if all ( data . get ( field ) == value for field , value in kwargs . items ()) ), ( None , None ) ) get_data ( key ) Gets an object of values given a project spatial id. Parameters: key ( str ) \u2013 Acts as key in dictionary. Returns: dict ( dict ) \u2013 the values that correspond to the given key Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 70 71 72 73 74 75 76 77 78 79 80 def get_data ( self , key : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: key (str): Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ key ] load_data () Load data from an existing data tracker or a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 116 117 118 119 120 121 122 123 def load_data ( self ) -> None : \"\"\" Load data from an existing data tracker or a database connection into class. \"\"\" if self . load_from == 'database' : self . load_from_database () else : self . load_from_file () load_from_database () Load data from a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = self . database_connection . get_columns ( schema = self . database_connection . schema , table = self . database_connection . table ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ self . database_pkey ] = values self . database_connection . disconnect () load_from_file () Load data from a file into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker ) for index , row in data_df . iterrows (): pkey = row . index [ 0 ] data = row . drop ( pkey ) . to_dict () self . add_data ( key = row [ pkey ], ** data ) save_data ( update = False ) Save data tracker information to data tracker or database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 155 156 157 158 159 160 161 162 163 164 165 def save_data ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to data tracker or database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" if self . save_to == 'database' : self . save_to_database ( update ) else : self . save_to_file () save_to_database ( update = False ) Save data tracker information to a database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) existing_keys = set ( row [ 0 ] for row in self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] )) for key , value in self . data_dict . items (): if key in existing_keys and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { field : value [ field ] for field in value }, condition = f \" { self . database_pkey } =' { key } '\" ) elif key not in existing_keys : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] + list ( value . keys ()), values = [ key ] + list ( value . values ()) ) self . database_connection . disconnect () save_to_file () Save data tracker information to a file. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 200 201 202 203 204 205 206 207 208 209 def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" df = pd . DataFrame ( list ( self . data_dict . values ())) df . insert ( 0 , 'key' , self . data_dict . keys ()) if not df . empty : df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' ) set_data ( key , ** kwargs ) Updates project data in the data tracker. Parameters: key ( str ) \u2013 Acts as key in dictionary. **kwargs ( any , default: {} ) \u2013 Keyword arguments for updating project data. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def set_data ( self , key : str , ** kwargs ) -> None : \"\"\" Updates project data in the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Keyword arguments for updating project data. \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( key , {}) for pkey , pvalue in kwargs . items (): if pvalue is not None : project_data [ pkey ] = pvalue self . data_dict [ key ] = project_data Datatracker2BT Bases: Datatracker Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 class Datatracker2BT ( Datatracker ): def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Data class with input parameters. Used to store the data tracker information. Args: data_traker_path (str): Path to data tracker to load data if exists. load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path (str, optional): The path to the log file if you wish to keep any errors that occur. \"\"\" super () . __init__ ( data_traker_path , load_from , save_to , log_path ) def add_data ( self , project_spatial_id : str , project_number : str , dropped : bool , raw_data_path : str , raw_gdb_path : str , absolute_file_path : str , in_raw_gdb : bool , contains_pdf : bool , contains_image : bool , extracted_attachments_path : str , editor_tracking_enabled : bool , processed : bool , entry_type : str ) -> None : \"\"\" Adds project data to the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number. dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path. raw_gdb_path (str): Absolute path to the output geodatabase. absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB. contains_pdf (bool): Indicates whether data contains PDF files. contains_image (bool): Indicates whether data contains image files. extracted_attachments_path (str): The path to the extracted attachments if applicable. editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb. processed (bool): Indicates whether data has been processed yet. entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry. \"\"\" self . data_dict [ project_spatial_id ] = { 'project_number' : project_number , 'dropped' : dropped , 'raw_data_path' : raw_data_path , 'raw_gdb_path' : raw_gdb_path , 'absolute_file_path' : absolute_file_path , 'in_raw_gdb' : in_raw_gdb , 'contains_pdf' : contains_pdf , 'contains_image' : contains_image , 'extracted_attachments_path' : extracted_attachments_path , 'editor_tracking_enabled' : editor_tracking_enabled , 'processed' : processed , 'entry_type' : entry_type } def set_data ( self , project_spatial_id : str , project_number : str = None , dropped : bool = None , raw_data_path : str = None , absolute_file_path : str = None , in_raw_gdb : bool = None , contains_pdf : bool = None , contains_image : bool = None , extracted_attachments_path : str = None , editor_tracking_enabled : bool = None , processed : bool = None , entry_type : str = None ) -> None : \"\"\" Updates project data in the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number (optional). dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path (optional). absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB (optional). contains_pdf (bool): Indicates whether data contains PDF files (optional). contains_image (bool): Indicates whether data contains image files (optional). extracted_attachments_path (str): The path to the extracted attachments if applicable (optional). editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed (bool): Indicates whether data has been processed yet (optional). entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional). \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( project_spatial_id , {}) if project_number is not None : project_data [ 'project_number' ] = project_number if dropped is not None : project_data [ 'dropped' ] = dropped if raw_data_path is not None : project_data [ 'raw_data_path' ] = raw_data_path if absolute_file_path is not None : project_data [ 'absolute_file_path' ] = absolute_file_path if in_raw_gdb is not None : project_data [ 'in_raw_gdb' ] = in_raw_gdb if contains_pdf is not None : project_data [ 'contains_pdf' ] = contains_pdf if contains_image is not None : project_data [ 'contains_image' ] = contains_image if extracted_attachments_path is not None : project_data [ 'extracted_attachments_path' ] = extracted_attachments_path if editor_tracking_enabled is not None : project_data [ 'editor_tracking_enabled' ] = editor_tracking_enabled if processed is not None : project_data [ 'processed' ] = processed if entry_type is not None : project_data [ 'entry_type' ] = entry_type def get_data ( self , project_spatial_id : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ project_spatial_id ] def find_matching_spatial_id ( self , raw_data_path : str ) -> str : \"\"\" Search for a matching entry for the raw data path. Args: raw_data_path (str): The path of the raw data. Returns: str: A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. \"\"\" return next ( ( project_spatial_id for project_spatial_id , project_data in self . data_dict . items () if project_data . get ( 'raw_data_path' ) == raw_data_path ), None ) def get_highest_suffix ( self , project_number : str ) -> int : \"\"\" Get the highest suffix for the given project number in the spatial_project_id field. Args: project_number (str): The project number to find the highest suffix for. Returns: int: The highest suffix found, or 0 if none are found. \"\"\" suffixes = [ int ( key . split ( '_' )[ - 1 ]) for key in self . data_dict . keys () if self . data_dict [ key ] . get ( 'project_number' ) == project_number ] return max ( suffixes , default = 0 ) def create_project_spatial_id ( self , project_number : str ) -> str : \"\"\" Create the next project spatial id for the file Args: project_number (str): The formatted project number. Returns: str: The project spatial id next in line. \"\"\" # Get the next suffix from the project spatial ids from the data entries results_next_id = self . get_highest_suffix ( project_number ) + 1 # Clean the project number and format to the correct project_spatial_id format clean_project_number = project_number . replace ( '- ' , '' ) . replace ( ' ' , '_' ) return clean_project_number + '_' + str ( results_next_id ) . zfill ( 2 ) def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = [ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' , 'created_at' ] rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : project_spatial_id = fields [ 0 ] values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ project_spatial_id ] = values self . database_connection . disconnect () def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker , dtype = { 'project_spatial_id' : object , 'project_number' : object , 'dropped' : bool , 'raw_data_path' : object , 'raw_gdb_path' : object , 'absolute_file_path' : object , 'in_raw_gdb' : bool , 'contains_pdf' : bool , 'contains_image' : bool , 'extracted_attachments_path' : object , 'editor_tracking_enabled' : bool , 'processed' : bool , 'entry_type' : str }, index_col = None ) for index , row in data_df . iterrows (): self . add_data ( project_spatial_id = row [ 'project_spatial_id' ], project_number = row [ 'project_number' ], dropped = row [ 'dropped' ], raw_data_path = row [ 'raw_data_path' ], raw_gdb_path = row [ 'raw_gdb_path' ], absolute_file_path = row [ 'absolute_file_path' ], in_raw_gdb = row [ 'in_raw_gdb' ], contains_pdf = row [ 'contains_pdf' ], contains_image = row [ 'contains_image' ], extracted_attachments_path = row [ 'extracted_attachments_path' ], editor_tracking_enabled = row [ 'editor_tracking_enabled' ], processed = row [ 'processed' ], entry_type = row [ 'entry_type' ] ) def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ 'project_spatial_id' ] ) existing_ids = set ( row [ 0 ] for row in rows ) for key , value in self . data_dict . items (): try : if key in existing_ids and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { 'dropped' : value [ 'dropped' ], 'in_raw_gdb' : value [ 'in_raw_gdb' ], 'contains_pdf' : value [ 'contains_pdf' ], 'contains_image' : value [ 'contains_image' ], 'editor_tracking_enabled' : value [ 'editor_tracking_enabled' ], 'processed' : value [ 'processed' ], 'entry_type' : value [ 'entry_type' ] }, condition = f \"project_spatial_id=' { key } '\" ) elif key not in existing_ids : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = ( 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ), values = ( key , value [ 'project_number' ], value [ 'dropped' ], value [ 'raw_data_path' ], value [ 'raw_gdb_path' ], value [ 'absolute_file_path' ], value [ 'in_raw_gdb' ], value [ 'contains_pdf' ], value [ 'contains_image' ], value [ 'extracted_attachments_path' ], value [ 'editor_tracking_enabled' ], value [ 'processed' ], value [ 'entry_type' ] ) ) except psycopg2 . errors . ForeignKeyViolation as error : log ( self . log_path , Colors . ERROR , error ) self . database_connection . disconnect () def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" # TODO: Work on getting the save to file exactly like save to database # Create a DataFrame and save it to Excel if the data tracker file doesn't exist df = pd . DataFrame ( list ( self . data_dict . values ()), columns = [ 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]) # Add 'project_spatial_id' to the DataFrame df [ 'project_spatial_id' ] = list ( self . data_dict . keys ()) # Reorder columns to have 'project_spatial_id' as the first column df = df [[ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]] # Sort the rows by the project_spatial_id column df = df . sort_values ( by = [ 'project_spatial_id' ]) # Check if the directory exists, if not, create it directory = os . path . dirname ( self . datatracker ) if not os . path . exists ( directory ): os . makedirs ( directory ) # Convert dataframe to excel df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' ) __init__ ( data_traker_path , load_from = 'database' , save_to = 'database' , log_path = None ) Initializes the Data class with input parameters. Used to store the data tracker information. Parameters: data_traker_path ( str ) \u2013 Path to data tracker to load data if exists. load_from ( str , default: 'database' ) \u2013 Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to ( str , default: 'database' ) \u2013 Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path ( str , default: None ) \u2013 The path to the log file if you wish to keep any errors that occur. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Data class with input parameters. Used to store the data tracker information. Args: data_traker_path (str): Path to data tracker to load data if exists. load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path (str, optional): The path to the log file if you wish to keep any errors that occur. \"\"\" super () . __init__ ( data_traker_path , load_from , save_to , log_path ) add_data ( project_spatial_id , project_number , dropped , raw_data_path , raw_gdb_path , absolute_file_path , in_raw_gdb , contains_pdf , contains_image , extracted_attachments_path , editor_tracking_enabled , processed , entry_type ) Adds project data to the data tracker. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. project_number ( str ) \u2013 Project number. dropped ( bool ) \u2013 Indicates whether the entry is dropped, non-valid etc. raw_data_path ( str ) \u2013 Raw data path. raw_gdb_path ( str ) \u2013 Absolute path to the output geodatabase. absolute_file_path ( str ) \u2013 The full absolute file path. in_raw_gdb ( bool ) \u2013 Indicates whether data is in raw GDB. contains_pdf ( bool ) \u2013 Indicates whether data contains PDF files. contains_image ( bool ) \u2013 Indicates whether data contains image files. extracted_attachments_path ( str ) \u2013 The path to the extracted attachments if applicable. editor_tracking_enabled ( bool ) \u2013 Indicates whether the editor tracking has been enabled for the layer in the gdb. processed ( bool ) \u2013 Indicates whether data has been processed yet. entry_type ( str ) \u2013 Indicates wether the entry contains information for an aspatial or spatial entry. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def add_data ( self , project_spatial_id : str , project_number : str , dropped : bool , raw_data_path : str , raw_gdb_path : str , absolute_file_path : str , in_raw_gdb : bool , contains_pdf : bool , contains_image : bool , extracted_attachments_path : str , editor_tracking_enabled : bool , processed : bool , entry_type : str ) -> None : \"\"\" Adds project data to the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number. dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path. raw_gdb_path (str): Absolute path to the output geodatabase. absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB. contains_pdf (bool): Indicates whether data contains PDF files. contains_image (bool): Indicates whether data contains image files. extracted_attachments_path (str): The path to the extracted attachments if applicable. editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb. processed (bool): Indicates whether data has been processed yet. entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry. \"\"\" self . data_dict [ project_spatial_id ] = { 'project_number' : project_number , 'dropped' : dropped , 'raw_data_path' : raw_data_path , 'raw_gdb_path' : raw_gdb_path , 'absolute_file_path' : absolute_file_path , 'in_raw_gdb' : in_raw_gdb , 'contains_pdf' : contains_pdf , 'contains_image' : contains_image , 'extracted_attachments_path' : extracted_attachments_path , 'editor_tracking_enabled' : editor_tracking_enabled , 'processed' : processed , 'entry_type' : entry_type } create_project_spatial_id ( project_number ) Create the next project spatial id for the file Parameters: project_number ( str ) \u2013 The formatted project number. Returns: str ( str ) \u2013 The project spatial id next in line. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def create_project_spatial_id ( self , project_number : str ) -> str : \"\"\" Create the next project spatial id for the file Args: project_number (str): The formatted project number. Returns: str: The project spatial id next in line. \"\"\" # Get the next suffix from the project spatial ids from the data entries results_next_id = self . get_highest_suffix ( project_number ) + 1 # Clean the project number and format to the correct project_spatial_id format clean_project_number = project_number . replace ( '- ' , '' ) . replace ( ' ' , '_' ) return clean_project_number + '_' + str ( results_next_id ) . zfill ( 2 ) find_matching_spatial_id ( raw_data_path ) Search for a matching entry for the raw data path. Parameters: raw_data_path ( str ) \u2013 The path of the raw data. Returns: str ( str ) \u2013 A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def find_matching_spatial_id ( self , raw_data_path : str ) -> str : \"\"\" Search for a matching entry for the raw data path. Args: raw_data_path (str): The path of the raw data. Returns: str: A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. \"\"\" return next ( ( project_spatial_id for project_spatial_id , project_data in self . data_dict . items () if project_data . get ( 'raw_data_path' ) == raw_data_path ), None ) get_data ( project_spatial_id ) Gets an object of values given a project spatial id. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. Returns: dict ( dict ) \u2013 the values that correspond to the given key Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 305 306 307 308 309 310 311 312 313 314 315 def get_data ( self , project_spatial_id : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ project_spatial_id ] get_highest_suffix ( project_number ) Get the highest suffix for the given project number in the spatial_project_id field. Parameters: project_number ( str ) \u2013 The project number to find the highest suffix for. Returns: int ( int ) \u2013 The highest suffix found, or 0 if none are found. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def get_highest_suffix ( self , project_number : str ) -> int : \"\"\" Get the highest suffix for the given project number in the spatial_project_id field. Args: project_number (str): The project number to find the highest suffix for. Returns: int: The highest suffix found, or 0 if none are found. \"\"\" suffixes = [ int ( key . split ( '_' )[ - 1 ]) for key in self . data_dict . keys () if self . data_dict [ key ] . get ( 'project_number' ) == project_number ] return max ( suffixes , default = 0 ) load_from_database () Load data from a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = [ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' , 'created_at' ] rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : project_spatial_id = fields [ 0 ] values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ project_spatial_id ] = values self . database_connection . disconnect () load_from_file () Load data from a file into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker , dtype = { 'project_spatial_id' : object , 'project_number' : object , 'dropped' : bool , 'raw_data_path' : object , 'raw_gdb_path' : object , 'absolute_file_path' : object , 'in_raw_gdb' : bool , 'contains_pdf' : bool , 'contains_image' : bool , 'extracted_attachments_path' : object , 'editor_tracking_enabled' : bool , 'processed' : bool , 'entry_type' : str }, index_col = None ) for index , row in data_df . iterrows (): self . add_data ( project_spatial_id = row [ 'project_spatial_id' ], project_number = row [ 'project_number' ], dropped = row [ 'dropped' ], raw_data_path = row [ 'raw_data_path' ], raw_gdb_path = row [ 'raw_gdb_path' ], absolute_file_path = row [ 'absolute_file_path' ], in_raw_gdb = row [ 'in_raw_gdb' ], contains_pdf = row [ 'contains_pdf' ], contains_image = row [ 'contains_image' ], extracted_attachments_path = row [ 'extracted_attachments_path' ], editor_tracking_enabled = row [ 'editor_tracking_enabled' ], processed = row [ 'processed' ], entry_type = row [ 'entry_type' ] ) save_to_database ( update = False ) Save data tracker information to a database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ 'project_spatial_id' ] ) existing_ids = set ( row [ 0 ] for row in rows ) for key , value in self . data_dict . items (): try : if key in existing_ids and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { 'dropped' : value [ 'dropped' ], 'in_raw_gdb' : value [ 'in_raw_gdb' ], 'contains_pdf' : value [ 'contains_pdf' ], 'contains_image' : value [ 'contains_image' ], 'editor_tracking_enabled' : value [ 'editor_tracking_enabled' ], 'processed' : value [ 'processed' ], 'entry_type' : value [ 'entry_type' ] }, condition = f \"project_spatial_id=' { key } '\" ) elif key not in existing_ids : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = ( 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ), values = ( key , value [ 'project_number' ], value [ 'dropped' ], value [ 'raw_data_path' ], value [ 'raw_gdb_path' ], value [ 'absolute_file_path' ], value [ 'in_raw_gdb' ], value [ 'contains_pdf' ], value [ 'contains_image' ], value [ 'extracted_attachments_path' ], value [ 'editor_tracking_enabled' ], value [ 'processed' ], value [ 'entry_type' ] ) ) except psycopg2 . errors . ForeignKeyViolation as error : log ( self . log_path , Colors . ERROR , error ) self . database_connection . disconnect () save_to_file () Save data tracker information to a file. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" # TODO: Work on getting the save to file exactly like save to database # Create a DataFrame and save it to Excel if the data tracker file doesn't exist df = pd . DataFrame ( list ( self . data_dict . values ()), columns = [ 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]) # Add 'project_spatial_id' to the DataFrame df [ 'project_spatial_id' ] = list ( self . data_dict . keys ()) # Reorder columns to have 'project_spatial_id' as the first column df = df [[ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]] # Sort the rows by the project_spatial_id column df = df . sort_values ( by = [ 'project_spatial_id' ]) # Check if the directory exists, if not, create it directory = os . path . dirname ( self . datatracker ) if not os . path . exists ( directory ): os . makedirs ( directory ) # Convert dataframe to excel df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' ) set_data ( project_spatial_id , project_number = None , dropped = None , raw_data_path = None , absolute_file_path = None , in_raw_gdb = None , contains_pdf = None , contains_image = None , extracted_attachments_path = None , editor_tracking_enabled = None , processed = None , entry_type = None ) Updates project data in the data tracker. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. project_number ( str , default: None ) \u2013 Project number (optional). dropped ( bool , default: None ) \u2013 Indicates whether the entry is dropped, non-valid etc. raw_data_path ( str , default: None ) \u2013 Raw data path (optional). absolute_file_path ( str , default: None ) \u2013 The full absolute file path. in_raw_gdb ( bool , default: None ) \u2013 Indicates whether data is in raw GDB (optional). contains_pdf ( bool , default: None ) \u2013 Indicates whether data contains PDF files (optional). contains_image ( bool , default: None ) \u2013 Indicates whether data contains image files (optional). extracted_attachments_path ( str , default: None ) \u2013 The path to the extracted attachments if applicable (optional). editor_tracking_enabled ( bool , default: None ) \u2013 Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed ( bool , default: None ) \u2013 Indicates whether data has been processed yet (optional). entry_type ( str , default: None ) \u2013 Indicates wether the entry contains information for an aspatial or spatial entry (optional). Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def set_data ( self , project_spatial_id : str , project_number : str = None , dropped : bool = None , raw_data_path : str = None , absolute_file_path : str = None , in_raw_gdb : bool = None , contains_pdf : bool = None , contains_image : bool = None , extracted_attachments_path : str = None , editor_tracking_enabled : bool = None , processed : bool = None , entry_type : str = None ) -> None : \"\"\" Updates project data in the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number (optional). dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path (optional). absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB (optional). contains_pdf (bool): Indicates whether data contains PDF files (optional). contains_image (bool): Indicates whether data contains image files (optional). extracted_attachments_path (str): The path to the extracted attachments if applicable (optional). editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed (bool): Indicates whether data has been processed yet (optional). entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional). \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( project_spatial_id , {}) if project_number is not None : project_data [ 'project_number' ] = project_number if dropped is not None : project_data [ 'dropped' ] = dropped if raw_data_path is not None : project_data [ 'raw_data_path' ] = raw_data_path if absolute_file_path is not None : project_data [ 'absolute_file_path' ] = absolute_file_path if in_raw_gdb is not None : project_data [ 'in_raw_gdb' ] = in_raw_gdb if contains_pdf is not None : project_data [ 'contains_pdf' ] = contains_pdf if contains_image is not None : project_data [ 'contains_image' ] = contains_image if extracted_attachments_path is not None : project_data [ 'extracted_attachments_path' ] = extracted_attachments_path if editor_tracking_enabled is not None : project_data [ 'editor_tracking_enabled' ] = editor_tracking_enabled if processed is not None : project_data [ 'processed' ] = processed if entry_type is not None : project_data [ 'entry_type' ] = entry_type","title":"Datatracker"},{"location":"pages/SpatialTransformer/Datatracker/#spatialtransformerdatatracker","text":"","title":"SpatialTransformer.Datatracker"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker","text":"Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 class Datatracker : def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Datatracker class with input parameters to store data tracker information. Args: data_traker_path (str): Path to data tracker file. load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path (str, optional): Path to log file for recording errors. \"\"\" self . data_dict = {} self . datatracker = data_traker_path self . load_from = load_from self . save_to = save_to self . log_path = log_path if load_from == 'database' or save_to == 'database' : # Create database object self . database_connection = Database () # Read connection parameters from the configuration file self . database_parameters = self . database_connection . get_params () self . database_connection . connect ( self . database_parameters ) self . database_pkey = self . database_connection . get_pkey ( self . database_connection . schema , self . database_connection . table ) self . database_connection . disconnect () self . load_data () def add_data ( self , key : str , ** kwargs ) -> None : \"\"\" Adds project data to the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Additional keyword arguments for project data. \"\"\" self . data_dict [ key ] = kwargs def set_data ( self , key : str , ** kwargs ) -> None : \"\"\" Updates project data in the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Keyword arguments for updating project data. \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( key , {}) for pkey , pvalue in kwargs . items (): if pvalue is not None : project_data [ pkey ] = pvalue self . data_dict [ key ] = project_data def get_data ( self , key : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: key (str): Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ key ] def find_matching_data ( self , ** kwargs ) -> ( str , dict ): # type: ignore \"\"\" Search for a matching entry in the data based on given parameters. Args: **kwargs (any): Keyword arguments for finding a matching key. Returns: str: A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. \"\"\" return next ( ( ( key , data ) for key , data in self . data_dict . items () if all ( data . get ( field ) == value for field , value in kwargs . items ()) ), ( None , None ) ) def count_occurances ( self , field : str , value : str ) -> int : \"\"\" Count the occurrences of a specified field in the data object. Args: field (str): Name of the parameter to count occurrences. value (str): Value of the parameter to count occurrences. Returns: int: Number of occurrences of the specified parameter. \"\"\" return sum ( 1 for data in self . data_dict . values () if data . get ( field ) == value ) def load_data ( self ) -> None : \"\"\" Load data from an existing data tracker or a database connection into class. \"\"\" if self . load_from == 'database' : self . load_from_database () else : self . load_from_file () def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = self . database_connection . get_columns ( schema = self . database_connection . schema , table = self . database_connection . table ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ self . database_pkey ] = values self . database_connection . disconnect () def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker ) for index , row in data_df . iterrows (): pkey = row . index [ 0 ] data = row . drop ( pkey ) . to_dict () self . add_data ( key = row [ pkey ], ** data ) def save_data ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to data tracker or database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" if self . save_to == 'database' : self . save_to_database ( update ) else : self . save_to_file () def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) existing_keys = set ( row [ 0 ] for row in self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] )) for key , value in self . data_dict . items (): if key in existing_keys and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { field : value [ field ] for field in value }, condition = f \" { self . database_pkey } =' { key } '\" ) elif key not in existing_keys : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] + list ( value . keys ()), values = [ key ] + list ( value . values ()) ) self . database_connection . disconnect () def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" df = pd . DataFrame ( list ( self . data_dict . values ())) df . insert ( 0 , 'key' , self . data_dict . keys ()) if not df . empty : df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' )","title":"Datatracker"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.__init__","text":"Initializes the Datatracker class with input parameters to store data tracker information. Parameters: data_traker_path ( str ) \u2013 Path to data tracker file. load_from ( str , default: 'database' ) \u2013 Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to ( str , default: 'database' ) \u2013 Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path ( str , default: None ) \u2013 Path to log file for recording errors. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Datatracker class with input parameters to store data tracker information. Args: data_traker_path (str): Path to data tracker file. load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'. save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'. log_path (str, optional): Path to log file for recording errors. \"\"\" self . data_dict = {} self . datatracker = data_traker_path self . load_from = load_from self . save_to = save_to self . log_path = log_path if load_from == 'database' or save_to == 'database' : # Create database object self . database_connection = Database () # Read connection parameters from the configuration file self . database_parameters = self . database_connection . get_params () self . database_connection . connect ( self . database_parameters ) self . database_pkey = self . database_connection . get_pkey ( self . database_connection . schema , self . database_connection . table ) self . database_connection . disconnect () self . load_data ()","title":"__init__"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.add_data","text":"Adds project data to the data tracker. Parameters: key ( str ) \u2013 Acts as key in dictionary. **kwargs ( any , default: {} ) \u2013 Additional keyword arguments for project data. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 43 44 45 46 47 48 49 50 51 def add_data ( self , key : str , ** kwargs ) -> None : \"\"\" Adds project data to the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Additional keyword arguments for project data. \"\"\" self . data_dict [ key ] = kwargs","title":"add_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.count_occurances","text":"Count the occurrences of a specified field in the data object. Parameters: field ( str ) \u2013 Name of the parameter to count occurrences. value ( str ) \u2013 Value of the parameter to count occurrences. Returns: int ( int ) \u2013 Number of occurrences of the specified parameter. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def count_occurances ( self , field : str , value : str ) -> int : \"\"\" Count the occurrences of a specified field in the data object. Args: field (str): Name of the parameter to count occurrences. value (str): Value of the parameter to count occurrences. Returns: int: Number of occurrences of the specified parameter. \"\"\" return sum ( 1 for data in self . data_dict . values () if data . get ( field ) == value )","title":"count_occurances"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.find_matching_data","text":"Search for a matching entry in the data based on given parameters. Parameters: **kwargs ( any , default: {} ) \u2013 Keyword arguments for finding a matching key. Returns: str ( ( str , dict ) ) \u2013 A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def find_matching_data ( self , ** kwargs ) -> ( str , dict ): # type: ignore \"\"\" Search for a matching entry in the data based on given parameters. Args: **kwargs (any): Keyword arguments for finding a matching key. Returns: str: A tuple of matching (key, data) if the parameters passed already exists in the dataframe, otherwise return None. \"\"\" return next ( ( ( key , data ) for key , data in self . data_dict . items () if all ( data . get ( field ) == value for field , value in kwargs . items ()) ), ( None , None ) )","title":"find_matching_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.get_data","text":"Gets an object of values given a project spatial id. Parameters: key ( str ) \u2013 Acts as key in dictionary. Returns: dict ( dict ) \u2013 the values that correspond to the given key Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 70 71 72 73 74 75 76 77 78 79 80 def get_data ( self , key : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: key (str): Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ key ]","title":"get_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_data","text":"Load data from an existing data tracker or a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 116 117 118 119 120 121 122 123 def load_data ( self ) -> None : \"\"\" Load data from an existing data tracker or a database connection into class. \"\"\" if self . load_from == 'database' : self . load_from_database () else : self . load_from_file ()","title":"load_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_from_database","text":"Load data from a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = self . database_connection . get_columns ( schema = self . database_connection . schema , table = self . database_connection . table ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ self . database_pkey ] = values self . database_connection . disconnect ()","title":"load_from_database"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_from_file","text":"Load data from a file into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 141 142 143 144 145 146 147 148 149 150 151 152 153 def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker ) for index , row in data_df . iterrows (): pkey = row . index [ 0 ] data = row . drop ( pkey ) . to_dict () self . add_data ( key = row [ pkey ], ** data )","title":"load_from_file"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_data","text":"Save data tracker information to data tracker or database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 155 156 157 158 159 160 161 162 163 164 165 def save_data ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to data tracker or database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" if self . save_to == 'database' : self . save_to_database ( update ) else : self . save_to_file ()","title":"save_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_to_database","text":"Save data tracker information to a database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) existing_keys = set ( row [ 0 ] for row in self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] )) for key , value in self . data_dict . items (): if key in existing_keys and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { field : value [ field ] for field in value }, condition = f \" { self . database_pkey } =' { key } '\" ) elif key not in existing_keys : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ self . database_pkey ] + list ( value . keys ()), values = [ key ] + list ( value . values ()) ) self . database_connection . disconnect ()","title":"save_to_database"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_to_file","text":"Save data tracker information to a file. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 200 201 202 203 204 205 206 207 208 209 def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" df = pd . DataFrame ( list ( self . data_dict . values ())) df . insert ( 0 , 'key' , self . data_dict . keys ()) if not df . empty : df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' )","title":"save_to_file"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.set_data","text":"Updates project data in the data tracker. Parameters: key ( str ) \u2013 Acts as key in dictionary. **kwargs ( any , default: {} ) \u2013 Keyword arguments for updating project data. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def set_data ( self , key : str , ** kwargs ) -> None : \"\"\" Updates project data in the data tracker. Args: key (str): Acts as key in dictionary. **kwargs (any): Keyword arguments for updating project data. \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( key , {}) for pkey , pvalue in kwargs . items (): if pvalue is not None : project_data [ pkey ] = pvalue self . data_dict [ key ] = project_data","title":"set_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT","text":"Bases: Datatracker Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 class Datatracker2BT ( Datatracker ): def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Data class with input parameters. Used to store the data tracker information. Args: data_traker_path (str): Path to data tracker to load data if exists. load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path (str, optional): The path to the log file if you wish to keep any errors that occur. \"\"\" super () . __init__ ( data_traker_path , load_from , save_to , log_path ) def add_data ( self , project_spatial_id : str , project_number : str , dropped : bool , raw_data_path : str , raw_gdb_path : str , absolute_file_path : str , in_raw_gdb : bool , contains_pdf : bool , contains_image : bool , extracted_attachments_path : str , editor_tracking_enabled : bool , processed : bool , entry_type : str ) -> None : \"\"\" Adds project data to the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number. dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path. raw_gdb_path (str): Absolute path to the output geodatabase. absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB. contains_pdf (bool): Indicates whether data contains PDF files. contains_image (bool): Indicates whether data contains image files. extracted_attachments_path (str): The path to the extracted attachments if applicable. editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb. processed (bool): Indicates whether data has been processed yet. entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry. \"\"\" self . data_dict [ project_spatial_id ] = { 'project_number' : project_number , 'dropped' : dropped , 'raw_data_path' : raw_data_path , 'raw_gdb_path' : raw_gdb_path , 'absolute_file_path' : absolute_file_path , 'in_raw_gdb' : in_raw_gdb , 'contains_pdf' : contains_pdf , 'contains_image' : contains_image , 'extracted_attachments_path' : extracted_attachments_path , 'editor_tracking_enabled' : editor_tracking_enabled , 'processed' : processed , 'entry_type' : entry_type } def set_data ( self , project_spatial_id : str , project_number : str = None , dropped : bool = None , raw_data_path : str = None , absolute_file_path : str = None , in_raw_gdb : bool = None , contains_pdf : bool = None , contains_image : bool = None , extracted_attachments_path : str = None , editor_tracking_enabled : bool = None , processed : bool = None , entry_type : str = None ) -> None : \"\"\" Updates project data in the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number (optional). dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path (optional). absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB (optional). contains_pdf (bool): Indicates whether data contains PDF files (optional). contains_image (bool): Indicates whether data contains image files (optional). extracted_attachments_path (str): The path to the extracted attachments if applicable (optional). editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed (bool): Indicates whether data has been processed yet (optional). entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional). \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( project_spatial_id , {}) if project_number is not None : project_data [ 'project_number' ] = project_number if dropped is not None : project_data [ 'dropped' ] = dropped if raw_data_path is not None : project_data [ 'raw_data_path' ] = raw_data_path if absolute_file_path is not None : project_data [ 'absolute_file_path' ] = absolute_file_path if in_raw_gdb is not None : project_data [ 'in_raw_gdb' ] = in_raw_gdb if contains_pdf is not None : project_data [ 'contains_pdf' ] = contains_pdf if contains_image is not None : project_data [ 'contains_image' ] = contains_image if extracted_attachments_path is not None : project_data [ 'extracted_attachments_path' ] = extracted_attachments_path if editor_tracking_enabled is not None : project_data [ 'editor_tracking_enabled' ] = editor_tracking_enabled if processed is not None : project_data [ 'processed' ] = processed if entry_type is not None : project_data [ 'entry_type' ] = entry_type def get_data ( self , project_spatial_id : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ project_spatial_id ] def find_matching_spatial_id ( self , raw_data_path : str ) -> str : \"\"\" Search for a matching entry for the raw data path. Args: raw_data_path (str): The path of the raw data. Returns: str: A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. \"\"\" return next ( ( project_spatial_id for project_spatial_id , project_data in self . data_dict . items () if project_data . get ( 'raw_data_path' ) == raw_data_path ), None ) def get_highest_suffix ( self , project_number : str ) -> int : \"\"\" Get the highest suffix for the given project number in the spatial_project_id field. Args: project_number (str): The project number to find the highest suffix for. Returns: int: The highest suffix found, or 0 if none are found. \"\"\" suffixes = [ int ( key . split ( '_' )[ - 1 ]) for key in self . data_dict . keys () if self . data_dict [ key ] . get ( 'project_number' ) == project_number ] return max ( suffixes , default = 0 ) def create_project_spatial_id ( self , project_number : str ) -> str : \"\"\" Create the next project spatial id for the file Args: project_number (str): The formatted project number. Returns: str: The project spatial id next in line. \"\"\" # Get the next suffix from the project spatial ids from the data entries results_next_id = self . get_highest_suffix ( project_number ) + 1 # Clean the project number and format to the correct project_spatial_id format clean_project_number = project_number . replace ( '- ' , '' ) . replace ( ' ' , '_' ) return clean_project_number + '_' + str ( results_next_id ) . zfill ( 2 ) def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = [ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' , 'created_at' ] rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : project_spatial_id = fields [ 0 ] values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ project_spatial_id ] = values self . database_connection . disconnect () def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker , dtype = { 'project_spatial_id' : object , 'project_number' : object , 'dropped' : bool , 'raw_data_path' : object , 'raw_gdb_path' : object , 'absolute_file_path' : object , 'in_raw_gdb' : bool , 'contains_pdf' : bool , 'contains_image' : bool , 'extracted_attachments_path' : object , 'editor_tracking_enabled' : bool , 'processed' : bool , 'entry_type' : str }, index_col = None ) for index , row in data_df . iterrows (): self . add_data ( project_spatial_id = row [ 'project_spatial_id' ], project_number = row [ 'project_number' ], dropped = row [ 'dropped' ], raw_data_path = row [ 'raw_data_path' ], raw_gdb_path = row [ 'raw_gdb_path' ], absolute_file_path = row [ 'absolute_file_path' ], in_raw_gdb = row [ 'in_raw_gdb' ], contains_pdf = row [ 'contains_pdf' ], contains_image = row [ 'contains_image' ], extracted_attachments_path = row [ 'extracted_attachments_path' ], editor_tracking_enabled = row [ 'editor_tracking_enabled' ], processed = row [ 'processed' ], entry_type = row [ 'entry_type' ] ) def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ 'project_spatial_id' ] ) existing_ids = set ( row [ 0 ] for row in rows ) for key , value in self . data_dict . items (): try : if key in existing_ids and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { 'dropped' : value [ 'dropped' ], 'in_raw_gdb' : value [ 'in_raw_gdb' ], 'contains_pdf' : value [ 'contains_pdf' ], 'contains_image' : value [ 'contains_image' ], 'editor_tracking_enabled' : value [ 'editor_tracking_enabled' ], 'processed' : value [ 'processed' ], 'entry_type' : value [ 'entry_type' ] }, condition = f \"project_spatial_id=' { key } '\" ) elif key not in existing_ids : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = ( 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ), values = ( key , value [ 'project_number' ], value [ 'dropped' ], value [ 'raw_data_path' ], value [ 'raw_gdb_path' ], value [ 'absolute_file_path' ], value [ 'in_raw_gdb' ], value [ 'contains_pdf' ], value [ 'contains_image' ], value [ 'extracted_attachments_path' ], value [ 'editor_tracking_enabled' ], value [ 'processed' ], value [ 'entry_type' ] ) ) except psycopg2 . errors . ForeignKeyViolation as error : log ( self . log_path , Colors . ERROR , error ) self . database_connection . disconnect () def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" # TODO: Work on getting the save to file exactly like save to database # Create a DataFrame and save it to Excel if the data tracker file doesn't exist df = pd . DataFrame ( list ( self . data_dict . values ()), columns = [ 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]) # Add 'project_spatial_id' to the DataFrame df [ 'project_spatial_id' ] = list ( self . data_dict . keys ()) # Reorder columns to have 'project_spatial_id' as the first column df = df [[ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]] # Sort the rows by the project_spatial_id column df = df . sort_values ( by = [ 'project_spatial_id' ]) # Check if the directory exists, if not, create it directory = os . path . dirname ( self . datatracker ) if not os . path . exists ( directory ): os . makedirs ( directory ) # Convert dataframe to excel df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' )","title":"Datatracker2BT"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.__init__","text":"Initializes the Data class with input parameters. Used to store the data tracker information. Parameters: data_traker_path ( str ) \u2013 Path to data tracker to load data if exists. load_from ( str , default: 'database' ) \u2013 Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to ( str , default: 'database' ) \u2013 Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path ( str , default: None ) \u2013 The path to the log file if you wish to keep any errors that occur. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 216 217 218 219 220 221 222 223 224 225 226 def __init__ ( self , data_traker_path : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None ) -> None : \"\"\" Initializes the Data class with input parameters. Used to store the data tracker information. Args: data_traker_path (str): Path to data tracker to load data if exists. load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'. save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'. log_path (str, optional): The path to the log file if you wish to keep any errors that occur. \"\"\" super () . __init__ ( data_traker_path , load_from , save_to , log_path )","title":"__init__"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.add_data","text":"Adds project data to the data tracker. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. project_number ( str ) \u2013 Project number. dropped ( bool ) \u2013 Indicates whether the entry is dropped, non-valid etc. raw_data_path ( str ) \u2013 Raw data path. raw_gdb_path ( str ) \u2013 Absolute path to the output geodatabase. absolute_file_path ( str ) \u2013 The full absolute file path. in_raw_gdb ( bool ) \u2013 Indicates whether data is in raw GDB. contains_pdf ( bool ) \u2013 Indicates whether data contains PDF files. contains_image ( bool ) \u2013 Indicates whether data contains image files. extracted_attachments_path ( str ) \u2013 The path to the extracted attachments if applicable. editor_tracking_enabled ( bool ) \u2013 Indicates whether the editor tracking has been enabled for the layer in the gdb. processed ( bool ) \u2013 Indicates whether data has been processed yet. entry_type ( str ) \u2013 Indicates wether the entry contains information for an aspatial or spatial entry. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 def add_data ( self , project_spatial_id : str , project_number : str , dropped : bool , raw_data_path : str , raw_gdb_path : str , absolute_file_path : str , in_raw_gdb : bool , contains_pdf : bool , contains_image : bool , extracted_attachments_path : str , editor_tracking_enabled : bool , processed : bool , entry_type : str ) -> None : \"\"\" Adds project data to the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number. dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path. raw_gdb_path (str): Absolute path to the output geodatabase. absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB. contains_pdf (bool): Indicates whether data contains PDF files. contains_image (bool): Indicates whether data contains image files. extracted_attachments_path (str): The path to the extracted attachments if applicable. editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb. processed (bool): Indicates whether data has been processed yet. entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry. \"\"\" self . data_dict [ project_spatial_id ] = { 'project_number' : project_number , 'dropped' : dropped , 'raw_data_path' : raw_data_path , 'raw_gdb_path' : raw_gdb_path , 'absolute_file_path' : absolute_file_path , 'in_raw_gdb' : in_raw_gdb , 'contains_pdf' : contains_pdf , 'contains_image' : contains_image , 'extracted_attachments_path' : extracted_attachments_path , 'editor_tracking_enabled' : editor_tracking_enabled , 'processed' : processed , 'entry_type' : entry_type }","title":"add_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.create_project_spatial_id","text":"Create the next project spatial id for the file Parameters: project_number ( str ) \u2013 The formatted project number. Returns: str ( str ) \u2013 The project spatial id next in line. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 def create_project_spatial_id ( self , project_number : str ) -> str : \"\"\" Create the next project spatial id for the file Args: project_number (str): The formatted project number. Returns: str: The project spatial id next in line. \"\"\" # Get the next suffix from the project spatial ids from the data entries results_next_id = self . get_highest_suffix ( project_number ) + 1 # Clean the project number and format to the correct project_spatial_id format clean_project_number = project_number . replace ( '- ' , '' ) . replace ( ' ' , '_' ) return clean_project_number + '_' + str ( results_next_id ) . zfill ( 2 )","title":"create_project_spatial_id"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.find_matching_spatial_id","text":"Search for a matching entry for the raw data path. Parameters: raw_data_path ( str ) \u2013 The path of the raw data. Returns: str ( str ) \u2013 A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 def find_matching_spatial_id ( self , raw_data_path : str ) -> str : \"\"\" Search for a matching entry for the raw data path. Args: raw_data_path (str): The path of the raw data. Returns: str: A matching project_spatial_id if it the raw data path already exists in the dataframe, otherwise return None. \"\"\" return next ( ( project_spatial_id for project_spatial_id , project_data in self . data_dict . items () if project_data . get ( 'raw_data_path' ) == raw_data_path ), None )","title":"find_matching_spatial_id"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.get_data","text":"Gets an object of values given a project spatial id. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. Returns: dict ( dict ) \u2013 the values that correspond to the given key Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 305 306 307 308 309 310 311 312 313 314 315 def get_data ( self , project_spatial_id : str ) -> dict : \"\"\" Gets an object of values given a project spatial id. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. Returns: dict: the values that correspond to the given key \"\"\" return self . data_dict [ project_spatial_id ]","title":"get_data"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.get_highest_suffix","text":"Get the highest suffix for the given project number in the spatial_project_id field. Parameters: project_number ( str ) \u2013 The project number to find the highest suffix for. Returns: int ( int ) \u2013 The highest suffix found, or 0 if none are found. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 def get_highest_suffix ( self , project_number : str ) -> int : \"\"\" Get the highest suffix for the given project number in the spatial_project_id field. Args: project_number (str): The project number to find the highest suffix for. Returns: int: The highest suffix found, or 0 if none are found. \"\"\" suffixes = [ int ( key . split ( '_' )[ - 1 ]) for key in self . data_dict . keys () if self . data_dict [ key ] . get ( 'project_number' ) == project_number ] return max ( suffixes , default = 0 )","title":"get_highest_suffix"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.load_from_database","text":"Load data from a database connection into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 def load_from_database ( self ) -> None : \"\"\" Load data from a database connection into class. \"\"\" self . database_connection . connect ( self . database_parameters ) columns = [ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' , 'created_at' ] rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = columns ) for fields in rows : project_spatial_id = fields [ 0 ] values = dict ( zip ( columns [ 1 :], fields [ 1 :])) self . data_dict [ project_spatial_id ] = values self . database_connection . disconnect ()","title":"load_from_database"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.load_from_file","text":"Load data from a file into class. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 def load_from_file ( self ) -> None : \"\"\" Load data from a file into class. \"\"\" if not os . path . exists ( self . datatracker ): return data_df = pd . read_excel ( self . datatracker , dtype = { 'project_spatial_id' : object , 'project_number' : object , 'dropped' : bool , 'raw_data_path' : object , 'raw_gdb_path' : object , 'absolute_file_path' : object , 'in_raw_gdb' : bool , 'contains_pdf' : bool , 'contains_image' : bool , 'extracted_attachments_path' : object , 'editor_tracking_enabled' : bool , 'processed' : bool , 'entry_type' : str }, index_col = None ) for index , row in data_df . iterrows (): self . add_data ( project_spatial_id = row [ 'project_spatial_id' ], project_number = row [ 'project_number' ], dropped = row [ 'dropped' ], raw_data_path = row [ 'raw_data_path' ], raw_gdb_path = row [ 'raw_gdb_path' ], absolute_file_path = row [ 'absolute_file_path' ], in_raw_gdb = row [ 'in_raw_gdb' ], contains_pdf = row [ 'contains_pdf' ], contains_image = row [ 'contains_image' ], extracted_attachments_path = row [ 'extracted_attachments_path' ], editor_tracking_enabled = row [ 'editor_tracking_enabled' ], processed = row [ 'processed' ], entry_type = row [ 'entry_type' ] )","title":"load_from_file"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.save_to_database","text":"Save data tracker information to a database connection. Parameters: update ( bool , default: False ) \u2013 Flag to determine if there are some entries in the data object that will need updating. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 def save_to_database ( self , update : bool = False ) -> None : \"\"\" Save data tracker information to a database connection. Args: update (bool): Flag to determine if there are some entries in the data object that will need updating. \"\"\" self . database_connection . connect ( self . database_parameters ) rows = self . database_connection . read ( schema = self . database_connection . schema , table = self . database_connection . table , columns = [ 'project_spatial_id' ] ) existing_ids = set ( row [ 0 ] for row in rows ) for key , value in self . data_dict . items (): try : if key in existing_ids and update : self . database_connection . update ( schema = self . database_connection . schema , table = self . database_connection . table , values_dict = { 'dropped' : value [ 'dropped' ], 'in_raw_gdb' : value [ 'in_raw_gdb' ], 'contains_pdf' : value [ 'contains_pdf' ], 'contains_image' : value [ 'contains_image' ], 'editor_tracking_enabled' : value [ 'editor_tracking_enabled' ], 'processed' : value [ 'processed' ], 'entry_type' : value [ 'entry_type' ] }, condition = f \"project_spatial_id=' { key } '\" ) elif key not in existing_ids : self . database_connection . create ( schema = self . database_connection . schema , table = self . database_connection . table , columns = ( 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ), values = ( key , value [ 'project_number' ], value [ 'dropped' ], value [ 'raw_data_path' ], value [ 'raw_gdb_path' ], value [ 'absolute_file_path' ], value [ 'in_raw_gdb' ], value [ 'contains_pdf' ], value [ 'contains_image' ], value [ 'extracted_attachments_path' ], value [ 'editor_tracking_enabled' ], value [ 'processed' ], value [ 'entry_type' ] ) ) except psycopg2 . errors . ForeignKeyViolation as error : log ( self . log_path , Colors . ERROR , error ) self . database_connection . disconnect ()","title":"save_to_database"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.save_to_file","text":"Save data tracker information to a file. Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 def save_to_file ( self ) -> None : \"\"\" Save data tracker information to a file. \"\"\" # TODO: Work on getting the save to file exactly like save to database # Create a DataFrame and save it to Excel if the data tracker file doesn't exist df = pd . DataFrame ( list ( self . data_dict . values ()), columns = [ 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]) # Add 'project_spatial_id' to the DataFrame df [ 'project_spatial_id' ] = list ( self . data_dict . keys ()) # Reorder columns to have 'project_spatial_id' as the first column df = df [[ 'project_spatial_id' , 'project_number' , 'dropped' , 'raw_data_path' , 'raw_gdb_path' , 'absolute_file_path' , 'in_raw_gdb' , 'contains_pdf' , 'contains_image' , 'extracted_attachments_path' , 'editor_tracking_enabled' , 'processed' , 'entry_type' ]] # Sort the rows by the project_spatial_id column df = df . sort_values ( by = [ 'project_spatial_id' ]) # Check if the directory exists, if not, create it directory = os . path . dirname ( self . datatracker ) if not os . path . exists ( directory ): os . makedirs ( directory ) # Convert dataframe to excel df . to_excel ( self . datatracker , index = False ) log ( None , Colors . INFO , f 'The data tracker \" { self . datatracker } \" has been created/updated successfully.' )","title":"save_to_file"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.set_data","text":"Updates project data in the data tracker. Parameters: project_spatial_id ( str ) \u2013 Project spatial ID. Acts as key in dictionary. project_number ( str , default: None ) \u2013 Project number (optional). dropped ( bool , default: None ) \u2013 Indicates whether the entry is dropped, non-valid etc. raw_data_path ( str , default: None ) \u2013 Raw data path (optional). absolute_file_path ( str , default: None ) \u2013 The full absolute file path. in_raw_gdb ( bool , default: None ) \u2013 Indicates whether data is in raw GDB (optional). contains_pdf ( bool , default: None ) \u2013 Indicates whether data contains PDF files (optional). contains_image ( bool , default: None ) \u2013 Indicates whether data contains image files (optional). extracted_attachments_path ( str , default: None ) \u2013 The path to the extracted attachments if applicable (optional). editor_tracking_enabled ( bool , default: None ) \u2013 Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed ( bool , default: None ) \u2013 Indicates whether data has been processed yet (optional). entry_type ( str , default: None ) \u2013 Indicates wether the entry contains information for an aspatial or spatial entry (optional). Source code in twobilliontoolkit\\SpatialTransformer\\Datatracker.py 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def set_data ( self , project_spatial_id : str , project_number : str = None , dropped : bool = None , raw_data_path : str = None , absolute_file_path : str = None , in_raw_gdb : bool = None , contains_pdf : bool = None , contains_image : bool = None , extracted_attachments_path : str = None , editor_tracking_enabled : bool = None , processed : bool = None , entry_type : str = None ) -> None : \"\"\" Updates project data in the data tracker. Args: project_spatial_id (str): Project spatial ID. Acts as key in dictionary. project_number (str): Project number (optional). dropped (bool): Indicates whether the entry is dropped, non-valid etc. raw_data_path (str): Raw data path (optional). absolute_file_path (str): The full absolute file path. in_raw_gdb (bool): Indicates whether data is in raw GDB (optional). contains_pdf (bool): Indicates whether data contains PDF files (optional). contains_image (bool): Indicates whether data contains image files (optional). extracted_attachments_path (str): The path to the extracted attachments if applicable (optional). editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional). processed (bool): Indicates whether data has been processed yet (optional). entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional). \"\"\" # Update specified parameters as sets project_data = self . data_dict . get ( project_spatial_id , {}) if project_number is not None : project_data [ 'project_number' ] = project_number if dropped is not None : project_data [ 'dropped' ] = dropped if raw_data_path is not None : project_data [ 'raw_data_path' ] = raw_data_path if absolute_file_path is not None : project_data [ 'absolute_file_path' ] = absolute_file_path if in_raw_gdb is not None : project_data [ 'in_raw_gdb' ] = in_raw_gdb if contains_pdf is not None : project_data [ 'contains_pdf' ] = contains_pdf if contains_image is not None : project_data [ 'contains_image' ] = contains_image if extracted_attachments_path is not None : project_data [ 'extracted_attachments_path' ] = extracted_attachments_path if editor_tracking_enabled is not None : project_data [ 'editor_tracking_enabled' ] = editor_tracking_enabled if processed is not None : project_data [ 'processed' ] = processed if entry_type is not None : project_data [ 'entry_type' ] = entry_type","title":"set_data"},{"location":"pages/SpatialTransformer/Parameters/","text":"SpatialTransformer.Parameters Parameters Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class Parameters : def __init__ ( self , input_path : str , output_path : str , gdb_path : str , master_data_path : str , datatracker : str , attachments : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" self . local_dir = r 'C:\\LocalTwoBillionToolkit' # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool. if attachments == '' : attachments = os . path . basename ( gdb_path ) . replace ( '.gdb' , '_Attachments' ) # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if load_from == 'datatracker' or save_to == 'datatracker' : if not datatracker : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker must be specified.\" ) else : self . validate_path ( 'datatracker' , datatracker , must_ends_with = '.xlsx' ) if load_from == 'datatracker' : if not master_data_path : raise argparse . ArgumentTypeError ( \"If --load is 'datatracker', --master_data_path must be specified.\" ) else : self . validate_path ( 'master_data_path' , master_data_path , must_exists = True , must_ends_with = '.xlsx' ) # Validate and set paths self . validate_path ( 'input_path' , input_path , must_exists = True ) self . validate_path ( 'output_network_path' , output_path ) self . validate_path ( 'gdb_path' , gdb_path , must_ends_with = '.gdb' ) # Build the paths datatracker = os . path . join ( self . local_dir , datatracker ) attachments = os . path . join ( self . local_dir , attachments ) self . input = input_path self . output = output_path self . gdb_path = gdb_path self . local_gdb_path = os . path . join ( self . local_dir , os . path . basename ( self . gdb_path )) self . load_from = load_from self . save_to = save_to self . datatracker = datatracker self . attachments = attachments self . log = log_path self . debug = debug self . resume = resume self . suppress = suppress self . ps_script = ps_script self . project_numbers = self . get_project_numbers ( master_data_path ) def validate_path ( self , argument : str , param : str , must_exists : bool = False , must_ends_with : bool = None ) -> None : \"\"\" Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. \"\"\" if not isinstance ( param , str ) or not param . strip (): raise ValueError ( f ' { argument } : { param } must be a non-empty string.' ) if must_ends_with is not None and not param . endswith ( must_ends_with ): raise ValueError ( f ' { argument } : { param } must be of type { must_ends_with } .' ) if must_exists and not os . path . exists ( param ): raise ValueError ( f ' { argument } : { param } path does not exist.' ) def handle_unzip ( self ) -> None : \"\"\" Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return ripple_unzip ( self . input , self . output , self . log ) def create_gdb ( self ) -> None : \"\"\" Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return # Create the .gdb if it does not already exists if not arcpy . Exists ( self . local_gdb_path ): try : # Create the directory if it does not exist directory_path = os . path . dirname ( self . local_gdb_path ) if not os . path . exists ( directory_path ): os . makedirs ( directory_path ) # Create the file geodatabase file = os . path . basename ( self . local_gdb_path ) arcpy . management . CreateFileGDB ( directory_path , file ) log ( None , Colors . INFO , f 'Geodatabase: { file } created successfully' ) except arcpy . ExecuteError : log ( self . log , Colors . ERROR , arcpy . GetMessages ( 2 ), ps_script = self . ps_script ) def get_project_numbers ( self , master_datasheet : str = None ) -> list [ str ]: \"\"\" Get a list of project numbers from either the database or the master datasheet Returns: list[str]: A list of project numbers \"\"\" if self . load_from == 'datatracker' : masterdata = pd . read_excel ( master_datasheet ) # Extra validation on master data to check it has project number column if 'BT_Legacy_Project_ID__c' not in masterdata . columns : raise ValueError ( f \"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\" ) # Convert masterdata to a list of strings return masterdata [ 'BT_Legacy_Project_ID__c' ] . unique () . tolist () # Create database object database_connection = Database () # Read connection parameters from the configuration file database_parameters = database_connection . get_params () database_connection . connect ( database_parameters ) self . project_numbers = database_connection . read ( database_connection . schema , table = 'project_number' ) database_connection . disconnect () return [ str ( num [ 0 ]) for num in self . project_numbers ] __init__ ( input_path , output_path , gdb_path , master_data_path , datatracker , attachments , load_from = 'database' , save_to = 'database' , log_path = None , debug = False , resume = False , suppress = False , ps_script = None ) Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , input_path : str , output_path : str , gdb_path : str , master_data_path : str , datatracker : str , attachments : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" self . local_dir = r 'C:\\LocalTwoBillionToolkit' # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool. if attachments == '' : attachments = os . path . basename ( gdb_path ) . replace ( '.gdb' , '_Attachments' ) # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if load_from == 'datatracker' or save_to == 'datatracker' : if not datatracker : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker must be specified.\" ) else : self . validate_path ( 'datatracker' , datatracker , must_ends_with = '.xlsx' ) if load_from == 'datatracker' : if not master_data_path : raise argparse . ArgumentTypeError ( \"If --load is 'datatracker', --master_data_path must be specified.\" ) else : self . validate_path ( 'master_data_path' , master_data_path , must_exists = True , must_ends_with = '.xlsx' ) # Validate and set paths self . validate_path ( 'input_path' , input_path , must_exists = True ) self . validate_path ( 'output_network_path' , output_path ) self . validate_path ( 'gdb_path' , gdb_path , must_ends_with = '.gdb' ) # Build the paths datatracker = os . path . join ( self . local_dir , datatracker ) attachments = os . path . join ( self . local_dir , attachments ) self . input = input_path self . output = output_path self . gdb_path = gdb_path self . local_gdb_path = os . path . join ( self . local_dir , os . path . basename ( self . gdb_path )) self . load_from = load_from self . save_to = save_to self . datatracker = datatracker self . attachments = attachments self . log = log_path self . debug = debug self . resume = resume self . suppress = suppress self . ps_script = ps_script self . project_numbers = self . get_project_numbers ( master_data_path ) create_gdb () Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def create_gdb ( self ) -> None : \"\"\" Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return # Create the .gdb if it does not already exists if not arcpy . Exists ( self . local_gdb_path ): try : # Create the directory if it does not exist directory_path = os . path . dirname ( self . local_gdb_path ) if not os . path . exists ( directory_path ): os . makedirs ( directory_path ) # Create the file geodatabase file = os . path . basename ( self . local_gdb_path ) arcpy . management . CreateFileGDB ( directory_path , file ) log ( None , Colors . INFO , f 'Geodatabase: { file } created successfully' ) except arcpy . ExecuteError : log ( self . log , Colors . ERROR , arcpy . GetMessages ( 2 ), ps_script = self . ps_script ) get_project_numbers ( master_datasheet = None ) Get a list of project numbers from either the database or the master datasheet Returns: list [ str ] \u2013 list[str]: A list of project numbers Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_project_numbers ( self , master_datasheet : str = None ) -> list [ str ]: \"\"\" Get a list of project numbers from either the database or the master datasheet Returns: list[str]: A list of project numbers \"\"\" if self . load_from == 'datatracker' : masterdata = pd . read_excel ( master_datasheet ) # Extra validation on master data to check it has project number column if 'BT_Legacy_Project_ID__c' not in masterdata . columns : raise ValueError ( f \"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\" ) # Convert masterdata to a list of strings return masterdata [ 'BT_Legacy_Project_ID__c' ] . unique () . tolist () # Create database object database_connection = Database () # Read connection parameters from the configuration file database_parameters = database_connection . get_params () database_connection . connect ( database_parameters ) self . project_numbers = database_connection . read ( database_connection . schema , table = 'project_number' ) database_connection . disconnect () return [ str ( num [ 0 ]) for num in self . project_numbers ] handle_unzip () Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 102 103 104 105 106 107 108 109 110 111 112 def handle_unzip ( self ) -> None : \"\"\" Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return ripple_unzip ( self . input , self . output , self . log ) validate_path ( argument , param , must_exists = False , must_ends_with = None ) Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def validate_path ( self , argument : str , param : str , must_exists : bool = False , must_ends_with : bool = None ) -> None : \"\"\" Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. \"\"\" if not isinstance ( param , str ) or not param . strip (): raise ValueError ( f ' { argument } : { param } must be a non-empty string.' ) if must_ends_with is not None and not param . endswith ( must_ends_with ): raise ValueError ( f ' { argument } : { param } must be of type { must_ends_with } .' ) if must_exists and not os . path . exists ( param ): raise ValueError ( f ' { argument } : { param } path does not exist.' )","title":"Parameters"},{"location":"pages/SpatialTransformer/Parameters/#spatialtransformerparameters","text":"","title":"SpatialTransformer.Parameters"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters","text":"Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 class Parameters : def __init__ ( self , input_path : str , output_path : str , gdb_path : str , master_data_path : str , datatracker : str , attachments : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" self . local_dir = r 'C:\\LocalTwoBillionToolkit' # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool. if attachments == '' : attachments = os . path . basename ( gdb_path ) . replace ( '.gdb' , '_Attachments' ) # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if load_from == 'datatracker' or save_to == 'datatracker' : if not datatracker : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker must be specified.\" ) else : self . validate_path ( 'datatracker' , datatracker , must_ends_with = '.xlsx' ) if load_from == 'datatracker' : if not master_data_path : raise argparse . ArgumentTypeError ( \"If --load is 'datatracker', --master_data_path must be specified.\" ) else : self . validate_path ( 'master_data_path' , master_data_path , must_exists = True , must_ends_with = '.xlsx' ) # Validate and set paths self . validate_path ( 'input_path' , input_path , must_exists = True ) self . validate_path ( 'output_network_path' , output_path ) self . validate_path ( 'gdb_path' , gdb_path , must_ends_with = '.gdb' ) # Build the paths datatracker = os . path . join ( self . local_dir , datatracker ) attachments = os . path . join ( self . local_dir , attachments ) self . input = input_path self . output = output_path self . gdb_path = gdb_path self . local_gdb_path = os . path . join ( self . local_dir , os . path . basename ( self . gdb_path )) self . load_from = load_from self . save_to = save_to self . datatracker = datatracker self . attachments = attachments self . log = log_path self . debug = debug self . resume = resume self . suppress = suppress self . ps_script = ps_script self . project_numbers = self . get_project_numbers ( master_data_path ) def validate_path ( self , argument : str , param : str , must_exists : bool = False , must_ends_with : bool = None ) -> None : \"\"\" Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. \"\"\" if not isinstance ( param , str ) or not param . strip (): raise ValueError ( f ' { argument } : { param } must be a non-empty string.' ) if must_ends_with is not None and not param . endswith ( must_ends_with ): raise ValueError ( f ' { argument } : { param } must be of type { must_ends_with } .' ) if must_exists and not os . path . exists ( param ): raise ValueError ( f ' { argument } : { param } path does not exist.' ) def handle_unzip ( self ) -> None : \"\"\" Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return ripple_unzip ( self . input , self . output , self . log ) def create_gdb ( self ) -> None : \"\"\" Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return # Create the .gdb if it does not already exists if not arcpy . Exists ( self . local_gdb_path ): try : # Create the directory if it does not exist directory_path = os . path . dirname ( self . local_gdb_path ) if not os . path . exists ( directory_path ): os . makedirs ( directory_path ) # Create the file geodatabase file = os . path . basename ( self . local_gdb_path ) arcpy . management . CreateFileGDB ( directory_path , file ) log ( None , Colors . INFO , f 'Geodatabase: { file } created successfully' ) except arcpy . ExecuteError : log ( self . log , Colors . ERROR , arcpy . GetMessages ( 2 ), ps_script = self . ps_script ) def get_project_numbers ( self , master_datasheet : str = None ) -> list [ str ]: \"\"\" Get a list of project numbers from either the database or the master datasheet Returns: list[str]: A list of project numbers \"\"\" if self . load_from == 'datatracker' : masterdata = pd . read_excel ( master_datasheet ) # Extra validation on master data to check it has project number column if 'BT_Legacy_Project_ID__c' not in masterdata . columns : raise ValueError ( f \"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\" ) # Convert masterdata to a list of strings return masterdata [ 'BT_Legacy_Project_ID__c' ] . unique () . tolist () # Create database object database_connection = Database () # Read connection parameters from the configuration file database_parameters = database_connection . get_params () database_connection . connect ( database_parameters ) self . project_numbers = database_connection . read ( database_connection . schema , table = 'project_number' ) database_connection . disconnect () return [ str ( num [ 0 ]) for num in self . project_numbers ]","title":"Parameters"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.__init__","text":"Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , input_path : str , output_path : str , gdb_path : str , master_data_path : str , datatracker : str , attachments : str , load_from : str = 'database' , save_to : str = 'database' , log_path : str = None , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" Initializes the Parameters class with input parameters. Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" self . local_dir = r 'C:\\LocalTwoBillionToolkit' # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool. if attachments == '' : attachments = os . path . basename ( gdb_path ) . replace ( '.gdb' , '_Attachments' ) # Ensure that if a datatracker is specified for loading or saving, then a path must be passed if load_from == 'datatracker' or save_to == 'datatracker' : if not datatracker : raise argparse . ArgumentTypeError ( \"If --load or --save is 'datatracker', --datatracker must be specified.\" ) else : self . validate_path ( 'datatracker' , datatracker , must_ends_with = '.xlsx' ) if load_from == 'datatracker' : if not master_data_path : raise argparse . ArgumentTypeError ( \"If --load is 'datatracker', --master_data_path must be specified.\" ) else : self . validate_path ( 'master_data_path' , master_data_path , must_exists = True , must_ends_with = '.xlsx' ) # Validate and set paths self . validate_path ( 'input_path' , input_path , must_exists = True ) self . validate_path ( 'output_network_path' , output_path ) self . validate_path ( 'gdb_path' , gdb_path , must_ends_with = '.gdb' ) # Build the paths datatracker = os . path . join ( self . local_dir , datatracker ) attachments = os . path . join ( self . local_dir , attachments ) self . input = input_path self . output = output_path self . gdb_path = gdb_path self . local_gdb_path = os . path . join ( self . local_dir , os . path . basename ( self . gdb_path )) self . load_from = load_from self . save_to = save_to self . datatracker = datatracker self . attachments = attachments self . log = log_path self . debug = debug self . resume = resume self . suppress = suppress self . ps_script = ps_script self . project_numbers = self . get_project_numbers ( master_data_path )","title":"__init__"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.create_gdb","text":"Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 def create_gdb ( self ) -> None : \"\"\" Creates a geodatabase file if it does not already exist. If the specified geodatabase file does not exist, it attempts to create one. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return # Create the .gdb if it does not already exists if not arcpy . Exists ( self . local_gdb_path ): try : # Create the directory if it does not exist directory_path = os . path . dirname ( self . local_gdb_path ) if not os . path . exists ( directory_path ): os . makedirs ( directory_path ) # Create the file geodatabase file = os . path . basename ( self . local_gdb_path ) arcpy . management . CreateFileGDB ( directory_path , file ) log ( None , Colors . INFO , f 'Geodatabase: { file } created successfully' ) except arcpy . ExecuteError : log ( self . log , Colors . ERROR , arcpy . GetMessages ( 2 ), ps_script = self . ps_script )","title":"create_gdb"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.get_project_numbers","text":"Get a list of project numbers from either the database or the master datasheet Returns: list [ str ] \u2013 list[str]: A list of project numbers Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def get_project_numbers ( self , master_datasheet : str = None ) -> list [ str ]: \"\"\" Get a list of project numbers from either the database or the master datasheet Returns: list[str]: A list of project numbers \"\"\" if self . load_from == 'datatracker' : masterdata = pd . read_excel ( master_datasheet ) # Extra validation on master data to check it has project number column if 'BT_Legacy_Project_ID__c' not in masterdata . columns : raise ValueError ( f \"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\" ) # Convert masterdata to a list of strings return masterdata [ 'BT_Legacy_Project_ID__c' ] . unique () . tolist () # Create database object database_connection = Database () # Read connection parameters from the configuration file database_parameters = database_connection . get_params () database_connection . connect ( database_parameters ) self . project_numbers = database_connection . read ( database_connection . schema , table = 'project_number' ) database_connection . disconnect () return [ str ( num [ 0 ]) for num in self . project_numbers ]","title":"get_project_numbers"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.handle_unzip","text":"Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 102 103 104 105 106 107 108 109 110 111 112 def handle_unzip ( self ) -> None : \"\"\" Handles the unzipping process using ripple_unzip. Calls the ripple_unzip function with input, output, and log paths. \"\"\" # If the resume after crash flag was specified, skip if self . resume : return ripple_unzip ( self . input , self . output , self . log )","title":"handle_unzip"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.validate_path","text":"Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. Source code in twobilliontoolkit\\SpatialTransformer\\Parameters.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def validate_path ( self , argument : str , param : str , must_exists : bool = False , must_ends_with : bool = None ) -> None : \"\"\" Validates the given path. Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string. Raises: - ValueError: If the path is not valid according to the specified conditions. \"\"\" if not isinstance ( param , str ) or not param . strip (): raise ValueError ( f ' { argument } : { param } must be a non-empty string.' ) if must_ends_with is not None and not param . endswith ( must_ends_with ): raise ValueError ( f ' { argument } : { param } must be of type { must_ends_with } .' ) if must_exists and not os . path . exists ( param ): raise ValueError ( f ' { argument } : { param } path does not exist.' )","title":"validate_path"},{"location":"pages/SpatialTransformer/Processor/","text":"SpatialTransformer.Processor Processor Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 class Processor : def __init__ ( self , params : Parameters ) -> None : \"\"\" Initializes the Processor class with input parameters. Args: params (Parameters): Instance of the Parameters class. \"\"\" self . params = params # Create the Data class to hold any data tracker information self . data = Datatracker2BT ( params . datatracker , params . load_from , params . save_to ) def del_gdb ( self ) -> None : \"\"\" Removes the local Geodatabase and folder after the processing has been completed. \"\"\" arcpy . Delete_management ( self . params . local_gdb_path ) arcpy . Delete_management ( self . params . local_dir ) def create_datatracker_entries ( self ) -> None : \"\"\" Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. \"\"\" # Step through unzip output path for root , dirs , files in os . walk ( self . params . output ): for dir in dirs : # Built full directory path directory_path = f \" { root } \\ { dir } \" # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( directory_path )) if data_entry : continue # Skip over the gdb if tool is running a second time if it is somehow in the folder if dir is self . params . gdb_path : continue if dir . endswith ( '.gdb' ): # Set the workspace to the specified .gdb arcpy . env . workspace = directory_path # Iterate through the feature classes and tables feature_tables = arcpy . ListTables () feature_classes = arcpy . ListFeatureClasses () for feature in feature_classes + feature_tables : if arcpy . Exists ( feature ) and arcpy . Describe ( feature ) . dataType == 'FeatureClass' : project_spatial_id = self . create_entry ( directory_path , f \" { directory_path } \\ { feature } \" ) # Remove the gdb from the dirs list so it doesnt walk through dirs . remove ( dir ) for file in files : # Built full file path file_path = f \" { root } \\ { file } \" # Ignore specified file extensions lowercase_file = file . lower () if lowercase_file . endswith ( IGNORE_EXTENSIONS ): continue # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( file_path )) if data_entry : continue if lowercase_file . endswith ( LAYOUT_FILE_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Layout file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( DATA_SHEET_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Datasheet: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( IMAGE_FILE_EXTENSIONS ): if lowercase_file . endswith ( '.pdf' ): project_spatial_id = self . create_entry ( file_path , file_path , contains_pdf = True , entry_type = 'Aspatial' , processed = True ) else : project_spatial_id = self . create_entry ( file_path , file_path , contains_image = True , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Image/PDF file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( '.shp' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.kml' , '.kmz' )): try : contain_point = False contain_polygon = False contain_linestring = False layers = fiona . listlayers ( file_path ) # Iterate through layers and check their geometry type for layer in layers : with fiona . open ( file_path , 'r' , driver = 'LIBKML' , layer = layer ) as src : for feat in src : if contain_point and contain_polygon and contain_linestring : break geom_type = feat . geometry . type if geom_type == 'Point' : contain_point = True elif geom_type == 'Polygon' : contain_polygon = True elif geom_type == 'LineString' : contain_linestring = True if contain_point : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Points\" ) if contain_polygon : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Polygons\" ) if contain_linestring : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Lines\" ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'KML/KMZ file: { file_path } has encountered an error when making a datatracker entry. { error } ' ) elif lowercase_file . endswith ( '.geojson' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.gpkg' , '.sqlite' )): project_spatial_id = self . create_entry ( file_path , file_path , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'GeoPackage/SQLite file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) else : # Log it log ( self . params . log , Colors . WARNING , f 'Unsupported Filetype: { file_path } has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.' , self . params . suppress , ps_script = self . params . ps_script ) def create_entry ( self , absolute_path : str , feature_path : str , in_raw_gdb : bool = False , contains_pdf : bool = False , contains_image : bool = False , entry_type : str = 'Spatial' , processed : bool = False ) -> str : \"\"\" Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Args: absolute_path (str): The absolute path of the input file. feature_path (str): The path to the feature data. in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf (bool): Indicates if the entry contains a PDF. Default is False. contains_image (bool): Indicates if the entry contains an image. Default is False. entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed (bool): Indicates if the entry has been processed. Default is False. Returns: str: The formatted project spatial ID. \"\"\" # Check project numbers and format the result formatted_result = self . check_project_numbers ( feature_path ) formatted_result = formatted_result . upper () # Create a unique identifier for the project spatial ID formatted_project_spatial_id = self . data . create_project_spatial_id ( formatted_result ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , feature_path ) log ( None , Colors . INFO , formatted_project_spatial_id ) # Convert the raw data path to a relative path raw_data_path = os . path . relpath ( feature_path , self . params . output ) # Convert the absolute path to the correct drive path format absolute_file_path = convert_drive_path ( absolute_path ) # Call a method to process raw data matching self . call_raw_data_match ( formatted_project_spatial_id , raw_data_path ) # Add data to the data class self . data . add_data ( project_spatial_id = formatted_project_spatial_id , project_number = formatted_result , dropped = False , raw_data_path = raw_data_path , raw_gdb_path = convert_drive_path ( self . params . gdb_path ), absolute_file_path = absolute_file_path , in_raw_gdb = in_raw_gdb , contains_pdf = contains_pdf , contains_image = contains_image , extracted_attachments_path = None , editor_tracking_enabled = False , processed = processed , entry_type = entry_type ) return formatted_project_spatial_id def process_entries ( self ) -> None : \"\"\" Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. \"\"\" # Iterate over each entry in the data dictionary for index , entry in enumerate ( self . data . data_dict ): # Get the absolute path of the entry file entry_absolute_path : str = self . data . data_dict [ entry ] . get ( 'absolute_file_path' ) try : # Check if the current entry has already been processed if self . data . data_dict [ entry ] . get ( 'processed' ): continue # Check if the created_by date of the entry is within the same day as now created_at = self . data . data_dict [ entry ] . get ( 'created_at' ) if self . params . load_from == 'database' and created_at : now = datetime . datetime . now () if created_at . date () != now . date (): continue # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase gdb_entry_name = f \"proj_ { entry } \" feature_gdb_path = os . path . join ( self . params . local_gdb_path , gdb_entry_name ) # Check the file type and export features accordingly if entry_absolute_path . endswith ( '.gdb' ): # Export features from one geodatabase to the output geodatabase arcpy . conversion . ExportFeatures ( os . path . join ( self . params . output , self . data . data_dict [ entry ] . get ( 'raw_data_path' )), feature_gdb_path ) elif entry_absolute_path . endswith ( '.shp' ): # Export features from shapefile to the output geodatabase arcpy . conversion . ExportFeatures ( entry_absolute_path , feature_gdb_path ) elif entry_absolute_path . endswith (( '.kml' , '.kmz' )): # Initialize an empty GeoDataFrame data = gpd . GeoDataFrame () # List all layers in the KML/KMZ file layers = fiona . listlayers ( entry_absolute_path ) for layer in layers : # Read each layer into a temporary GeoDataFrame layer_gdb = gpd . read_file ( entry_absolute_path , driver = 'LIBKML' , layer = layer ) # Check if the temporary GeoDataFrame is empty before concatenating if not layer_gdb . empty : data = pd . concat ([ data , layer_gdb ], ignore_index = True ) # Rename the 'OBJECTID' column if it exists if 'OBJECTID' in data . columns : data . rename ( columns = { 'OBJECTID' : 'OBJECTID_STRING' }, inplace = True ) # Convert all datetime columns to string with specified utc=True for col in data . select_dtypes ( include = [ 'datetime64[ns]' , 'datetime64[ns, UTC]' ]) . columns : data [ col ] = pd . to_datetime ( data [ col ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Check if 'timestamp' column exists if 'timestamp' in data . columns : data [ 'timestamp' ] = pd . to_datetime ( data [ 'timestamp' ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Determine the path for raw data and entry basename raw_data_path = self . data . data_dict [ entry ] . get ( 'raw_data_path' ) entry_data_basename = os . path . basename ( raw_data_path ) # Export the GeoDataFrame to the appropriate feature class based on geometry type if entry_data_basename == 'Points' : points_gdf = data [ data . geometry . type == 'Point' ] points_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Polygons' : polygons_gdf = data [ data . geometry . type == 'Polygon' ] polygons_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Lines' : lines_gdf = data [ data . geometry . type == 'LineString' ] lines_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_absolute_path . endswith ( '.geojson' ): # Export features from GeoJSON to the output geodatabase arcpy . conversion . JSONToFeatures ( entry_absolute_path , feature_gdb_path ) # Update the entry status to indicate it has been processed and exists in raw geodatabase format self . data . set_data ( project_spatial_id = entry , in_raw_gdb = True , processed = True ) # Enable the version control of the layer in the geodatabase self . enable_version_control ( feature_gdb_path ) # Update the entry status to indicate that editor tracking is enabled self . data . set_data ( project_spatial_id = entry , editor_tracking_enabled = True ) except ( arcpy . ExecuteError , arcgisscripting . ExecuteError ) as error : log ( self . params . log , Colors . ERROR , f 'An error occurred when processing the layer for { entry_absolute_path } , you can fix or remove it from the datatracker/database, then run the command again with --resume \\n { error } ' , ps_script = self . params . ps_script , project_id = entry ) # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on # raise Exception(error) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An uncaught error occurred when processing the layer for { entry_absolute_path } ' , ps_script = self . params . ps_script , project_id = entry ) raise Exception ( error ) def check_project_numbers ( self , file_path : str ) -> str : \"\"\" Check project numbers against a master data sheet. Args: file_path (str): Filepath to check. Returns: str: the formatted result for the project number of the spatial file path. \"\"\" # Define a regular expression pattern to extract project numbers and search for pattern in file path pattern = r '(\\d {4} )[\\s_\u2013-]*([a-zA-Z] {3} )[\\s_\u2013-]*(\\d {3} )' search = re . search ( pattern , file_path ) # If no match is found, log a warning and assign an arbitrary project number if not search : log ( self . params . log , Colors . WARNING , f 'Could not find a project number for: { file_path } - Giving it an arbitrary project number \"0000 XXX - 000\"' , self . params . suppress , ps_script = self . params . ps_script ) formatted_result = '0000 XXX - 000' else : # Format the result using the matched groups formatted_result = ' {} {} - {} ' . format ( search . group ( 1 ), search . group ( 2 ), search . group ( 3 )) # Check if the project number is in the project numbers list project_found = None for project_number in self . params . project_numbers : if project_number . lower () . replace ( ' ' , '' ) == formatted_result . lower () . replace ( ' ' , '' ): # Exit the loop as soon as the first occurrence is found project_found = project_number break if not project_found : log ( self . params . log , Colors . WARNING , f 'The project number { formatted_result } does not match any know project number in the master datasheet' , self . params . suppress , ps_script = self . params . ps_script ) return formatted_result def call_raw_data_match ( self , current_spatial_id : str , raw_data_path : str ) -> None : \"\"\" Call the method that finds a matching raw data path and returns the project spatial id. Args: current_spatial_id (str): The current spatial project id being checked. raw_data_path (str): The raw data path to be searched in the dictionary. \"\"\" # Find a corresponding project spatial ID in the data dictionary based on the raw data path ( found_match , _ ) = self . data . find_matching_data ( raw_data_path = raw_data_path ) if found_match is not None : log ( self . params . log , Colors . WARNING , f 'Raw path: { raw_data_path } already exists in the data tracker! - Current Spatial ID: { current_spatial_id } Matching Spatial ID: { found_match } ' , self . params . suppress , ps_script = self . params . ps_script , project_id = current_spatial_id ) def extract_attachments ( self ) -> None : \"\"\" Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. \"\"\" # Find and process attachments from the gdb attachment_dict = find_attachments ( self . params . local_gdb_path , self . params . attachments ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , attachment_dict ) # Iterating through key-value pairs using items() for key , value in attachment_dict . items (): # Update the entry of the Geodatabase feature class self . data . set_data ( project_spatial_id = key . replace ( 'proj_' , '' ), # extracted_attachments_path=value extracted_attachments_path = os . path . join ( self . params . gdb_path , value . replace ( f \"C:\\LocalTwoBillionToolkit \\\\ \" , '' )) ) # Log completion of this task log ( None , Colors . INFO , 'All attachments have been extracted from the result Geodatabase.' ) def enable_version_control ( self , feature_class ) -> None : \"\"\" Enable the editor tracking version control for a feature class in the Geodatabase. \"\"\" try : # Set the arc environement to the resulting GDB arcpy . env . workspace = self . params . local_gdb_path # Add a site id for mapping in a later tool arcpy . management . AddField ( feature_class , 'bt_site_id' , 'SHORT' ) # Enable the 4 fields for editor tracking arcpy . EnableEditorTracking_management ( feature_class , \"bt_created_by\" , \"bt_date_created\" , \"bt_last_edited_by\" , \"bt_date_edited\" , \"ADD_FIELDS\" , \"UTC\" ) # Set flag in data object for editor tracking to True self . data . set_data ( project_spatial_id = os . path . basename ( feature_class ) . replace ( 'proj_' , '' ), editor_tracking_enabled = True ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An error has been caught while trying to enable editor tracking for { feature_class } in resulting gdb, { error } ' , ps_script = self . params . ps_script ) __init__ ( params ) Initializes the Processor class with input parameters. Parameters: params ( Parameters ) \u2013 Instance of the Parameters class. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , params : Parameters ) -> None : \"\"\" Initializes the Processor class with input parameters. Args: params (Parameters): Instance of the Parameters class. \"\"\" self . params = params # Create the Data class to hold any data tracker information self . data = Datatracker2BT ( params . datatracker , params . load_from , params . save_to ) call_raw_data_match ( current_spatial_id , raw_data_path ) Call the method that finds a matching raw data path and returns the project spatial id. Parameters: current_spatial_id ( str ) \u2013 The current spatial project id being checked. raw_data_path ( str ) \u2013 The raw data path to be searched in the dictionary. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 372 373 374 375 376 377 378 379 380 381 382 383 def call_raw_data_match ( self , current_spatial_id : str , raw_data_path : str ) -> None : \"\"\" Call the method that finds a matching raw data path and returns the project spatial id. Args: current_spatial_id (str): The current spatial project id being checked. raw_data_path (str): The raw data path to be searched in the dictionary. \"\"\" # Find a corresponding project spatial ID in the data dictionary based on the raw data path ( found_match , _ ) = self . data . find_matching_data ( raw_data_path = raw_data_path ) if found_match is not None : log ( self . params . log , Colors . WARNING , f 'Raw path: { raw_data_path } already exists in the data tracker! - Current Spatial ID: { current_spatial_id } Matching Spatial ID: { found_match } ' , self . params . suppress , ps_script = self . params . ps_script , project_id = current_spatial_id ) check_project_numbers ( file_path ) Check project numbers against a master data sheet. Parameters: file_path ( str ) \u2013 Filepath to check. Returns: str ( str ) \u2013 the formatted result for the project number of the spatial file path. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def check_project_numbers ( self , file_path : str ) -> str : \"\"\" Check project numbers against a master data sheet. Args: file_path (str): Filepath to check. Returns: str: the formatted result for the project number of the spatial file path. \"\"\" # Define a regular expression pattern to extract project numbers and search for pattern in file path pattern = r '(\\d {4} )[\\s_\u2013-]*([a-zA-Z] {3} )[\\s_\u2013-]*(\\d {3} )' search = re . search ( pattern , file_path ) # If no match is found, log a warning and assign an arbitrary project number if not search : log ( self . params . log , Colors . WARNING , f 'Could not find a project number for: { file_path } - Giving it an arbitrary project number \"0000 XXX - 000\"' , self . params . suppress , ps_script = self . params . ps_script ) formatted_result = '0000 XXX - 000' else : # Format the result using the matched groups formatted_result = ' {} {} - {} ' . format ( search . group ( 1 ), search . group ( 2 ), search . group ( 3 )) # Check if the project number is in the project numbers list project_found = None for project_number in self . params . project_numbers : if project_number . lower () . replace ( ' ' , '' ) == formatted_result . lower () . replace ( ' ' , '' ): # Exit the loop as soon as the first occurrence is found project_found = project_number break if not project_found : log ( self . params . log , Colors . WARNING , f 'The project number { formatted_result } does not match any know project number in the master datasheet' , self . params . suppress , ps_script = self . params . ps_script ) return formatted_result create_datatracker_entries () Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def create_datatracker_entries ( self ) -> None : \"\"\" Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. \"\"\" # Step through unzip output path for root , dirs , files in os . walk ( self . params . output ): for dir in dirs : # Built full directory path directory_path = f \" { root } \\ { dir } \" # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( directory_path )) if data_entry : continue # Skip over the gdb if tool is running a second time if it is somehow in the folder if dir is self . params . gdb_path : continue if dir . endswith ( '.gdb' ): # Set the workspace to the specified .gdb arcpy . env . workspace = directory_path # Iterate through the feature classes and tables feature_tables = arcpy . ListTables () feature_classes = arcpy . ListFeatureClasses () for feature in feature_classes + feature_tables : if arcpy . Exists ( feature ) and arcpy . Describe ( feature ) . dataType == 'FeatureClass' : project_spatial_id = self . create_entry ( directory_path , f \" { directory_path } \\ { feature } \" ) # Remove the gdb from the dirs list so it doesnt walk through dirs . remove ( dir ) for file in files : # Built full file path file_path = f \" { root } \\ { file } \" # Ignore specified file extensions lowercase_file = file . lower () if lowercase_file . endswith ( IGNORE_EXTENSIONS ): continue # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( file_path )) if data_entry : continue if lowercase_file . endswith ( LAYOUT_FILE_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Layout file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( DATA_SHEET_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Datasheet: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( IMAGE_FILE_EXTENSIONS ): if lowercase_file . endswith ( '.pdf' ): project_spatial_id = self . create_entry ( file_path , file_path , contains_pdf = True , entry_type = 'Aspatial' , processed = True ) else : project_spatial_id = self . create_entry ( file_path , file_path , contains_image = True , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Image/PDF file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( '.shp' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.kml' , '.kmz' )): try : contain_point = False contain_polygon = False contain_linestring = False layers = fiona . listlayers ( file_path ) # Iterate through layers and check their geometry type for layer in layers : with fiona . open ( file_path , 'r' , driver = 'LIBKML' , layer = layer ) as src : for feat in src : if contain_point and contain_polygon and contain_linestring : break geom_type = feat . geometry . type if geom_type == 'Point' : contain_point = True elif geom_type == 'Polygon' : contain_polygon = True elif geom_type == 'LineString' : contain_linestring = True if contain_point : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Points\" ) if contain_polygon : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Polygons\" ) if contain_linestring : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Lines\" ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'KML/KMZ file: { file_path } has encountered an error when making a datatracker entry. { error } ' ) elif lowercase_file . endswith ( '.geojson' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.gpkg' , '.sqlite' )): project_spatial_id = self . create_entry ( file_path , file_path , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'GeoPackage/SQLite file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) else : # Log it log ( self . params . log , Colors . WARNING , f 'Unsupported Filetype: { file_path } has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.' , self . params . suppress , ps_script = self . params . ps_script ) create_entry ( absolute_path , feature_path , in_raw_gdb = False , contains_pdf = False , contains_image = False , entry_type = 'Spatial' , processed = False ) Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Parameters: absolute_path ( str ) \u2013 The absolute path of the input file. feature_path ( str ) \u2013 The path to the feature data. in_raw_gdb ( bool , default: False ) \u2013 Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf ( bool , default: False ) \u2013 Indicates if the entry contains a PDF. Default is False. contains_image ( bool , default: False ) \u2013 Indicates if the entry contains an image. Default is False. entry_type ( str , default: 'Spatial' ) \u2013 The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed ( bool , default: False ) \u2013 Indicates if the entry has been processed. Default is False. Returns: str ( str ) \u2013 The formatted project spatial ID. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def create_entry ( self , absolute_path : str , feature_path : str , in_raw_gdb : bool = False , contains_pdf : bool = False , contains_image : bool = False , entry_type : str = 'Spatial' , processed : bool = False ) -> str : \"\"\" Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Args: absolute_path (str): The absolute path of the input file. feature_path (str): The path to the feature data. in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf (bool): Indicates if the entry contains a PDF. Default is False. contains_image (bool): Indicates if the entry contains an image. Default is False. entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed (bool): Indicates if the entry has been processed. Default is False. Returns: str: The formatted project spatial ID. \"\"\" # Check project numbers and format the result formatted_result = self . check_project_numbers ( feature_path ) formatted_result = formatted_result . upper () # Create a unique identifier for the project spatial ID formatted_project_spatial_id = self . data . create_project_spatial_id ( formatted_result ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , feature_path ) log ( None , Colors . INFO , formatted_project_spatial_id ) # Convert the raw data path to a relative path raw_data_path = os . path . relpath ( feature_path , self . params . output ) # Convert the absolute path to the correct drive path format absolute_file_path = convert_drive_path ( absolute_path ) # Call a method to process raw data matching self . call_raw_data_match ( formatted_project_spatial_id , raw_data_path ) # Add data to the data class self . data . add_data ( project_spatial_id = formatted_project_spatial_id , project_number = formatted_result , dropped = False , raw_data_path = raw_data_path , raw_gdb_path = convert_drive_path ( self . params . gdb_path ), absolute_file_path = absolute_file_path , in_raw_gdb = in_raw_gdb , contains_pdf = contains_pdf , contains_image = contains_image , extracted_attachments_path = None , editor_tracking_enabled = False , processed = processed , entry_type = entry_type ) return formatted_project_spatial_id del_gdb () Removes the local Geodatabase and folder after the processing has been completed. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 39 40 41 42 43 44 def del_gdb ( self ) -> None : \"\"\" Removes the local Geodatabase and folder after the processing has been completed. \"\"\" arcpy . Delete_management ( self . params . local_gdb_path ) arcpy . Delete_management ( self . params . local_dir ) enable_version_control ( feature_class ) Enable the editor tracking version control for a feature class in the Geodatabase. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def enable_version_control ( self , feature_class ) -> None : \"\"\" Enable the editor tracking version control for a feature class in the Geodatabase. \"\"\" try : # Set the arc environement to the resulting GDB arcpy . env . workspace = self . params . local_gdb_path # Add a site id for mapping in a later tool arcpy . management . AddField ( feature_class , 'bt_site_id' , 'SHORT' ) # Enable the 4 fields for editor tracking arcpy . EnableEditorTracking_management ( feature_class , \"bt_created_by\" , \"bt_date_created\" , \"bt_last_edited_by\" , \"bt_date_edited\" , \"ADD_FIELDS\" , \"UTC\" ) # Set flag in data object for editor tracking to True self . data . set_data ( project_spatial_id = os . path . basename ( feature_class ) . replace ( 'proj_' , '' ), editor_tracking_enabled = True ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An error has been caught while trying to enable editor tracking for { feature_class } in resulting gdb, { error } ' , ps_script = self . params . ps_script ) extract_attachments () Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def extract_attachments ( self ) -> None : \"\"\" Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. \"\"\" # Find and process attachments from the gdb attachment_dict = find_attachments ( self . params . local_gdb_path , self . params . attachments ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , attachment_dict ) # Iterating through key-value pairs using items() for key , value in attachment_dict . items (): # Update the entry of the Geodatabase feature class self . data . set_data ( project_spatial_id = key . replace ( 'proj_' , '' ), # extracted_attachments_path=value extracted_attachments_path = os . path . join ( self . params . gdb_path , value . replace ( f \"C:\\LocalTwoBillionToolkit \\\\ \" , '' )) ) # Log completion of this task log ( None , Colors . INFO , 'All attachments have been extracted from the result Geodatabase.' ) process_entries () Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def process_entries ( self ) -> None : \"\"\" Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. \"\"\" # Iterate over each entry in the data dictionary for index , entry in enumerate ( self . data . data_dict ): # Get the absolute path of the entry file entry_absolute_path : str = self . data . data_dict [ entry ] . get ( 'absolute_file_path' ) try : # Check if the current entry has already been processed if self . data . data_dict [ entry ] . get ( 'processed' ): continue # Check if the created_by date of the entry is within the same day as now created_at = self . data . data_dict [ entry ] . get ( 'created_at' ) if self . params . load_from == 'database' and created_at : now = datetime . datetime . now () if created_at . date () != now . date (): continue # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase gdb_entry_name = f \"proj_ { entry } \" feature_gdb_path = os . path . join ( self . params . local_gdb_path , gdb_entry_name ) # Check the file type and export features accordingly if entry_absolute_path . endswith ( '.gdb' ): # Export features from one geodatabase to the output geodatabase arcpy . conversion . ExportFeatures ( os . path . join ( self . params . output , self . data . data_dict [ entry ] . get ( 'raw_data_path' )), feature_gdb_path ) elif entry_absolute_path . endswith ( '.shp' ): # Export features from shapefile to the output geodatabase arcpy . conversion . ExportFeatures ( entry_absolute_path , feature_gdb_path ) elif entry_absolute_path . endswith (( '.kml' , '.kmz' )): # Initialize an empty GeoDataFrame data = gpd . GeoDataFrame () # List all layers in the KML/KMZ file layers = fiona . listlayers ( entry_absolute_path ) for layer in layers : # Read each layer into a temporary GeoDataFrame layer_gdb = gpd . read_file ( entry_absolute_path , driver = 'LIBKML' , layer = layer ) # Check if the temporary GeoDataFrame is empty before concatenating if not layer_gdb . empty : data = pd . concat ([ data , layer_gdb ], ignore_index = True ) # Rename the 'OBJECTID' column if it exists if 'OBJECTID' in data . columns : data . rename ( columns = { 'OBJECTID' : 'OBJECTID_STRING' }, inplace = True ) # Convert all datetime columns to string with specified utc=True for col in data . select_dtypes ( include = [ 'datetime64[ns]' , 'datetime64[ns, UTC]' ]) . columns : data [ col ] = pd . to_datetime ( data [ col ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Check if 'timestamp' column exists if 'timestamp' in data . columns : data [ 'timestamp' ] = pd . to_datetime ( data [ 'timestamp' ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Determine the path for raw data and entry basename raw_data_path = self . data . data_dict [ entry ] . get ( 'raw_data_path' ) entry_data_basename = os . path . basename ( raw_data_path ) # Export the GeoDataFrame to the appropriate feature class based on geometry type if entry_data_basename == 'Points' : points_gdf = data [ data . geometry . type == 'Point' ] points_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Polygons' : polygons_gdf = data [ data . geometry . type == 'Polygon' ] polygons_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Lines' : lines_gdf = data [ data . geometry . type == 'LineString' ] lines_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_absolute_path . endswith ( '.geojson' ): # Export features from GeoJSON to the output geodatabase arcpy . conversion . JSONToFeatures ( entry_absolute_path , feature_gdb_path ) # Update the entry status to indicate it has been processed and exists in raw geodatabase format self . data . set_data ( project_spatial_id = entry , in_raw_gdb = True , processed = True ) # Enable the version control of the layer in the geodatabase self . enable_version_control ( feature_gdb_path ) # Update the entry status to indicate that editor tracking is enabled self . data . set_data ( project_spatial_id = entry , editor_tracking_enabled = True ) except ( arcpy . ExecuteError , arcgisscripting . ExecuteError ) as error : log ( self . params . log , Colors . ERROR , f 'An error occurred when processing the layer for { entry_absolute_path } , you can fix or remove it from the datatracker/database, then run the command again with --resume \\n { error } ' , ps_script = self . params . ps_script , project_id = entry ) # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on # raise Exception(error) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An uncaught error occurred when processing the layer for { entry_absolute_path } ' , ps_script = self . params . ps_script , project_id = entry ) raise Exception ( error ) convert_drive_path ( file_path ) Converts a path with a mapped drive (ie. M:\\, V:) to the actual network drive name. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def convert_drive_path ( file_path ): \"\"\" Converts a path with a mapped drive (ie. M:\\, V:\\) to the actual network drive name. \"\"\" abs_file_path = os . path . abspath ( file_path ) actual_drive_path = abs_file_path try : # Extract drive letter from absolute file path drive_letter , _ = os . path . splitdrive ( abs_file_path ) # Check if the path contains a drive letter if re . match ( r \"[A-Za-z] {1} : {1} \" , drive_letter ): # Convert mapped drive path to UNC path actual_drive_path = win32wnet . WNetGetUniversalName ( actual_drive_path , 1 ) except Exception as e : print ( f \"Error: { e } \" ) return actual_drive_path","title":"Processor"},{"location":"pages/SpatialTransformer/Processor/#spatialtransformerprocessor","text":"","title":"SpatialTransformer.Processor"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor","text":"Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 class Processor : def __init__ ( self , params : Parameters ) -> None : \"\"\" Initializes the Processor class with input parameters. Args: params (Parameters): Instance of the Parameters class. \"\"\" self . params = params # Create the Data class to hold any data tracker information self . data = Datatracker2BT ( params . datatracker , params . load_from , params . save_to ) def del_gdb ( self ) -> None : \"\"\" Removes the local Geodatabase and folder after the processing has been completed. \"\"\" arcpy . Delete_management ( self . params . local_gdb_path ) arcpy . Delete_management ( self . params . local_dir ) def create_datatracker_entries ( self ) -> None : \"\"\" Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. \"\"\" # Step through unzip output path for root , dirs , files in os . walk ( self . params . output ): for dir in dirs : # Built full directory path directory_path = f \" { root } \\ { dir } \" # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( directory_path )) if data_entry : continue # Skip over the gdb if tool is running a second time if it is somehow in the folder if dir is self . params . gdb_path : continue if dir . endswith ( '.gdb' ): # Set the workspace to the specified .gdb arcpy . env . workspace = directory_path # Iterate through the feature classes and tables feature_tables = arcpy . ListTables () feature_classes = arcpy . ListFeatureClasses () for feature in feature_classes + feature_tables : if arcpy . Exists ( feature ) and arcpy . Describe ( feature ) . dataType == 'FeatureClass' : project_spatial_id = self . create_entry ( directory_path , f \" { directory_path } \\ { feature } \" ) # Remove the gdb from the dirs list so it doesnt walk through dirs . remove ( dir ) for file in files : # Built full file path file_path = f \" { root } \\ { file } \" # Ignore specified file extensions lowercase_file = file . lower () if lowercase_file . endswith ( IGNORE_EXTENSIONS ): continue # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( file_path )) if data_entry : continue if lowercase_file . endswith ( LAYOUT_FILE_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Layout file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( DATA_SHEET_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Datasheet: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( IMAGE_FILE_EXTENSIONS ): if lowercase_file . endswith ( '.pdf' ): project_spatial_id = self . create_entry ( file_path , file_path , contains_pdf = True , entry_type = 'Aspatial' , processed = True ) else : project_spatial_id = self . create_entry ( file_path , file_path , contains_image = True , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Image/PDF file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( '.shp' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.kml' , '.kmz' )): try : contain_point = False contain_polygon = False contain_linestring = False layers = fiona . listlayers ( file_path ) # Iterate through layers and check their geometry type for layer in layers : with fiona . open ( file_path , 'r' , driver = 'LIBKML' , layer = layer ) as src : for feat in src : if contain_point and contain_polygon and contain_linestring : break geom_type = feat . geometry . type if geom_type == 'Point' : contain_point = True elif geom_type == 'Polygon' : contain_polygon = True elif geom_type == 'LineString' : contain_linestring = True if contain_point : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Points\" ) if contain_polygon : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Polygons\" ) if contain_linestring : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Lines\" ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'KML/KMZ file: { file_path } has encountered an error when making a datatracker entry. { error } ' ) elif lowercase_file . endswith ( '.geojson' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.gpkg' , '.sqlite' )): project_spatial_id = self . create_entry ( file_path , file_path , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'GeoPackage/SQLite file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) else : # Log it log ( self . params . log , Colors . WARNING , f 'Unsupported Filetype: { file_path } has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.' , self . params . suppress , ps_script = self . params . ps_script ) def create_entry ( self , absolute_path : str , feature_path : str , in_raw_gdb : bool = False , contains_pdf : bool = False , contains_image : bool = False , entry_type : str = 'Spatial' , processed : bool = False ) -> str : \"\"\" Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Args: absolute_path (str): The absolute path of the input file. feature_path (str): The path to the feature data. in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf (bool): Indicates if the entry contains a PDF. Default is False. contains_image (bool): Indicates if the entry contains an image. Default is False. entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed (bool): Indicates if the entry has been processed. Default is False. Returns: str: The formatted project spatial ID. \"\"\" # Check project numbers and format the result formatted_result = self . check_project_numbers ( feature_path ) formatted_result = formatted_result . upper () # Create a unique identifier for the project spatial ID formatted_project_spatial_id = self . data . create_project_spatial_id ( formatted_result ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , feature_path ) log ( None , Colors . INFO , formatted_project_spatial_id ) # Convert the raw data path to a relative path raw_data_path = os . path . relpath ( feature_path , self . params . output ) # Convert the absolute path to the correct drive path format absolute_file_path = convert_drive_path ( absolute_path ) # Call a method to process raw data matching self . call_raw_data_match ( formatted_project_spatial_id , raw_data_path ) # Add data to the data class self . data . add_data ( project_spatial_id = formatted_project_spatial_id , project_number = formatted_result , dropped = False , raw_data_path = raw_data_path , raw_gdb_path = convert_drive_path ( self . params . gdb_path ), absolute_file_path = absolute_file_path , in_raw_gdb = in_raw_gdb , contains_pdf = contains_pdf , contains_image = contains_image , extracted_attachments_path = None , editor_tracking_enabled = False , processed = processed , entry_type = entry_type ) return formatted_project_spatial_id def process_entries ( self ) -> None : \"\"\" Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. \"\"\" # Iterate over each entry in the data dictionary for index , entry in enumerate ( self . data . data_dict ): # Get the absolute path of the entry file entry_absolute_path : str = self . data . data_dict [ entry ] . get ( 'absolute_file_path' ) try : # Check if the current entry has already been processed if self . data . data_dict [ entry ] . get ( 'processed' ): continue # Check if the created_by date of the entry is within the same day as now created_at = self . data . data_dict [ entry ] . get ( 'created_at' ) if self . params . load_from == 'database' and created_at : now = datetime . datetime . now () if created_at . date () != now . date (): continue # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase gdb_entry_name = f \"proj_ { entry } \" feature_gdb_path = os . path . join ( self . params . local_gdb_path , gdb_entry_name ) # Check the file type and export features accordingly if entry_absolute_path . endswith ( '.gdb' ): # Export features from one geodatabase to the output geodatabase arcpy . conversion . ExportFeatures ( os . path . join ( self . params . output , self . data . data_dict [ entry ] . get ( 'raw_data_path' )), feature_gdb_path ) elif entry_absolute_path . endswith ( '.shp' ): # Export features from shapefile to the output geodatabase arcpy . conversion . ExportFeatures ( entry_absolute_path , feature_gdb_path ) elif entry_absolute_path . endswith (( '.kml' , '.kmz' )): # Initialize an empty GeoDataFrame data = gpd . GeoDataFrame () # List all layers in the KML/KMZ file layers = fiona . listlayers ( entry_absolute_path ) for layer in layers : # Read each layer into a temporary GeoDataFrame layer_gdb = gpd . read_file ( entry_absolute_path , driver = 'LIBKML' , layer = layer ) # Check if the temporary GeoDataFrame is empty before concatenating if not layer_gdb . empty : data = pd . concat ([ data , layer_gdb ], ignore_index = True ) # Rename the 'OBJECTID' column if it exists if 'OBJECTID' in data . columns : data . rename ( columns = { 'OBJECTID' : 'OBJECTID_STRING' }, inplace = True ) # Convert all datetime columns to string with specified utc=True for col in data . select_dtypes ( include = [ 'datetime64[ns]' , 'datetime64[ns, UTC]' ]) . columns : data [ col ] = pd . to_datetime ( data [ col ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Check if 'timestamp' column exists if 'timestamp' in data . columns : data [ 'timestamp' ] = pd . to_datetime ( data [ 'timestamp' ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Determine the path for raw data and entry basename raw_data_path = self . data . data_dict [ entry ] . get ( 'raw_data_path' ) entry_data_basename = os . path . basename ( raw_data_path ) # Export the GeoDataFrame to the appropriate feature class based on geometry type if entry_data_basename == 'Points' : points_gdf = data [ data . geometry . type == 'Point' ] points_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Polygons' : polygons_gdf = data [ data . geometry . type == 'Polygon' ] polygons_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Lines' : lines_gdf = data [ data . geometry . type == 'LineString' ] lines_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_absolute_path . endswith ( '.geojson' ): # Export features from GeoJSON to the output geodatabase arcpy . conversion . JSONToFeatures ( entry_absolute_path , feature_gdb_path ) # Update the entry status to indicate it has been processed and exists in raw geodatabase format self . data . set_data ( project_spatial_id = entry , in_raw_gdb = True , processed = True ) # Enable the version control of the layer in the geodatabase self . enable_version_control ( feature_gdb_path ) # Update the entry status to indicate that editor tracking is enabled self . data . set_data ( project_spatial_id = entry , editor_tracking_enabled = True ) except ( arcpy . ExecuteError , arcgisscripting . ExecuteError ) as error : log ( self . params . log , Colors . ERROR , f 'An error occurred when processing the layer for { entry_absolute_path } , you can fix or remove it from the datatracker/database, then run the command again with --resume \\n { error } ' , ps_script = self . params . ps_script , project_id = entry ) # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on # raise Exception(error) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An uncaught error occurred when processing the layer for { entry_absolute_path } ' , ps_script = self . params . ps_script , project_id = entry ) raise Exception ( error ) def check_project_numbers ( self , file_path : str ) -> str : \"\"\" Check project numbers against a master data sheet. Args: file_path (str): Filepath to check. Returns: str: the formatted result for the project number of the spatial file path. \"\"\" # Define a regular expression pattern to extract project numbers and search for pattern in file path pattern = r '(\\d {4} )[\\s_\u2013-]*([a-zA-Z] {3} )[\\s_\u2013-]*(\\d {3} )' search = re . search ( pattern , file_path ) # If no match is found, log a warning and assign an arbitrary project number if not search : log ( self . params . log , Colors . WARNING , f 'Could not find a project number for: { file_path } - Giving it an arbitrary project number \"0000 XXX - 000\"' , self . params . suppress , ps_script = self . params . ps_script ) formatted_result = '0000 XXX - 000' else : # Format the result using the matched groups formatted_result = ' {} {} - {} ' . format ( search . group ( 1 ), search . group ( 2 ), search . group ( 3 )) # Check if the project number is in the project numbers list project_found = None for project_number in self . params . project_numbers : if project_number . lower () . replace ( ' ' , '' ) == formatted_result . lower () . replace ( ' ' , '' ): # Exit the loop as soon as the first occurrence is found project_found = project_number break if not project_found : log ( self . params . log , Colors . WARNING , f 'The project number { formatted_result } does not match any know project number in the master datasheet' , self . params . suppress , ps_script = self . params . ps_script ) return formatted_result def call_raw_data_match ( self , current_spatial_id : str , raw_data_path : str ) -> None : \"\"\" Call the method that finds a matching raw data path and returns the project spatial id. Args: current_spatial_id (str): The current spatial project id being checked. raw_data_path (str): The raw data path to be searched in the dictionary. \"\"\" # Find a corresponding project spatial ID in the data dictionary based on the raw data path ( found_match , _ ) = self . data . find_matching_data ( raw_data_path = raw_data_path ) if found_match is not None : log ( self . params . log , Colors . WARNING , f 'Raw path: { raw_data_path } already exists in the data tracker! - Current Spatial ID: { current_spatial_id } Matching Spatial ID: { found_match } ' , self . params . suppress , ps_script = self . params . ps_script , project_id = current_spatial_id ) def extract_attachments ( self ) -> None : \"\"\" Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. \"\"\" # Find and process attachments from the gdb attachment_dict = find_attachments ( self . params . local_gdb_path , self . params . attachments ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , attachment_dict ) # Iterating through key-value pairs using items() for key , value in attachment_dict . items (): # Update the entry of the Geodatabase feature class self . data . set_data ( project_spatial_id = key . replace ( 'proj_' , '' ), # extracted_attachments_path=value extracted_attachments_path = os . path . join ( self . params . gdb_path , value . replace ( f \"C:\\LocalTwoBillionToolkit \\\\ \" , '' )) ) # Log completion of this task log ( None , Colors . INFO , 'All attachments have been extracted from the result Geodatabase.' ) def enable_version_control ( self , feature_class ) -> None : \"\"\" Enable the editor tracking version control for a feature class in the Geodatabase. \"\"\" try : # Set the arc environement to the resulting GDB arcpy . env . workspace = self . params . local_gdb_path # Add a site id for mapping in a later tool arcpy . management . AddField ( feature_class , 'bt_site_id' , 'SHORT' ) # Enable the 4 fields for editor tracking arcpy . EnableEditorTracking_management ( feature_class , \"bt_created_by\" , \"bt_date_created\" , \"bt_last_edited_by\" , \"bt_date_edited\" , \"ADD_FIELDS\" , \"UTC\" ) # Set flag in data object for editor tracking to True self . data . set_data ( project_spatial_id = os . path . basename ( feature_class ) . replace ( 'proj_' , '' ), editor_tracking_enabled = True ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An error has been caught while trying to enable editor tracking for { feature_class } in resulting gdb, { error } ' , ps_script = self . params . ps_script )","title":"Processor"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.__init__","text":"Initializes the Processor class with input parameters. Parameters: params ( Parameters ) \u2013 Instance of the Parameters class. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 27 28 29 30 31 32 33 34 35 36 37 def __init__ ( self , params : Parameters ) -> None : \"\"\" Initializes the Processor class with input parameters. Args: params (Parameters): Instance of the Parameters class. \"\"\" self . params = params # Create the Data class to hold any data tracker information self . data = Datatracker2BT ( params . datatracker , params . load_from , params . save_to )","title":"__init__"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.call_raw_data_match","text":"Call the method that finds a matching raw data path and returns the project spatial id. Parameters: current_spatial_id ( str ) \u2013 The current spatial project id being checked. raw_data_path ( str ) \u2013 The raw data path to be searched in the dictionary. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 372 373 374 375 376 377 378 379 380 381 382 383 def call_raw_data_match ( self , current_spatial_id : str , raw_data_path : str ) -> None : \"\"\" Call the method that finds a matching raw data path and returns the project spatial id. Args: current_spatial_id (str): The current spatial project id being checked. raw_data_path (str): The raw data path to be searched in the dictionary. \"\"\" # Find a corresponding project spatial ID in the data dictionary based on the raw data path ( found_match , _ ) = self . data . find_matching_data ( raw_data_path = raw_data_path ) if found_match is not None : log ( self . params . log , Colors . WARNING , f 'Raw path: { raw_data_path } already exists in the data tracker! - Current Spatial ID: { current_spatial_id } Matching Spatial ID: { found_match } ' , self . params . suppress , ps_script = self . params . ps_script , project_id = current_spatial_id )","title":"call_raw_data_match"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.check_project_numbers","text":"Check project numbers against a master data sheet. Parameters: file_path ( str ) \u2013 Filepath to check. Returns: str ( str ) \u2013 the formatted result for the project number of the spatial file path. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def check_project_numbers ( self , file_path : str ) -> str : \"\"\" Check project numbers against a master data sheet. Args: file_path (str): Filepath to check. Returns: str: the formatted result for the project number of the spatial file path. \"\"\" # Define a regular expression pattern to extract project numbers and search for pattern in file path pattern = r '(\\d {4} )[\\s_\u2013-]*([a-zA-Z] {3} )[\\s_\u2013-]*(\\d {3} )' search = re . search ( pattern , file_path ) # If no match is found, log a warning and assign an arbitrary project number if not search : log ( self . params . log , Colors . WARNING , f 'Could not find a project number for: { file_path } - Giving it an arbitrary project number \"0000 XXX - 000\"' , self . params . suppress , ps_script = self . params . ps_script ) formatted_result = '0000 XXX - 000' else : # Format the result using the matched groups formatted_result = ' {} {} - {} ' . format ( search . group ( 1 ), search . group ( 2 ), search . group ( 3 )) # Check if the project number is in the project numbers list project_found = None for project_number in self . params . project_numbers : if project_number . lower () . replace ( ' ' , '' ) == formatted_result . lower () . replace ( ' ' , '' ): # Exit the loop as soon as the first occurrence is found project_found = project_number break if not project_found : log ( self . params . log , Colors . WARNING , f 'The project number { formatted_result } does not match any know project number in the master datasheet' , self . params . suppress , ps_script = self . params . ps_script ) return formatted_result","title":"check_project_numbers"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.create_datatracker_entries","text":"Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def create_datatracker_entries ( self ) -> None : \"\"\" Creates data tracker entries by processing files and directories within the output path. This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker. \"\"\" # Step through unzip output path for root , dirs , files in os . walk ( self . params . output ): for dir in dirs : # Built full directory path directory_path = f \" { root } \\ { dir } \" # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( directory_path )) if data_entry : continue # Skip over the gdb if tool is running a second time if it is somehow in the folder if dir is self . params . gdb_path : continue if dir . endswith ( '.gdb' ): # Set the workspace to the specified .gdb arcpy . env . workspace = directory_path # Iterate through the feature classes and tables feature_tables = arcpy . ListTables () feature_classes = arcpy . ListFeatureClasses () for feature in feature_classes + feature_tables : if arcpy . Exists ( feature ) and arcpy . Describe ( feature ) . dataType == 'FeatureClass' : project_spatial_id = self . create_entry ( directory_path , f \" { directory_path } \\ { feature } \" ) # Remove the gdb from the dirs list so it doesnt walk through dirs . remove ( dir ) for file in files : # Built full file path file_path = f \" { root } \\ { file } \" # Ignore specified file extensions lowercase_file = file . lower () if lowercase_file . endswith ( IGNORE_EXTENSIONS ): continue # Skip over entiries if in resume mode if self . params . resume : ( _ , data_entry ) = self . data . find_matching_data ( absolute_file_path = convert_drive_path ( file_path )) if data_entry : continue if lowercase_file . endswith ( LAYOUT_FILE_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Layout file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( DATA_SHEET_EXTENSIONS ): project_spatial_id = self . create_entry ( file_path , file_path , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Datasheet: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( IMAGE_FILE_EXTENSIONS ): if lowercase_file . endswith ( '.pdf' ): project_spatial_id = self . create_entry ( file_path , file_path , contains_pdf = True , entry_type = 'Aspatial' , processed = True ) else : project_spatial_id = self . create_entry ( file_path , file_path , contains_image = True , entry_type = 'Aspatial' , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'Image/PDF file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) elif lowercase_file . endswith ( '.shp' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.kml' , '.kmz' )): try : contain_point = False contain_polygon = False contain_linestring = False layers = fiona . listlayers ( file_path ) # Iterate through layers and check their geometry type for layer in layers : with fiona . open ( file_path , 'r' , driver = 'LIBKML' , layer = layer ) as src : for feat in src : if contain_point and contain_polygon and contain_linestring : break geom_type = feat . geometry . type if geom_type == 'Point' : contain_point = True elif geom_type == 'Polygon' : contain_polygon = True elif geom_type == 'LineString' : contain_linestring = True if contain_point : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Points\" ) if contain_polygon : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Polygons\" ) if contain_linestring : project_spatial_id = self . create_entry ( file_path , f \" { file_path } \\Lines\" ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'KML/KMZ file: { file_path } has encountered an error when making a datatracker entry. { error } ' ) elif lowercase_file . endswith ( '.geojson' ): project_spatial_id = self . create_entry ( file_path , file_path ) elif lowercase_file . endswith (( '.gpkg' , '.sqlite' )): project_spatial_id = self . create_entry ( file_path , file_path , processed = True ) # Log it log ( self . params . log , Colors . WARNING , f 'GeoPackage/SQLite file: { file_path } will be added to data tracker but not resulting gdb.' , self . params . suppress , ps_script = self . params . ps_script , project_id = project_spatial_id ) else : # Log it log ( self . params . log , Colors . WARNING , f 'Unsupported Filetype: { file_path } has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.' , self . params . suppress , ps_script = self . params . ps_script )","title":"create_datatracker_entries"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.create_entry","text":"Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Parameters: absolute_path ( str ) \u2013 The absolute path of the input file. feature_path ( str ) \u2013 The path to the feature data. in_raw_gdb ( bool , default: False ) \u2013 Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf ( bool , default: False ) \u2013 Indicates if the entry contains a PDF. Default is False. contains_image ( bool , default: False ) \u2013 Indicates if the entry contains an image. Default is False. entry_type ( str , default: 'Spatial' ) \u2013 The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed ( bool , default: False ) \u2013 Indicates if the entry has been processed. Default is False. Returns: str ( str ) \u2013 The formatted project spatial ID. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def create_entry ( self , absolute_path : str , feature_path : str , in_raw_gdb : bool = False , contains_pdf : bool = False , contains_image : bool = False , entry_type : str = 'Spatial' , processed : bool = False ) -> str : \"\"\" Creates a new entry in the data dictionary for spatial data processing. This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes. Args: absolute_path (str): The absolute path of the input file. feature_path (str): The path to the feature data. in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False. contains_pdf (bool): Indicates if the entry contains a PDF. Default is False. contains_image (bool): Indicates if the entry contains an image. Default is False. entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'. processed (bool): Indicates if the entry has been processed. Default is False. Returns: str: The formatted project spatial ID. \"\"\" # Check project numbers and format the result formatted_result = self . check_project_numbers ( feature_path ) formatted_result = formatted_result . upper () # Create a unique identifier for the project spatial ID formatted_project_spatial_id = self . data . create_project_spatial_id ( formatted_result ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , feature_path ) log ( None , Colors . INFO , formatted_project_spatial_id ) # Convert the raw data path to a relative path raw_data_path = os . path . relpath ( feature_path , self . params . output ) # Convert the absolute path to the correct drive path format absolute_file_path = convert_drive_path ( absolute_path ) # Call a method to process raw data matching self . call_raw_data_match ( formatted_project_spatial_id , raw_data_path ) # Add data to the data class self . data . add_data ( project_spatial_id = formatted_project_spatial_id , project_number = formatted_result , dropped = False , raw_data_path = raw_data_path , raw_gdb_path = convert_drive_path ( self . params . gdb_path ), absolute_file_path = absolute_file_path , in_raw_gdb = in_raw_gdb , contains_pdf = contains_pdf , contains_image = contains_image , extracted_attachments_path = None , editor_tracking_enabled = False , processed = processed , entry_type = entry_type ) return formatted_project_spatial_id","title":"create_entry"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.del_gdb","text":"Removes the local Geodatabase and folder after the processing has been completed. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 39 40 41 42 43 44 def del_gdb ( self ) -> None : \"\"\" Removes the local Geodatabase and folder after the processing has been completed. \"\"\" arcpy . Delete_management ( self . params . local_gdb_path ) arcpy . Delete_management ( self . params . local_dir )","title":"del_gdb"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.enable_version_control","text":"Enable the editor tracking version control for a feature class in the Geodatabase. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def enable_version_control ( self , feature_class ) -> None : \"\"\" Enable the editor tracking version control for a feature class in the Geodatabase. \"\"\" try : # Set the arc environement to the resulting GDB arcpy . env . workspace = self . params . local_gdb_path # Add a site id for mapping in a later tool arcpy . management . AddField ( feature_class , 'bt_site_id' , 'SHORT' ) # Enable the 4 fields for editor tracking arcpy . EnableEditorTracking_management ( feature_class , \"bt_created_by\" , \"bt_date_created\" , \"bt_last_edited_by\" , \"bt_date_edited\" , \"ADD_FIELDS\" , \"UTC\" ) # Set flag in data object for editor tracking to True self . data . set_data ( project_spatial_id = os . path . basename ( feature_class ) . replace ( 'proj_' , '' ), editor_tracking_enabled = True ) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An error has been caught while trying to enable editor tracking for { feature_class } in resulting gdb, { error } ' , ps_script = self . params . ps_script )","title":"enable_version_control"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.extract_attachments","text":"Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 def extract_attachments ( self ) -> None : \"\"\" Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB. \"\"\" # Find and process attachments from the gdb attachment_dict = find_attachments ( self . params . local_gdb_path , self . params . attachments ) # Print file information if debugging is enabled if self . params . debug : log ( None , Colors . INFO , attachment_dict ) # Iterating through key-value pairs using items() for key , value in attachment_dict . items (): # Update the entry of the Geodatabase feature class self . data . set_data ( project_spatial_id = key . replace ( 'proj_' , '' ), # extracted_attachments_path=value extracted_attachments_path = os . path . join ( self . params . gdb_path , value . replace ( f \"C:\\LocalTwoBillionToolkit \\\\ \" , '' )) ) # Log completion of this task log ( None , Colors . INFO , 'All attachments have been extracted from the result Geodatabase.' )","title":"extract_attachments"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.process_entries","text":"Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 def process_entries ( self ) -> None : \"\"\" Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking. The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status. \"\"\" # Iterate over each entry in the data dictionary for index , entry in enumerate ( self . data . data_dict ): # Get the absolute path of the entry file entry_absolute_path : str = self . data . data_dict [ entry ] . get ( 'absolute_file_path' ) try : # Check if the current entry has already been processed if self . data . data_dict [ entry ] . get ( 'processed' ): continue # Check if the created_by date of the entry is within the same day as now created_at = self . data . data_dict [ entry ] . get ( 'created_at' ) if self . params . load_from == 'database' and created_at : now = datetime . datetime . now () if created_at . date () != now . date (): continue # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase gdb_entry_name = f \"proj_ { entry } \" feature_gdb_path = os . path . join ( self . params . local_gdb_path , gdb_entry_name ) # Check the file type and export features accordingly if entry_absolute_path . endswith ( '.gdb' ): # Export features from one geodatabase to the output geodatabase arcpy . conversion . ExportFeatures ( os . path . join ( self . params . output , self . data . data_dict [ entry ] . get ( 'raw_data_path' )), feature_gdb_path ) elif entry_absolute_path . endswith ( '.shp' ): # Export features from shapefile to the output geodatabase arcpy . conversion . ExportFeatures ( entry_absolute_path , feature_gdb_path ) elif entry_absolute_path . endswith (( '.kml' , '.kmz' )): # Initialize an empty GeoDataFrame data = gpd . GeoDataFrame () # List all layers in the KML/KMZ file layers = fiona . listlayers ( entry_absolute_path ) for layer in layers : # Read each layer into a temporary GeoDataFrame layer_gdb = gpd . read_file ( entry_absolute_path , driver = 'LIBKML' , layer = layer ) # Check if the temporary GeoDataFrame is empty before concatenating if not layer_gdb . empty : data = pd . concat ([ data , layer_gdb ], ignore_index = True ) # Rename the 'OBJECTID' column if it exists if 'OBJECTID' in data . columns : data . rename ( columns = { 'OBJECTID' : 'OBJECTID_STRING' }, inplace = True ) # Convert all datetime columns to string with specified utc=True for col in data . select_dtypes ( include = [ 'datetime64[ns]' , 'datetime64[ns, UTC]' ]) . columns : data [ col ] = pd . to_datetime ( data [ col ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Check if 'timestamp' column exists if 'timestamp' in data . columns : data [ 'timestamp' ] = pd . to_datetime ( data [ 'timestamp' ], utc = True , errors = 'coerce' ) . dt . strftime ( '%Y-%m- %d ' ) # Determine the path for raw data and entry basename raw_data_path = self . data . data_dict [ entry ] . get ( 'raw_data_path' ) entry_data_basename = os . path . basename ( raw_data_path ) # Export the GeoDataFrame to the appropriate feature class based on geometry type if entry_data_basename == 'Points' : points_gdf = data [ data . geometry . type == 'Point' ] points_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Polygons' : polygons_gdf = data [ data . geometry . type == 'Polygon' ] polygons_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_data_basename == 'Lines' : lines_gdf = data [ data . geometry . type == 'LineString' ] lines_gdf . to_file ( self . params . local_gdb_path , driver = 'OpenFileGDB' , layer = gdb_entry_name ) elif entry_absolute_path . endswith ( '.geojson' ): # Export features from GeoJSON to the output geodatabase arcpy . conversion . JSONToFeatures ( entry_absolute_path , feature_gdb_path ) # Update the entry status to indicate it has been processed and exists in raw geodatabase format self . data . set_data ( project_spatial_id = entry , in_raw_gdb = True , processed = True ) # Enable the version control of the layer in the geodatabase self . enable_version_control ( feature_gdb_path ) # Update the entry status to indicate that editor tracking is enabled self . data . set_data ( project_spatial_id = entry , editor_tracking_enabled = True ) except ( arcpy . ExecuteError , arcgisscripting . ExecuteError ) as error : log ( self . params . log , Colors . ERROR , f 'An error occurred when processing the layer for { entry_absolute_path } , you can fix or remove it from the datatracker/database, then run the command again with --resume \\n { error } ' , ps_script = self . params . ps_script , project_id = entry ) # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on # raise Exception(error) except Exception as error : log ( self . params . log , Colors . ERROR , f 'An uncaught error occurred when processing the layer for { entry_absolute_path } ' , ps_script = self . params . ps_script , project_id = entry ) raise Exception ( error )","title":"process_entries"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.convert_drive_path","text":"Converts a path with a mapped drive (ie. M:\\, V:) to the actual network drive name. Source code in twobilliontoolkit\\SpatialTransformer\\Processor.py 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 def convert_drive_path ( file_path ): \"\"\" Converts a path with a mapped drive (ie. M:\\, V:\\) to the actual network drive name. \"\"\" abs_file_path = os . path . abspath ( file_path ) actual_drive_path = abs_file_path try : # Extract drive letter from absolute file path drive_letter , _ = os . path . splitdrive ( abs_file_path ) # Check if the path contains a drive letter if re . match ( r \"[A-Za-z] {1} : {1} \" , drive_letter ): # Convert mapped drive path to UNC path actual_drive_path = win32wnet . WNetGetUniversalName ( actual_drive_path , 1 ) except Exception as e : print ( f \"Error: { e } \" ) return actual_drive_path","title":"convert_drive_path"},{"location":"pages/SpatialTransformer/SpatialTransformer/","text":"SpatialTransformer File: twobilliontoolkit/SpatialTransformer/spatial_transformer.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Wed November 15 10:30:00 PST 2023 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The spatial_transformer.py script is a Python tool for processing spatial data. It handles tasks like geodatabase creation, file validation, and checking project numbers against a master data sheet. Usage python path/to/spatial_transformer.py [-h] --input_path input_path [--output_path output_path] --gdb_path gdb_path --master master_data_path --load {datatracker,database} --save {datatracker,database} [--datatracker datatracker_path] [--attachments attachments_path] --debug [--resume] main () The main function of the spatial_transformer.py script Source code in twobilliontoolkit\\SpatialTransformer\\spatial_transformer.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def main (): \"\"\" The main function of the spatial_transformer.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , f 'Tool is starting... Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = 'Spatial Transformer Tool' ) # Define command-line arguments parser . add_argument ( '--input_path' , required = True , help = 'Directory or Compressed file location that will be handed to Ripple Unzipple' ) parser . add_argument ( '--output_path' , required = True , help = 'Where the final output of Ripple Unzipple will extract to.' ) parser . add_argument ( '--load' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to load from (datatracker or database)' ) parser . add_argument ( '--save' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to save to (datatracker or database)' ) parser . add_argument ( '--gdb_path' , required = True , default = '' , help = 'Path of where the geodatabase will be saved, if it does not already exist, it will be created.' ) parser . add_argument ( '--datatracker' , default = '' , help = 'Name of the datatracker file that will be saved adjacent to the geodatabase if provided' ) parser . add_argument ( '--attachments' , default = '' , help = 'Name of the attachments folder that will be saved adjacent to the geodatabase' ) parser . add_argument ( '--master' , default = '' , help = 'The location of the master aspatial datasheet' ) parser . add_argument ( '--debug' , action = 'store_true' , default = False , help = 'Enable debug mode' ) parser . add_argument ( '--resume' , action = 'store_true' , default = False , help = 'Resume from where a crash happened' ) parser . add_argument ( '--suppress' , action = 'store_true' , default = False , help = 'Suppress Warnings in the command-line and only show Errors' ) parser . add_argument ( '--ps_script' , default = '' , help = 'The location of the script to run commands if used.' ) # Parse the command-line arguments args = parser . parse_args () # Call the entry function spatial_transformer ( args . input_path , args . output_path , args . load , args . save , args . gdb_path , args . datatracker , args . attachments , args . master , args . debug , args . resume , args . suppress , args . ps_script ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , f 'Tool has completed. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' ) spatial_transformer ( input_path , output_path , load_from , save_to , gdb_path , datatracker , attachments , master_data_path , debug = False , resume = False , suppress = False , ps_script = None ) The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to output data of Ripple Unzipple. load_from ( str ) \u2013 Either 'database' or 'datatracker' to determine what to load the data from. save_to ( str ) \u2013 Either 'database' or 'datatracker' to determine what to save the data to. gdb_path ( str ) \u2013 Path to the Geodatabase. datatracker ( str ) \u2013 Datatracker file name. attachments ( str ) \u2013 Attachment folder name. master_data_path ( str ) \u2013 Path to the aspatial master data. debug ( bool , default: False ) \u2013 Determines if the program is in debug mode. Defaults False. resume ( bool , default: False ) \u2013 Determines if the program should resume from where a crash happened. Defaults False. suppress ( bool , default: False ) \u2013 Determines if the program will suppress Warning Messages to the command line while running. ps_script ( str , default: None ) \u2013 The path location of the script to run spatial transformer. Source code in twobilliontoolkit\\SpatialTransformer\\spatial_transformer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def spatial_transformer ( input_path : str , output_path : str , load_from : str , save_to : str , gdb_path : str , datatracker : str , attachments : str , master_data_path : str , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to output data of Ripple Unzipple. load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. gdb_path (str): Path to the Geodatabase. datatracker (str): Datatracker file name. attachments (str): Attachment folder name. master_data_path (str): Path to the aspatial master data. debug (bool, optional): Determines if the program is in debug mode. Defaults False. resume (bool, optional): Determines if the program should resume from where a crash happened. Defaults False. suppress (bool, optional): Determines if the program will suppress Warning Messages to the command line while running. ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" # Create the logfile path log_file = os . path . basename ( gdb_path ) . replace ( '.gdb' , f \"_Log_ { datetime . datetime . now () . strftime ( '%Y-%m- %d ' ) } .txt\" ) # Initialize a variable for the processor in case an error occurs beforehand spatial_processor = None try : # Initialize Parameters class setup_parameters = Parameters ( input_path , output_path , gdb_path , master_data_path , datatracker , attachments , load_from , save_to , log_file , debug , resume , suppress , ps_script ) # Start the unzip tool setup_parameters . handle_unzip () log ( None , Colors . INFO , f 'Ripple Unzipple has completed extracted the files. Now starting to create the datatracker entries from the files. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Create the GDB setup_parameters . create_gdb () # Initialize the SpatialData class spatial_processor = Processor ( setup_parameters ) # Search for any spatial data and create an entry in the datatracker for each one spatial_processor . create_datatracker_entries () log ( None , Colors . INFO , f 'All entries have been created in the datatracker for the aspatial and spatial files. Now starting to process those found spatial files. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Go through the dictionary of entries and process them into the output geodatabase spatial_processor . process_entries () log ( None , Colors . INFO , f 'The Processor has completed processing the files into the Geodatabase. Now starting to extract attachments from the Geodatabase. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Extract attachments from the Geodatabase spatial_processor . extract_attachments () log ( None , Colors . INFO , f 'The Attachments Seeker has completed extracting the attachments from the geodatabase. Now starting to transfer over the files from the local directory to the specified output. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Move the local files to the specified output success = transfer ( spatial_processor . params . local_dir , os . path . dirname ( spatial_processor . params . gdb_path ), [ os . path . basename ( spatial_processor . params . gdb_path ), os . path . basename ( spatial_processor . params . datatracker ), os . path . basename ( spatial_processor . params . attachments ), spatial_processor . params . log [: - 4 ] + '_WARNING.txt' , spatial_processor . params . log [: - 4 ] + '_ERROR.txt' ], spatial_processor . params . log ) log ( None , Colors . INFO , f 'The Network Transfer has completed moving the files from local to the network. Now saving the data. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Save the data tracker before returning spatial_processor . data . save_data ( True if resume else False ) log ( None , Colors . INFO , f 'The changes have successfully been saved to the specified datatracker. Now opening Record Reviser. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Open the record reviser filter = { 'created_at' : datetime . datetime . now ()} call_record_reviser ( spatial_processor . data , spatial_processor . params . gdb_path , filter ) log ( None , Colors . INFO , 'The Record Reviser has completed editing any entries and is closing.' ) if not debug and success : # Remove the local contents spatial_processor . del_gdb () os . mkdir ( setup_parameters . local_dir ) log ( None , Colors . INFO , f 'Removing contents from the local directory completed. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) except ( ValueError , Exception ) as error : # Log the error log ( log_file , Colors . ERROR , traceback . format_exc (), ps_script = ps_script ) # Save the data to the datatracker in case of crashing if spatial_processor : spatial_processor . data . save_data ( True if resume else False ) log ( None , Colors . INFO , 'A checkpoint has been made at the point of failure.' ) exit ( 1 )","title":"SpatialTransformer"},{"location":"pages/SpatialTransformer/SpatialTransformer/#spatialtransformer","text":"File: twobilliontoolkit/SpatialTransformer/spatial_transformer.py Created By: Anthony Rodway Email: anthony.rodway@nrcan-rncan.gc.ca Creation Date: Wed November 15 10:30:00 PST 2023 Organization: Natural Resources of Canada Team: Carbon Accounting Team Description The spatial_transformer.py script is a Python tool for processing spatial data. It handles tasks like geodatabase creation, file validation, and checking project numbers against a master data sheet. Usage python path/to/spatial_transformer.py [-h] --input_path input_path [--output_path output_path] --gdb_path gdb_path --master master_data_path --load {datatracker,database} --save {datatracker,database} [--datatracker datatracker_path] [--attachments attachments_path] --debug [--resume]","title":"SpatialTransformer"},{"location":"pages/SpatialTransformer/SpatialTransformer/#twobilliontoolkit.SpatialTransformer.spatial_transformer.main","text":"The main function of the spatial_transformer.py script Source code in twobilliontoolkit\\SpatialTransformer\\spatial_transformer.py 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def main (): \"\"\" The main function of the spatial_transformer.py script \"\"\" # Get the start time of the script start_time = time . time () log ( None , Colors . INFO , f 'Tool is starting... Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Initialize the argument parse parser = argparse . ArgumentParser ( description = 'Spatial Transformer Tool' ) # Define command-line arguments parser . add_argument ( '--input_path' , required = True , help = 'Directory or Compressed file location that will be handed to Ripple Unzipple' ) parser . add_argument ( '--output_path' , required = True , help = 'Where the final output of Ripple Unzipple will extract to.' ) parser . add_argument ( '--load' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to load from (datatracker or database)' ) parser . add_argument ( '--save' , choices = [ 'datatracker' , 'database' ], required = True , default = 'database' , help = 'Specify what to save to (datatracker or database)' ) parser . add_argument ( '--gdb_path' , required = True , default = '' , help = 'Path of where the geodatabase will be saved, if it does not already exist, it will be created.' ) parser . add_argument ( '--datatracker' , default = '' , help = 'Name of the datatracker file that will be saved adjacent to the geodatabase if provided' ) parser . add_argument ( '--attachments' , default = '' , help = 'Name of the attachments folder that will be saved adjacent to the geodatabase' ) parser . add_argument ( '--master' , default = '' , help = 'The location of the master aspatial datasheet' ) parser . add_argument ( '--debug' , action = 'store_true' , default = False , help = 'Enable debug mode' ) parser . add_argument ( '--resume' , action = 'store_true' , default = False , help = 'Resume from where a crash happened' ) parser . add_argument ( '--suppress' , action = 'store_true' , default = False , help = 'Suppress Warnings in the command-line and only show Errors' ) parser . add_argument ( '--ps_script' , default = '' , help = 'The location of the script to run commands if used.' ) # Parse the command-line arguments args = parser . parse_args () # Call the entry function spatial_transformer ( args . input_path , args . output_path , args . load , args . save , args . gdb_path , args . datatracker , args . attachments , args . master , args . debug , args . resume , args . suppress , args . ps_script ) # Get the end time of the script and calculate the elapsed time end_time = time . time () log ( None , Colors . INFO , f 'Tool has completed. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) log ( None , Colors . INFO , f 'Elapsed time: { end_time - start_time : .2f } seconds' )","title":"main"},{"location":"pages/SpatialTransformer/SpatialTransformer/#twobilliontoolkit.SpatialTransformer.spatial_transformer.spatial_transformer","text":"The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation. Parameters: input_path ( str ) \u2013 Path to the input directory or compressed file. output_path ( str ) \u2013 Path to output data of Ripple Unzipple. load_from ( str ) \u2013 Either 'database' or 'datatracker' to determine what to load the data from. save_to ( str ) \u2013 Either 'database' or 'datatracker' to determine what to save the data to. gdb_path ( str ) \u2013 Path to the Geodatabase. datatracker ( str ) \u2013 Datatracker file name. attachments ( str ) \u2013 Attachment folder name. master_data_path ( str ) \u2013 Path to the aspatial master data. debug ( bool , default: False ) \u2013 Determines if the program is in debug mode. Defaults False. resume ( bool , default: False ) \u2013 Determines if the program should resume from where a crash happened. Defaults False. suppress ( bool , default: False ) \u2013 Determines if the program will suppress Warning Messages to the command line while running. ps_script ( str , default: None ) \u2013 The path location of the script to run spatial transformer. Source code in twobilliontoolkit\\SpatialTransformer\\spatial_transformer.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 def spatial_transformer ( input_path : str , output_path : str , load_from : str , save_to : str , gdb_path : str , datatracker : str , attachments : str , master_data_path : str , debug : bool = False , resume : bool = False , suppress : bool = False , ps_script : str = None ) -> None : \"\"\" The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation. Args: input_path (str): Path to the input directory or compressed file. output_path (str): Path to output data of Ripple Unzipple. load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. gdb_path (str): Path to the Geodatabase. datatracker (str): Datatracker file name. attachments (str): Attachment folder name. master_data_path (str): Path to the aspatial master data. debug (bool, optional): Determines if the program is in debug mode. Defaults False. resume (bool, optional): Determines if the program should resume from where a crash happened. Defaults False. suppress (bool, optional): Determines if the program will suppress Warning Messages to the command line while running. ps_script (str, optional): The path location of the script to run spatial transformer. \"\"\" # Create the logfile path log_file = os . path . basename ( gdb_path ) . replace ( '.gdb' , f \"_Log_ { datetime . datetime . now () . strftime ( '%Y-%m- %d ' ) } .txt\" ) # Initialize a variable for the processor in case an error occurs beforehand spatial_processor = None try : # Initialize Parameters class setup_parameters = Parameters ( input_path , output_path , gdb_path , master_data_path , datatracker , attachments , load_from , save_to , log_file , debug , resume , suppress , ps_script ) # Start the unzip tool setup_parameters . handle_unzip () log ( None , Colors . INFO , f 'Ripple Unzipple has completed extracted the files. Now starting to create the datatracker entries from the files. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Create the GDB setup_parameters . create_gdb () # Initialize the SpatialData class spatial_processor = Processor ( setup_parameters ) # Search for any spatial data and create an entry in the datatracker for each one spatial_processor . create_datatracker_entries () log ( None , Colors . INFO , f 'All entries have been created in the datatracker for the aspatial and spatial files. Now starting to process those found spatial files. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Go through the dictionary of entries and process them into the output geodatabase spatial_processor . process_entries () log ( None , Colors . INFO , f 'The Processor has completed processing the files into the Geodatabase. Now starting to extract attachments from the Geodatabase. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Extract attachments from the Geodatabase spatial_processor . extract_attachments () log ( None , Colors . INFO , f 'The Attachments Seeker has completed extracting the attachments from the geodatabase. Now starting to transfer over the files from the local directory to the specified output. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Move the local files to the specified output success = transfer ( spatial_processor . params . local_dir , os . path . dirname ( spatial_processor . params . gdb_path ), [ os . path . basename ( spatial_processor . params . gdb_path ), os . path . basename ( spatial_processor . params . datatracker ), os . path . basename ( spatial_processor . params . attachments ), spatial_processor . params . log [: - 4 ] + '_WARNING.txt' , spatial_processor . params . log [: - 4 ] + '_ERROR.txt' ], spatial_processor . params . log ) log ( None , Colors . INFO , f 'The Network Transfer has completed moving the files from local to the network. Now saving the data. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Save the data tracker before returning spatial_processor . data . save_data ( True if resume else False ) log ( None , Colors . INFO , f 'The changes have successfully been saved to the specified datatracker. Now opening Record Reviser. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) # Open the record reviser filter = { 'created_at' : datetime . datetime . now ()} call_record_reviser ( spatial_processor . data , spatial_processor . params . gdb_path , filter ) log ( None , Colors . INFO , 'The Record Reviser has completed editing any entries and is closing.' ) if not debug and success : # Remove the local contents spatial_processor . del_gdb () os . mkdir ( setup_parameters . local_dir ) log ( None , Colors . INFO , f 'Removing contents from the local directory completed. Time: { datetime . datetime . now () . strftime ( \"%H:%M:%S\" ) } ' ) except ( ValueError , Exception ) as error : # Log the error log ( log_file , Colors . ERROR , traceback . format_exc (), ps_script = ps_script ) # Save the data to the datatracker in case of crashing if spatial_processor : spatial_processor . data . save_data ( True if resume else False ) log ( None , Colors . INFO , 'A checkpoint has been made at the point of failure.' ) exit ( 1 )","title":"spatial_transformer"}]}