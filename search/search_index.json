{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TwoBillionToolkit Home","text":""},{"location":"#pages","title":"Pages","text":"<ul> <li>SpatialTransformer Database</li> <li>SpatialTransformer Datatracker</li> <li>SpatialTransformer Parameters</li> <li>SpatialTransformer Processor</li> <li>SpatialTransformer</li> <li>GeoAttachmentSeeker</li> <li>Logger</li> <li>NetworkTransfer</li> <li>RecordReviser</li> <li>RippleUnzipple</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml                      # The configuration file.\ndocs/\n    index.md                    # The documentation homepage.\n    pages/\n        SpatialTransformer/\n            Database.md         # Documentation for the Database module\n            ...\n        GeoAttachmentSeeker.md  # Documentation for the attachment seeker tool\n        ... \nGeoAttachmentSeeker/\n    geo_attachment_seeker.py\nLogger/\n    logger.py\nNetworkTransfer/\n    network_transfer.py\nRecordReviser/\n    record_reviser.py\nRippleUnzipple/\n    ripple_unzipple.py\nSpatialTransformer/             \n    common.py \n    database.ini\n    Database.py\n    spatial_transformer.py\n    ...\n</code></pre>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs -h</code> - Print help message and exit.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> </ul> <p>For full documentation visit mkdocs.org.</p>"},{"location":"pages/GeoAttachmentSeeker/","title":"GeoAttachmentSeeker","text":"<p>File: twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Fri January 05 08:30:00 PST 2024 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>The script identifies and processes attachment tables within the specified GDB, filtering out non-attachment tables. It then extracts attachment files from each table and exports them to a specified output directory. The attachments are exported to a directory structure organized by project IDs. Each attachment file is named with a prefix (e.g., \"ATT{attachment_id}_\") to distinguish them.</p> Usage <p>python path/to/geo_attachment_seeker.py gdb_path output_path</p>"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.find_attachments","title":"<code>find_attachments(gdb_path, output_path)</code>","text":"<p>Searches through the GDB and finds all relevant attachments.</p> <p>Parameters:</p> Name Type Description Default <code>gdb_path</code> <code>str</code> <p>Path to input GDB.</p> required <code>output_path</code> <code>str</code> <p>Path to export the attachments to.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to.</p> Source code in <code>twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py</code> <pre><code>def find_attachments(gdb_path: str, output_path: str) -&gt; dict:\n    \"\"\"\n    Searches through the GDB and finds all relevant attachments.\n\n    Args:\n        gdb_path (str): Path to input GDB.\n        output_path (str): Path to export the attachments to.\n\n    Returns:\n        dict: A dictionary of key-value pairs tying each project ID that had attachments to the path they were extracted to.\n    \"\"\" \n    # validate the gdb_path\n    if not isinstance(gdb_path, str) or not gdb_path.strip():\n        raise ValueError(f'The provided gdb_path must be a non-empty string.')\n\n    if not gdb_path.endswith('.gdb'):\n        raise ValueError(f'The provided gdb_path must be of type \".gdb\".')\n\n    if not os.path.exists(gdb_path):\n        raise ValueError(f'The provided gdb_path path does not exist.')\n\n    # Set the arc environment\n    arcpy.env.workspace = gdb_path\n\n    # Work all attach tables\n    attachment_dict = {}\n    for table in arcpy.ListTables(): \n        # Filter out non attachment tables   \n        if '__ATTACH' not in table:\n            continue\n\n        # Build the output project paths\n        project_id = table.replace('__ATTACH', '')\n        output_project_path = os.path.abspath(os.path.join(output_path, project_id))\n        table_path = os.path.join(gdb_path, table)\n\n        # Call the processing function\n        process_attachment(output_project_path, table_path)\n\n        # Add to the dictionary\n        attachment_dict[project_id] = output_project_path\n\n    # Return the attachement dictionary\n    return attachment_dict\n</code></pre>"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.main","title":"<code>main()</code>","text":"<p>The main function of the geo_attachment_seeker.py script</p> Source code in <code>twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py</code> <pre><code>def main():#\n    \"\"\" The main function of the geo_attachment_seeker.py script \"\"\"\n    # Get the start time of the script\n    start_time = time.time()\n    log(None, Colors.INFO, 'Tool is starting...')\n\n    # Initialize the argument parse\n    parser = argparse.ArgumentParser(description='')\n\n    # Define command-line arguments\n    parser.add_argument('gdb_path', help='Input GDB path')\n    parser.add_argument('output_path', help='Where to export the attachments to')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    # Access the values using the attribute notation\n    gdb_path = args.gdb_path\n    output_path = args.output_path\n\n    # Call the function to perform the processing\n    find_attachments(gdb_path, output_path)\n\n    # Get the end time of the script and calculate the elapsed time\n    end_time = time.time()\n    log(None, Colors.INFO, 'Tool has completed')\n    log(None, Colors.INFO, f'Elapsed time: {end_time - start_time:.2f} seconds')\n</code></pre>"},{"location":"pages/GeoAttachmentSeeker/#twobilliontoolkit.GeoAttachmentSeeker.geo_attachment_seeker.process_attachment","title":"<code>process_attachment(output_project_path, table_path)</code>","text":"<p>Processes any attachments handed to it.</p> <p>Parameters:</p> Name Type Description Default <code>output_project_path</code> <code>str</code> <p>The path where the attachments will be exported.</p> required <code>table_path</code> <code>str</code> <p>The path to a __ATTACH table in a GDB.</p> required Source code in <code>twobilliontoolkit/GeoAttachmentSeeker/geo_attachment_seeker.py</code> <pre><code>def process_attachment(output_project_path: str, table_path : str) -&gt; None:\n    \"\"\"\n    Processes any attachments handed to it.\n\n    Args:\n        output_project_path (str): The path where the attachments will be exported.\n        table_path (str): The path to a __ATTACH table in a GDB.\n    \"\"\"                \n    # Check if the directory exists, if not, create it\n    if not os.path.exists(output_project_path):\n        os.makedirs(output_project_path)\n\n    # Extract the attachment files from each table\n    # Credits: A Modified version of the method Andrea found at https://support.esri.com/en-us/knowledge-base/how-to-batch-export-attachments-from-a-feature-class-in-000011912\n    with arcpy.da.SearchCursor(table_path, ['DATA', 'ATT_NAME', 'ATTACHMENTID']) as cursor:\n        for attachment, att_name, attachment_id in cursor:\n            filenum = f\"ATT{attachment_id}_\"\n            filename = os.path.join(output_project_path, filenum + att_name)\n\n            with open(filename, 'wb') as file:\n                file.write(attachment.tobytes())\n</code></pre>"},{"location":"pages/Logger/","title":"Logger","text":"<p>File: twobilliontoolkit/Logger/logger.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Thur January 18 12:00:00 PST 2024 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>The module holds the function that handles the logging for the package, the user may provide a log path so that each warning and error gets written to that, otherwise everything will be written to the terminal.</p> Usage <p>Not callable from the command-line</p>"},{"location":"pages/Logger/#twobilliontoolkit.Logger.logger.generate_header","title":"<code>generate_header(file_path, ps_script=None)</code>","text":"<p>Generate the header for the log file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the log file.</p> required <code>ps_script</code> <code>str</code> <p>The path location of the script to run spatial transformer. Defaults to an empty string.</p> <code>None</code> Source code in <code>twobilliontoolkit/Logger/logger.py</code> <pre><code>def generate_header(file_path: str, ps_script: str = None):\n    \"\"\"\n    Generate the header for the log file.\n\n    Args:\n        file_path (str): Path to the log file.\n        ps_script (str, optional): The path location of the script to run spatial transformer. Defaults to an empty string.\n    \"\"\"\n    try:\n        with open(file_path, 'a') as log_file:\n            log_file.write(\"___________________________________________________________\\n\")\n            log_file.write(f\"{os.path.basename(file_path)} file header\\n\")\n            log_file.write(\"___________________________________________________________\\n\")\n            log_file.write(f\"User: {os.getlogin()}\\n\")\n            log_file.write(\"Log Header Created: \" + datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\" + \"\\n\"))\n            log_file.write(f\"Script Path: {ps_script}\\n\")\n            log_file.write(f\"twobilliontoolkit package version: {version('twobilliontoolkit')} \\n\")\n            log_file.write(\"___________________________________________________________\\n\")\n    except Exception as error:\n        print(error)\n</code></pre>"},{"location":"pages/Logger/#twobilliontoolkit.Logger.logger.log","title":"<code>log(file_path='', type=Colors.ERROR, message='', suppress=False, ps_script='', project_id='', absolute_provided=False)</code>","text":"<p>Log messages with colored tags and timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the log file.</p> <code>''</code> <code>type</code> <code>str</code> <p>Color code for the log message type.</p> <code>ERROR</code> <code>message</code> <code>str</code> <p>The log message.</p> <code>''</code> <code>suppress</code> <code>bool</code> <p>Suppress warnings in the command line.</p> <code>False</code> <code>ps_script</code> <code>str</code> <p>The path location of the script to run spatial transformer. </p> <code>''</code> <code>project_id</code> <code>str</code> <p>A 2BT specific variable to include a project id to more easily </p> <code>''</code> <code>absolute_provided</code> <code>bool</code> <p>A Flag to indicate whether the filepath provided is an </p> <code>False</code> Source code in <code>twobilliontoolkit/Logger/logger.py</code> <pre><code>def log(file_path: str = '', type: str = Colors.ERROR, message: str = '', suppress: bool = False, ps_script: str = '', project_id: str = '', absolute_provided: bool = False) -&gt; None:\n    \"\"\"\n    Log messages with colored tags and timestamps.\n\n    Args:\n        file_path (str, optional): Path to the log file.\n        type (str): Color code for the log message type.\n        message (str): The log message.\n        suppress (bool, optional): Suppress warnings in the command line.\n        ps_script (str, optional): The path location of the script to run spatial transformer. \n        project_id (str, optional): A 2BT specific variable to include a project id to more easily \n        correlate errors and fix them.\n        absolute_provided (bool, optional): A Flag to indicate whether the filepath provided is an \n        absolute path and should not be processed.\n    \"\"\"       \n    if project_id:\n        project_id = f'- Project Spatial ID: {project_id} - '    \n\n    # Set the tag and print to the console\n    if type == Colors.INFO:\n        tag = 'INFO'\n        print(f'{type}[{tag}] {message}{Colors.END}')\n    elif type == Colors.WARNING and not suppress:\n        tag = 'WARNING'\n        print(f'{type}[{tag}] {project_id}{message}{Colors.END}')\n    elif type == Colors.WARNING and suppress:\n        tag = 'WARNING'\n    elif type == Colors.ERROR:\n        tag = 'ERROR'\n        print(f'{type}[{tag}] {project_id}{message}{Colors.END}')\n\n    # If a file path is provided\n    if file_path is not None:\n        if not absolute_provided:\n            # Split the log files into seperate Warning and Error logs\n            file_path = r'C:\\LocalTwoBillionToolkit\\\\' + file_path[:-4] + '_' + tag + '.txt'\n\n        # Check if the directory exists, if not, create it\n        directory = os.path.dirname(file_path)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        if not os.path.exists(file_path):\n            generate_header(file_path, ps_script)\n\n        try:\n            # Open the file in append mode and append a log message\n            with open(file_path, 'a') as log_file:\n                log_file.write(f'{datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")} [{tag}] {project_id}{message}\\n\\n')\n        except PermissionError as e:\n            print(f\"Permission denied to write to file: {file_path}. Error: {e}\")\n</code></pre>"},{"location":"pages/NetworkTransfer/","title":"NetworkTransfer","text":"<p>File: twobilliontoolkit/NetworkTransfer/network_transfer.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Fri March 29 02:30:00 PST 2024 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>The script will transfer all files or any files specified from a source directory to a destination. It's main purpose will be used to transfer files from the local computers to a network drive with specific focus on the two billion trees toolkit processing.</p> Usage <p>python path/to/network_transfer.py local_path source_path network_path destination_path [--files [...list of files...]]</p>"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.main","title":"<code>main()</code>","text":"<p>The main function of the network_transfer.py script</p> Source code in <code>twobilliontoolkit/NetworkTransfer/network_transfer.py</code> <pre><code>def main():\n    \"\"\" The main function of the network_transfer.py script \"\"\"\n    # Get the start time of the script\n    start_time = time.time()\n    log(None, Colors.INFO, 'Tool is starting...')\n\n    # Initialize the argument parse\n    parser = argparse.ArgumentParser(description=\"Transfer files from local to network location\")\n    parser.add_argument(\"local_path\", type=str, help=\"Path to the local directory\")\n    parser.add_argument(\"network_path\", type=str, help=\"Path to the network directory\")\n    parser.add_argument(\"--files\", nargs=\"*\", help=\"Optional. All of the files to transfer to the destination.\")\n    args = parser.parse_args()\n\n    files = args.files or None\n\n    # Transfer files\n    _ = transfer(args.local_path, args.network_path, files)\n\n    # Get the end time of the script and calculate the elapsed time\n    end_time = time.time()\n    log(None, Colors.INFO, 'Tool has completed')\n    log(None, Colors.INFO, f'Elapsed time: {end_time - start_time:.2f} seconds')\n</code></pre>"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.merge_directories","title":"<code>merge_directories(src_dir, dst_dir, log_path)</code>","text":"<p>Merge source directory into destination directory.</p> <p>Parameters:</p> Name Type Description Default <code>src_dir</code> <code>str</code> <p>Path to the source directory.</p> required <code>dst_dir</code> <code>str</code> <p>Path to the destination directory.</p> required <code>log_path</code> <code>str</code> <p>path to the log file.</p> required Source code in <code>twobilliontoolkit/NetworkTransfer/network_transfer.py</code> <pre><code>def merge_directories(src_dir, dst_dir, log_path):\n    \"\"\"\n    Merge source directory into destination directory.\n\n    Args:\n        src_dir (str): Path to the source directory.\n        dst_dir (str): Path to the destination directory.\n        log_path (str): path to the log file.\n    \"\"\"\n    try:\n        if not os.path.exists(dst_dir):\n            shutil.copytree(src_dir, dst_dir)\n\n        # Iterate over files and subdirectories in the source directory\n        for item in os.listdir(src_dir):\n            src_item = os.path.join(src_dir, item)\n            dst_item = os.path.join(dst_dir, item)\n\n            # If the item is a file, copy it to the destination directory\n            if os.path.isfile(src_item):\n                shutil.copy2(src_item, dst_item)\n\n            # If the item is a directory, recursively merge it with the corresponding directory in the destination\n            elif os.path.isdir(src_item):\n                merge_directories(src_item, dst_item, log_path)\n    except Exception as error:\n        log(log_path, Colors.ERROR, f'An error has been caught while trying to merge the {src_dir} to {dst_dir}: {error}')\n</code></pre>"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.merge_gdbs","title":"<code>merge_gdbs(src_gdb, dest_gdb, datatracker_2bt=None, log_path=None)</code>","text":"<p>Merge source Geodatabase into destination Geodatabase.</p> <p>Parameters:</p> Name Type Description Default <code>src_gdb</code> <code>str</code> <p>Path to the source Geodatabase.</p> required <code>dest_gdb</code> <code>str</code> <p>Path to the destination Geodatabase.</p> required <code>datatracker_2bt</code> <code>Datatracker2BT</code> <p>Optional. A variable for the 2bt data processing that allows the tool to make changes in this step of the process.</p> <code>None</code> <code>log_path</code> <code>str</code> <p>path to the log file.</p> <code>None</code> Return <p>(bool): success flag of the operation.</p> Source code in <code>twobilliontoolkit/NetworkTransfer/network_transfer.py</code> <pre><code>def merge_gdbs(src_gdb: str, dest_gdb: str, datatracker_2bt: Datatracker2BT = None, log_path: str = None) -&gt; bool:\n    \"\"\"\n    Merge source Geodatabase into destination Geodatabase.\n\n    Args:\n        src_gdb (str): Path to the source Geodatabase.\n        dest_gdb (str): Path to the destination Geodatabase.\n        datatracker_2bt (Datatracker2BT): Optional. A variable for the 2bt data processing that allows the tool to make changes in this step of the process.\n        log_path (str): path to the log file.\n\n    Return:\n        (bool): success flag of the operation.\n    \"\"\"    \n    # Set the workplace for the source geodatabase\n    arcpy.env.workspace = src_gdb\n    try:\n        # Copy the whole GDB if it does not exist\n        if not arcpy.Exists(dest_gdb):\n            if not os.path.exists(os.path.dirname(dest_gdb)):\n                os.mkdir(os.path.dirname(dest_gdb))\n\n            # Copy over the whole gdb\n            arcpy.management.Copy(\n                src_gdb,\n                dest_gdb\n            )\n\n            log(None, Colors.INFO, f'Copy to {dest_gdb} has completed.')\n            return True\n\n    except Exception as error:\n        log(log_path, Colors.ERROR, f'An error has been caught while trying to copy the geodatabase to {dest_gdb}: {error}')\n        return False\n\n    # Get a list of feature classes in the source geodatabase\n    feature_classes = arcpy.ListFeatureClasses()\n    for feature_class in feature_classes:\n        try:\n            # Skip if already exists in destination\n            if arcpy.Exists(os.path.join(dest_gdb, feature_class)):\n                continue  \n\n            # Copy over the specified feature\n            arcpy.management.Copy(\n                os.path.join(src_gdb, feature_class),\n                os.path.join(dest_gdb, feature_class)\n            )\n\n        except Exception as error:\n            log(log_path, Colors.ERROR, f'An error has been caught while trying to merge the geodatabase to {dest_gdb}: {error}')\n\n            if datatracker_2bt:\n                datatracker_2bt.set_data(\n                    project_spatial_id=feature_class.replace(\"proj_\", \"\"), \n                    in_raw_gdb=False\n                )\n\n            return False\n\n    log(None, Colors.INFO, f'Merging to {dest_gdb} has completed.')   \n    return True\n</code></pre>"},{"location":"pages/NetworkTransfer/#twobilliontoolkit.NetworkTransfer.network_transfer.transfer","title":"<code>transfer(local_path, network_path, list_files=None, datatracker_2bt=None, log_path=None)</code>","text":"<p>Transfer files from local directory to network directory.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>str</code> <p>Path to the local directory.</p> required <code>network_path</code> <code>str</code> <p>Path to the network directory.</p> required <code>list_files</code> <code>list</code> <p>Optional. A provided list of files to transfew instead of all.</p> <code>None</code> <code>datatracker_2bt</code> <code>Datatracker2BT</code> <p>Optional. A variable for the 2bt data processing that allows the tool to make changes in this step of the process.</p> <code>None</code> <code>log_path</code> <code>str</code> <p>path to the log file.</p> <code>None</code> Return <p>(bool): success flag of the operation.</p> Source code in <code>twobilliontoolkit/NetworkTransfer/network_transfer.py</code> <pre><code>def transfer(local_path: str, network_path: str, list_files: list[str] = None, datatracker_2bt: Datatracker2BT = None, log_path: str = None) -&gt; bool:\n    \"\"\"\n    Transfer files from local directory to network directory.\n\n    Args:\n        local_path (str): Path to the local directory.\n        network_path (str): Path to the network directory.\n        list_files (list): Optional. A provided list of files to transfew instead of all.\n        datatracker_2bt (Datatracker2BT): Optional. A variable for the 2bt data processing that allows the tool to make changes in this step of the process.\n        log_path (str): path to the log file.\n\n    Return:\n        (bool): success flag of the operation.\n    \"\"\"\n    try:\n        if list_files is None:\n            # Get list of files and directories in the local directory\n            items = os.listdir(local_path)\n        else:\n            items = list_files\n\n        # Iterate over files in the local directory\n        for item in items:            \n            # Build full paths for source and destination\n            src_path = os.path.join(local_path, item)\n            dest_path = os.path.join(network_path, item)\n\n            # Skip processing files that do not exist in the source directory\n            if not os.path.exists(src_path):\n                continue\n\n            # Transfer files or directories\n            if os.path.isdir(src_path):\n                # Merge Geodatabases if destination exists\n                if item.endswith(\".gdb\"):\n                    success = merge_gdbs(src_path, dest_path, datatracker_2bt, log_path)\n                    if not success:\n                        return False\n                else:\n                    merge_directories(src_path, dest_path, log_path)\n            else:\n                if item.endswith(\".txt\") and os.path.exists(dest_path):\n                    # Append text files if destination file exists\n                    with open(dest_path, \"a\") as dest_file:\n                        with open(src_path, \"r\") as src_file:\n                            shutil.copyfileobj(src_file, dest_file)\n                else:      \n                    shutil.copy2(src_path, dest_path)  # preserves metadata              \n\n        log(None, Colors.INFO, f'The transfer of files has been completed')\n    except Exception as error:\n        log(log_path, Colors.ERROR, f'An error has been caught while transferring files from {local_path} to {network_path}: {error}')\n        return False\n\n    return True\n</code></pre>"},{"location":"pages/RecordReviser/","title":"RecordReviser","text":"<p>File: twobilliontoolkit/RecordReviser/record_reviser.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Wed January 17 10:30:00 PST 2024 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>The script will be used to revise any records that have been created in the 2BT Spatial Tools. It provides a graphical user interface (GUI) for viewing and updating records in the Data Tracker. The GUI allows users to make changes to various fields, including 'project_spatial_id', 'project_number', 'in_raw_gdb', 'absolute_path', and 'entry_type'. Additionally, the script supports the creation of duplicate records when updating 'project_number', ensuring data integrity. Changes made through the GUI can be committed to either the Data Tracker Excel file or a Postgres DB table. The script also offers functionality to update associated Raw Data GDB layers and attachment folders.</p> Usage <p>python path/to/record_reviser.py --gdb /path/to/geodatabase --load [datatracker/database] --save [datatracker/database] --datatracker /path/to/datatracker.xlsx --changes \"{key: {field: newvalue, field2: newvalue2...}, key2: {field: newfield}...}\"</p>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp","title":"<code>DataTableApp</code>","text":"<p>               Bases: <code>QWidget</code></p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>class DataTableApp(QWidget):\n    def __init__(self, data: Datatracker2BT, gdb: str = None, filter: dict = None) -&gt; None:\n        \"\"\"\n        Initialize the DataTableApp with the provided data.\n\n        Args:\n            data (Datatracker2BT): An instance of the Datatracker2BT class.\n            gdb (str, optional): The path to the gdb that changes will be made to if applicable.\n            filter (dict, optional): The dictionary of filters for the display data.\n        \"\"\"        \n        super().__init__()\n\n        # Columns that are not editable and the key\n        self.columns_noedit = ['project_spatial_id', 'created_at']\n        self.columns_to_add = ['project_spatial_id', 'project_number', 'in_raw_gdb', 'absolute_file_path', 'entry_type']\n        self.key = 'project_spatial_id'\n\n        # Store the original and current dataframes\n        self.filter = filter\n        self.refresh_data(data)\n        self.gdb = gdb\n\n        # Initialize the user interface\n        self.init_ui()\n\n    def init_ui(self) -&gt; None:\n        \"\"\"\n        Initialize the user interface components.\n        \"\"\"\n        # Create the main layout\n        self.layout = QVBoxLayout()\n\n        # Create the table and populate it\n        self.table = QTableWidget()\n        self.populate_table()\n\n        # Enable sorting for the columns\n        self.table.setSortingEnabled(True)\n\n        # Create a QHBoxLayout for buttons\n        button_layout = QHBoxLayout()\n\n        # Create Save and Reset buttons\n        self.edit_button = QPushButton('Save')\n        self.edit_button.clicked.connect(self.save_changes)\n\n        self.reset_button = QPushButton('Reset')\n        self.reset_button.clicked.connect(self.reset_changes)\n\n        # Add buttons to the button layout\n        button_layout.addWidget(self.edit_button)\n        button_layout.addWidget(self.reset_button)\n\n        # Add the table and button layout to the main layout\n        self.layout.addWidget(self.table)\n        self.layout.addLayout(button_layout)\n\n        # Set the main layout for the widget\n        self.setLayout(self.layout)\n        self.setGeometry(100, 100, 800, 600)\n        self.setWindowTitle('Record Reviser')\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        icon_path = os.path.join(script_dir, 'revision.png')\n        self.setWindowIcon(QIcon(icon_path)) # Credit: https://www.flaticon.com/free-icons/revision\n        self.setWindowFlags(Qt.WindowStaysOnTopHint)\n        self.show()\n\n    def refresh_data(self, data: Datatracker2BT) -&gt; None:\n        \"\"\"\n        Refresh the data in the application.\n\n        Args:\n            data (Datatracker2BT): The new data to be displayed.\n        \"\"\"\n        # Update the data, original and current dataframe\n        self.data = data\n        formatted_data = self.format_data(data)\n        self.original_dataframe = formatted_data[formatted_data['dropped'] != True]\n\n        conditions = []\n\n        # Filter out the data if there was a filter given\n        if self.filter is not None:\n            for key, value in self.filter.items():\n                if key == \"created_at\":\n                    if 'created_at' not in self.original_dataframe:\n                        self.original_dataframe['created_at'] = pd.Series([pd.NaT] * len(self.original_dataframe), dtype='datetime64[ns]')\n                    date = value.date()\n                    condition = (\n                        (self.original_dataframe.created_at.dt.date == date) | \n                        (pd.isna(self.original_dataframe.created_at))\n                    )\n                else:\n                    condition = (self.original_dataframe[key] == value)\n\n                conditions.append(condition)\n\n            # Combine all conditions using &amp; (and) operator\n            combined_condition = conditions[0]\n            for condition in conditions[1:]:\n                combined_condition &amp;= condition\n\n        # If the variable does not exist (no filter was provided)\n        if 'combined_condition' not in locals():\n            combined_condition = pd.Series([True] * len(self.original_dataframe), index=self.original_dataframe.index) \n\n        # If there has been a change create the combined conition\n        if session_added_entries:\n            # Always add the condition for project_spatial_id\n            combined_condition |= self.original_dataframe.project_spatial_id.isin(session_added_entries)\n\n        # Apply the combined condition to the DataFrame\n        self.original_dataframe = self.original_dataframe[combined_condition]\n\n        # Make a working copy of the original dataframe\n        self.dataframe = self.original_dataframe.copy()\n\n    def format_data(self, data: Datatracker2BT) -&gt; pd.DataFrame:\n\n        \"\"\"\n        Format the raw data into a pandas DataFrame.\n\n        Args:\n            data (Datatracker2BT): Raw data to be formatted.\n\n        Returns:\n            pd.DataFrame: A formatted pandas DataFrame.\n        \"\"\"\n        # Convert raw data to a DataFrame and rename index column\n        dataframe = pd.DataFrame.from_dict(data.data_dict, orient='index').reset_index()\n        dataframe.rename(columns={'index': self.key}, inplace=True)\n\n        # Sort the DataFrame by the 'self.key' column alphabetically\n        dataframe_sorted = dataframe.sort_values(by=self.key)\n\n        return dataframe_sorted \n\n    def populate_table(self) -&gt; None:\n        \"\"\"\n        Populate the table with data from the dataframe.\n        \"\"\"\n        # Filter the dataframe to include only the columns_to_add\n        dataframe_filtered = self.dataframe[self.columns_to_add]\n\n        # Set the number of columns and rows in the table\n        self.table.setColumnCount(len(dataframe_filtered.columns))\n        self.table.setRowCount(len(dataframe_filtered))\n\n        # Set headers in the table\n        headers = [str(header) for header in dataframe_filtered.columns]\n        self.table.setHorizontalHeaderLabels(headers)\n\n        # Populate each cell in the table with corresponding data\n        for i in range(len(dataframe_filtered.index)):\n            for j in range(len(dataframe_filtered.columns)):\n                item = QTableWidgetItem(str(dataframe_filtered.iloc[i, j]))\n\n                # Set flags for non-editable columns\n                if dataframe_filtered.columns[j] in self.columns_noedit:\n                    item.setFlags(item.flags() &amp; ~Qt.ItemIsEditable | Qt.ItemIsSelectable)\n\n                self.table.setItem(i, j, item) \n\n        # Resize columns to fit the content\n        self.table.resizeColumnsToContents()   \n\n        # Connect the itemChanged signal to a custom slot (function)\n        self.table.itemChanged.connect(self.item_changed)\n\n    def item_changed(self, item: QTableWidgetItem) -&gt; None:\n        \"\"\"\n        Handle changes in the table items.\n\n        Args:\n            item (QTableWidgetItem): The changed item in the table.\n        \"\"\"\n        # Get the project_spatial_id of the row changed\n        row = item.row()\n        project_spatial_id = self.table.item(row, 0).text()\n\n        # Get the column name from the horizontal header\n        column_name = self.table.horizontalHeaderItem(item.column()).text()\n\n        # Fetch the original value using project_spatial_id\n        original_value = str(self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column_name].values[0])\n\n        # Highlight the cell if the value is different\n        if item.text() != original_value:\n            item.setForeground(QColor('red'))\n        else:\n            item.setForeground(QColor('black'))\n\n    def save_changes(self) -&gt; None:\n        \"\"\"\n        Save the changes made in the GUI.\n        \"\"\"\n        # Dictionary to store changes made in the GUI\n        changes_dict = {}\n\n        # Iterate over rows to identify changes\n        for row in range(len(self.dataframe.index)):\n            project_spatial_id = self.table.item(row, 0).text()\n            row_changes = {}\n\n            for column in range(len(self.dataframe.columns)):\n                item = self.table.item(row, column)\n                if item is not None:\n                    edited_value = item.text()\n                    column_name = self.table.horizontalHeaderItem(column).text()\n                    original_value = str(self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column_name].values[0])\n\n                    # Record changes if the value is different\n                    if edited_value != original_value:\n                        row_changes[column_name] = edited_value\n\n            if row_changes:\n                changes_dict[project_spatial_id] = row_changes\n\n        # Log the changes\n        log(None, Colors.INFO, f'The changes made in the GUI were: {changes_dict}')\n\n        # Update the original data with the changes\n        for project_spatial_id, changes in changes_dict.items():\n            for column, value in changes.items():\n                # Explicitly convert 'value' to the appropriate data type\n                if self.original_dataframe[column].dtype == bool:\n                    if value in ['True', 'False']:\n                        value = bool(value)\n                    else:\n                        log(self.data.log_path, Colors.ERROR, f'The value {value} must be a bool in {column}')\n                        return\n                elif self.original_dataframe[column].dtype == int:\n                    value = int(value)\n                elif self.original_dataframe[column].dtype == float:\n                    value = float(value)\n\n                self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column] = value\n\n        # Update the records in the original data class\n        update_records(self.data, changes_dict, self.gdb)\n\n        # Refresh the data being put into the table and reset the table to the current state\n        self.refresh_data(self.data)\n        self.reset_changes()\n\n    def reset_changes(self) -&gt; None:\n        \"\"\"\n        Reset the data in the table to its original state.\n        \"\"\"\n        # Disconnect the itemChanged signal temporarily\n        self.table.itemChanged.disconnect(self.item_changed)\n\n        # Clear the table\n        self.table.clear()\n\n        # Reset the dataframe to the original state\n        self.dataframe = self.original_dataframe.copy()\n\n        # Reset table sort so no confusion between dataframe and table occurs\n        self.table.sortByColumn(0, Qt.AscendingOrder)\n\n        # Repopulate the table with the original data\n        self.populate_table()\n\n        # Reconnect the itemChanged signal\n        self.table.itemChanged.connect(self.item_changed)\n\n        # Print a message or perform any other necessary actions\n        log(None, Colors.INFO, f'GUI data has been reset to original state')\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.__init__","title":"<code>__init__(data, gdb=None, filter=None)</code>","text":"<p>Initialize the DataTableApp with the provided data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>An instance of the Datatracker2BT class.</p> required <code>gdb</code> <code>str</code> <p>The path to the gdb that changes will be made to if applicable.</p> <code>None</code> <code>filter</code> <code>dict</code> <p>The dictionary of filters for the display data.</p> <code>None</code> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def __init__(self, data: Datatracker2BT, gdb: str = None, filter: dict = None) -&gt; None:\n    \"\"\"\n    Initialize the DataTableApp with the provided data.\n\n    Args:\n        data (Datatracker2BT): An instance of the Datatracker2BT class.\n        gdb (str, optional): The path to the gdb that changes will be made to if applicable.\n        filter (dict, optional): The dictionary of filters for the display data.\n    \"\"\"        \n    super().__init__()\n\n    # Columns that are not editable and the key\n    self.columns_noedit = ['project_spatial_id', 'created_at']\n    self.columns_to_add = ['project_spatial_id', 'project_number', 'in_raw_gdb', 'absolute_file_path', 'entry_type']\n    self.key = 'project_spatial_id'\n\n    # Store the original and current dataframes\n    self.filter = filter\n    self.refresh_data(data)\n    self.gdb = gdb\n\n    # Initialize the user interface\n    self.init_ui()\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.format_data","title":"<code>format_data(data)</code>","text":"<p>Format the raw data into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>Raw data to be formatted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A formatted pandas DataFrame.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def format_data(self, data: Datatracker2BT) -&gt; pd.DataFrame:\n\n    \"\"\"\n    Format the raw data into a pandas DataFrame.\n\n    Args:\n        data (Datatracker2BT): Raw data to be formatted.\n\n    Returns:\n        pd.DataFrame: A formatted pandas DataFrame.\n    \"\"\"\n    # Convert raw data to a DataFrame and rename index column\n    dataframe = pd.DataFrame.from_dict(data.data_dict, orient='index').reset_index()\n    dataframe.rename(columns={'index': self.key}, inplace=True)\n\n    # Sort the DataFrame by the 'self.key' column alphabetically\n    dataframe_sorted = dataframe.sort_values(by=self.key)\n\n    return dataframe_sorted \n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.init_ui","title":"<code>init_ui()</code>","text":"<p>Initialize the user interface components.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def init_ui(self) -&gt; None:\n    \"\"\"\n    Initialize the user interface components.\n    \"\"\"\n    # Create the main layout\n    self.layout = QVBoxLayout()\n\n    # Create the table and populate it\n    self.table = QTableWidget()\n    self.populate_table()\n\n    # Enable sorting for the columns\n    self.table.setSortingEnabled(True)\n\n    # Create a QHBoxLayout for buttons\n    button_layout = QHBoxLayout()\n\n    # Create Save and Reset buttons\n    self.edit_button = QPushButton('Save')\n    self.edit_button.clicked.connect(self.save_changes)\n\n    self.reset_button = QPushButton('Reset')\n    self.reset_button.clicked.connect(self.reset_changes)\n\n    # Add buttons to the button layout\n    button_layout.addWidget(self.edit_button)\n    button_layout.addWidget(self.reset_button)\n\n    # Add the table and button layout to the main layout\n    self.layout.addWidget(self.table)\n    self.layout.addLayout(button_layout)\n\n    # Set the main layout for the widget\n    self.setLayout(self.layout)\n    self.setGeometry(100, 100, 800, 600)\n    self.setWindowTitle('Record Reviser')\n    script_dir = os.path.dirname(os.path.abspath(__file__))\n    icon_path = os.path.join(script_dir, 'revision.png')\n    self.setWindowIcon(QIcon(icon_path)) # Credit: https://www.flaticon.com/free-icons/revision\n    self.setWindowFlags(Qt.WindowStaysOnTopHint)\n    self.show()\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.item_changed","title":"<code>item_changed(item)</code>","text":"<p>Handle changes in the table items.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>QTableWidgetItem</code> <p>The changed item in the table.</p> required Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def item_changed(self, item: QTableWidgetItem) -&gt; None:\n    \"\"\"\n    Handle changes in the table items.\n\n    Args:\n        item (QTableWidgetItem): The changed item in the table.\n    \"\"\"\n    # Get the project_spatial_id of the row changed\n    row = item.row()\n    project_spatial_id = self.table.item(row, 0).text()\n\n    # Get the column name from the horizontal header\n    column_name = self.table.horizontalHeaderItem(item.column()).text()\n\n    # Fetch the original value using project_spatial_id\n    original_value = str(self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column_name].values[0])\n\n    # Highlight the cell if the value is different\n    if item.text() != original_value:\n        item.setForeground(QColor('red'))\n    else:\n        item.setForeground(QColor('black'))\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.populate_table","title":"<code>populate_table()</code>","text":"<p>Populate the table with data from the dataframe.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def populate_table(self) -&gt; None:\n    \"\"\"\n    Populate the table with data from the dataframe.\n    \"\"\"\n    # Filter the dataframe to include only the columns_to_add\n    dataframe_filtered = self.dataframe[self.columns_to_add]\n\n    # Set the number of columns and rows in the table\n    self.table.setColumnCount(len(dataframe_filtered.columns))\n    self.table.setRowCount(len(dataframe_filtered))\n\n    # Set headers in the table\n    headers = [str(header) for header in dataframe_filtered.columns]\n    self.table.setHorizontalHeaderLabels(headers)\n\n    # Populate each cell in the table with corresponding data\n    for i in range(len(dataframe_filtered.index)):\n        for j in range(len(dataframe_filtered.columns)):\n            item = QTableWidgetItem(str(dataframe_filtered.iloc[i, j]))\n\n            # Set flags for non-editable columns\n            if dataframe_filtered.columns[j] in self.columns_noedit:\n                item.setFlags(item.flags() &amp; ~Qt.ItemIsEditable | Qt.ItemIsSelectable)\n\n            self.table.setItem(i, j, item) \n\n    # Resize columns to fit the content\n    self.table.resizeColumnsToContents()   \n\n    # Connect the itemChanged signal to a custom slot (function)\n    self.table.itemChanged.connect(self.item_changed)\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.refresh_data","title":"<code>refresh_data(data)</code>","text":"<p>Refresh the data in the application.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>The new data to be displayed.</p> required Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def refresh_data(self, data: Datatracker2BT) -&gt; None:\n    \"\"\"\n    Refresh the data in the application.\n\n    Args:\n        data (Datatracker2BT): The new data to be displayed.\n    \"\"\"\n    # Update the data, original and current dataframe\n    self.data = data\n    formatted_data = self.format_data(data)\n    self.original_dataframe = formatted_data[formatted_data['dropped'] != True]\n\n    conditions = []\n\n    # Filter out the data if there was a filter given\n    if self.filter is not None:\n        for key, value in self.filter.items():\n            if key == \"created_at\":\n                if 'created_at' not in self.original_dataframe:\n                    self.original_dataframe['created_at'] = pd.Series([pd.NaT] * len(self.original_dataframe), dtype='datetime64[ns]')\n                date = value.date()\n                condition = (\n                    (self.original_dataframe.created_at.dt.date == date) | \n                    (pd.isna(self.original_dataframe.created_at))\n                )\n            else:\n                condition = (self.original_dataframe[key] == value)\n\n            conditions.append(condition)\n\n        # Combine all conditions using &amp; (and) operator\n        combined_condition = conditions[0]\n        for condition in conditions[1:]:\n            combined_condition &amp;= condition\n\n    # If the variable does not exist (no filter was provided)\n    if 'combined_condition' not in locals():\n        combined_condition = pd.Series([True] * len(self.original_dataframe), index=self.original_dataframe.index) \n\n    # If there has been a change create the combined conition\n    if session_added_entries:\n        # Always add the condition for project_spatial_id\n        combined_condition |= self.original_dataframe.project_spatial_id.isin(session_added_entries)\n\n    # Apply the combined condition to the DataFrame\n    self.original_dataframe = self.original_dataframe[combined_condition]\n\n    # Make a working copy of the original dataframe\n    self.dataframe = self.original_dataframe.copy()\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.reset_changes","title":"<code>reset_changes()</code>","text":"<p>Reset the data in the table to its original state.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def reset_changes(self) -&gt; None:\n    \"\"\"\n    Reset the data in the table to its original state.\n    \"\"\"\n    # Disconnect the itemChanged signal temporarily\n    self.table.itemChanged.disconnect(self.item_changed)\n\n    # Clear the table\n    self.table.clear()\n\n    # Reset the dataframe to the original state\n    self.dataframe = self.original_dataframe.copy()\n\n    # Reset table sort so no confusion between dataframe and table occurs\n    self.table.sortByColumn(0, Qt.AscendingOrder)\n\n    # Repopulate the table with the original data\n    self.populate_table()\n\n    # Reconnect the itemChanged signal\n    self.table.itemChanged.connect(self.item_changed)\n\n    # Print a message or perform any other necessary actions\n    log(None, Colors.INFO, f'GUI data has been reset to original state')\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.DataTableApp.save_changes","title":"<code>save_changes()</code>","text":"<p>Save the changes made in the GUI.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def save_changes(self) -&gt; None:\n    \"\"\"\n    Save the changes made in the GUI.\n    \"\"\"\n    # Dictionary to store changes made in the GUI\n    changes_dict = {}\n\n    # Iterate over rows to identify changes\n    for row in range(len(self.dataframe.index)):\n        project_spatial_id = self.table.item(row, 0).text()\n        row_changes = {}\n\n        for column in range(len(self.dataframe.columns)):\n            item = self.table.item(row, column)\n            if item is not None:\n                edited_value = item.text()\n                column_name = self.table.horizontalHeaderItem(column).text()\n                original_value = str(self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column_name].values[0])\n\n                # Record changes if the value is different\n                if edited_value != original_value:\n                    row_changes[column_name] = edited_value\n\n        if row_changes:\n            changes_dict[project_spatial_id] = row_changes\n\n    # Log the changes\n    log(None, Colors.INFO, f'The changes made in the GUI were: {changes_dict}')\n\n    # Update the original data with the changes\n    for project_spatial_id, changes in changes_dict.items():\n        for column, value in changes.items():\n            # Explicitly convert 'value' to the appropriate data type\n            if self.original_dataframe[column].dtype == bool:\n                if value in ['True', 'False']:\n                    value = bool(value)\n                else:\n                    log(self.data.log_path, Colors.ERROR, f'The value {value} must be a bool in {column}')\n                    return\n            elif self.original_dataframe[column].dtype == int:\n                value = int(value)\n            elif self.original_dataframe[column].dtype == float:\n                value = float(value)\n\n            self.original_dataframe.loc[self.original_dataframe[self.key] == project_spatial_id, column] = value\n\n    # Update the records in the original data class\n    update_records(self.data, changes_dict, self.gdb)\n\n    # Refresh the data being put into the table and reset the table to the current state\n    self.refresh_data(self.data)\n    self.reset_changes()\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.call_record_reviser","title":"<code>call_record_reviser(data, gdb=None, filter=None, changes=None)</code>","text":"<p>Handles calling the record reviser parts so the tool can be used outside of command-line as well.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>An instance of Datatracker2BT.</p> required <code>gdb</code> <code>str</code> <p>The geodatabase path. If provided, updates are applied to the geodatabase.</p> <code>None</code> <code>filter</code> <code>dict</code> <p>A dictionary containing filters for the data to be displayed.</p> <code>None</code> <code>changes</code> <code>str</code> <p>A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit.</p> <code>None</code> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def call_record_reviser(data: Datatracker2BT, gdb: str = None, filter: dict = None, changes: str = None) -&gt; None:\n    \"\"\"\n    Handles calling the record reviser parts so the tool can be used outside of command-line as well.\n\n    Args:\n        data (Datatracker2BT): An instance of Datatracker2BT.\n        gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase.\n        filter (dict, optional): A dictionary containing filters for the data to be displayed.\n        changes (str, optional): A string dictionary containing changes for each project. If provided, no GUI will appear and only process the changes in the dictionary, else a GUI will appear and the user can alter the data as they see fit.\n    \"\"\"\n    if changes:\n        try:\n            # Parse the changes argument and update records\n            changes_dict = ast.literal_eval(changes)\n            update_records(data, changes_dict, gdb)\n        except (ValueError, SyntaxError) as e:\n            log(None, Colors.INFO, f'Error parsing changes argument: {e}')\n    else:\n        # If no changes dict is provided, open a PyQt application for data visualization\n        try:\n            app = QApplication([])\n            window = DataTableApp(data, gdb, filter)\n            app.exec_()  \n        except RuntimeWarning as error:\n            log(None, Colors.INFO, error)\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.create_duplicate","title":"<code>create_duplicate(data, project_spatial_id, new_project_number)</code>","text":"<p>Create a duplicate entry in the data for a given project with a new project number.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>An instance of Datatracker2BT.</p> required <code>project_spatial_id</code> <code>str</code> <p>The unique identifier of the project to duplicate.</p> required <code>new_project_number</code> <code>str</code> <p>The new project number for the duplicated entry.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The spatial identifier of the newly created duplicate entry.</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def create_duplicate(data: Datatracker2BT, project_spatial_id: str, new_project_number: str) -&gt; str:\n    \"\"\"\n    Create a duplicate entry in the data for a given project with a new project number.\n\n    Args:\n        data (Datatracker2BT): An instance of Datatracker2BT.\n        project_spatial_id (str): The unique identifier of the project to duplicate.\n        new_project_number (str): The new project number for the duplicated entry.\n\n    Returns:\n        str: The spatial identifier of the newly created duplicate entry.\n    \"\"\"\n    # Retrieve data for the project to duplicate\n    entry_to_duplicate = data.get_data(project_spatial_id)\n\n    # Create a new spatial identifier for the duplicated project\n    new_project_spatial_id = data.create_project_spatial_id(new_project_number)\n\n    # Add the duplicated entry with the new project number to the data\n    data.add_data(\n        project_spatial_id=new_project_spatial_id,\n        project_number=new_project_number,\n        dropped=entry_to_duplicate.get('dropped'),\n        raw_data_path=entry_to_duplicate.get('raw_data_path'),\n        raw_gdb_path=entry_to_duplicate.get('raw_gdb_path'),\n        absolute_file_path=entry_to_duplicate.get('absolute_file_path'),\n        in_raw_gdb=entry_to_duplicate.get('in_raw_gdb'),\n        contains_pdf=entry_to_duplicate.get('contains_pdf'),\n        contains_image=entry_to_duplicate.get('contains_image'),\n        extracted_attachments_path=entry_to_duplicate.get('extracted_attachments_path'),\n        editor_tracking_enabled=entry_to_duplicate.get('editor_tracking_enabled'),\n        processed=entry_to_duplicate.get('processed'),\n        entry_type=entry_to_duplicate.get('entry_type'),\n    )\n\n    # Add new project spatial id's to be included in the table\n    session_added_entries.append(new_project_spatial_id)\n\n    # Set the 'dropped' attribute to True for the original project\n    data.set_data(project_spatial_id, dropped=True)\n\n    return new_project_spatial_id\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.main","title":"<code>main()</code>","text":"<p>The main function of the record_reviser.py script</p> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def main():\n    \"\"\" The main function of the record_reviser.py script \"\"\"\n    # Get the start time of the script\n    start_time = time.time()\n    log(None, Colors.INFO, 'Tool is starting...')\n\n    # Initialize the argument parse\n    parser = argparse.ArgumentParser(description='')\n\n    # Define command-line arguments\n    parser.add_argument('--gdb', required=True, default='', help='The new location or where an exsiting Geodatabase is located')\n    parser.add_argument('--load', choices=['datatracker', 'database'], required=True, default='database', help='Specify what to load from (datatracker or database)')\n    parser.add_argument('--save', choices=['datatracker', 'database'], required=True, default='database', help='Specify what to save to (datatracker or database)')\n    parser.add_argument('--datatracker', required=False, default=None, help='The new location or where an exsiting data tracker is located')\n    parser.add_argument('--changes', required=False, default=None, help='The changes that you want to update, in form \"{project_spaital_id: {field: newvalue, field2:newvalue2...}, project_spatial_id: {field: newfield}...\"')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    # Access the values using the attribute notation\n    load_from = args.load\n    save_to = args.save\n    datatracker_path = args.datatracker\n    gdb_path = args.gdb\n    changes = args.changes\n\n    # Ensure that if a datatracker is specified for loading or saving, then a path must be passed\n    if (load_from == 'datatracker' or save_to == 'datatracker') and datatracker_path == None:\n        raise argparse.ArgumentTypeError(\"If --load or --save is 'datatracker', --datatracker_path must be specified.\")\n    elif (load_from == 'datatracker' or save_to == 'datatracker') and datatracker_path != None:\n        if not isinstance(datatracker_path, str) or not datatracker_path.strip():\n            raise ValueError(f'datatracker_path: {datatracker_path} must be a non-empty string.')\n        if not datatracker_path.endswith('.xlsx'):\n            raise ValueError(f'datatracker_path: {datatracker_path} must be of type .xlsx.')\n        if not os.path.exists(datatracker_path):\n            raise ValueError(f'datatracker_path: {datatracker_path} path does not exist.')\n\n    # Create the logfile path\n    log_path = gdb_path.replace('.gdb', f\"{datetime.datetime.now().strftime('%Y-%m-%d')}.txt\")\n\n    # Create an instance of the Datatracker2BT class\n    data = Datatracker2BT(datatracker_path, load_from, save_to, log_path)\n\n    # Call the handler function\n    call_record_reviser(data=data, gdb=gdb_path, changes=changes or None)\n\n    # Get the end time of the script and calculate the elapsed time\n    end_time = time.time()\n    log(None, Colors.INFO, 'Tool has completed')\n    log(None, Colors.INFO, f'Elapsed time: {end_time - start_time:.2f} seconds')\n</code></pre>"},{"location":"pages/RecordReviser/#twobilliontoolkit.RecordReviser.record_reviser.update_records","title":"<code>update_records(data, changes_dict, gdb=None)</code>","text":"<p>Update records in the data based on the changes provided in the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Datatracker2BT</code> <p>An instance of Datatracker2BT.</p> required <code>changes_dict</code> <code>dict</code> <p>A dictionary containing changes for each project.</p> required <code>gdb</code> <code>str</code> <p>The geodatabase path. If provided, updates are applied to the geodatabase.</p> <code>None</code> Source code in <code>twobilliontoolkit/RecordReviser/record_reviser.py</code> <pre><code>def update_records(data: Datatracker2BT, changes_dict: dict, gdb: str = None) -&gt; None:\n    \"\"\"\n    Update records in the data based on the changes provided in the dictionary.\n\n    Args:\n        data (Datatracker2BT): An instance of Datatracker2BT.\n        changes_dict (dict): A dictionary containing changes for each project.\n        gdb (str, optional): The geodatabase path. If provided, updates are applied to the geodatabase.\n    \"\"\"\n    for project_spatial_id, value in changes_dict.items():\n        # Check if the current change updated the project number\n        new_project_number = value.get('project_number')\n        if new_project_number:\n            # Check if the project number entered was valid\n            data.database_connection.connect(data.database_parameters)\n            found = data.database_connection.read(\n                schema=data.database_connection.schema,\n                table='project_number',\n                condition=f\"project_number='{new_project_number}'\"\n            )               \n            data.database_connection.disconnect()\n            if not found:\n                print(f'Project number {new_project_number} is not a valid project number in the database. Skipping changing this...')\n                continue\n\n            # Duplicate the project with the new project number\n            old_project_spatial_id = 'proj_' + project_spatial_id\n            project_spatial_id = create_duplicate(data, project_spatial_id, new_project_number)\n\n            if gdb and data.get_data(project_spatial_id).get('in_raw_gdb') == True:\n                # If geodatabase is provided, rename the corresponding entries\n                arcpy.management.Rename(\n                    os.path.join(gdb, old_project_spatial_id),\n                    'proj_' + project_spatial_id\n                )\n\n                # Rename attachments path if it exists\n                attachments_path = str(data.get_data(project_spatial_id).get('extracted_attachments_path'))\n                if attachments_path not in ['nan', None, 'None']:\n                    # update the attachment path\n                    new_attachments_path = attachments_path.split('proj_')[0] + 'proj_' + project_spatial_id\n\n                    # Rename the path to the attachments\n                    os.rename(\n                        attachments_path, \n                        new_attachments_path\n                    )\n\n                    # Set the corresponding attachments path\n                    data.set_data(\n                        project_spatial_id, extracted_attachments_path=new_attachments_path\n                    )\n\n                    # Also need to update the linked geodatabase things\n                    arcpy.management.Rename(\n                        os.path.join(gdb, old_project_spatial_id) + '__ATTACH',\n                        'proj_' + project_spatial_id + '__ATTACH'\n                    )\n                    arcpy.management.Rename(\n                        os.path.join(gdb, old_project_spatial_id) + '__ATTACHREL',\n                        'proj_' + project_spatial_id + '__ATTACHREL'\n                    )\n\n        # Update other attributes in the data\n        data.set_data(\n            project_spatial_id,\n            in_raw_gdb=value.get('in_raw_gdb'),\n            absolute_file_path=value.get('absolute_file_path'),\n            entry_type=value.get('entry_type')\n        )\n\n    # Save the updated data\n    data.save_data(update=True)\n</code></pre>"},{"location":"pages/RippleUnzipple/","title":"RippleUnzipple","text":"<p>File: twobilliontoolkit/RippleUnzipple/ripple_unzipple.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Fri November 10 14:00:00 PST 2023 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>Recursively unzips all compressed folders in a given directory.</p> Usage <p>python ripple_unzipple.py input_path output_path [log_path]</p>"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.main","title":"<code>main()</code>","text":"<p>The main function of the ripple_unzipple.py script</p> Source code in <code>twobilliontoolkit/RippleUnzipple/ripple_unzipple.py</code> <pre><code>def main():\n    \"\"\" The main function of the ripple_unzipple.py script \"\"\"\n    # Get the start time of the script\n    start_time = time.time()\n    log(None, Colors.INFO, 'Tool is starting...')\n\n    parser = argparse.ArgumentParser(description='Recursively unzip all compressed folders in a given directory.')\n    parser.add_argument('--input', required=True, help='Path to the input directory or compressed file.')\n    parser.add_argument('--output', required=True, help='Path to the output directory.')\n    parser.add_argument('--log', default='', help='Path to the log file.')\n\n    args = parser.parse_args()\n\n    try:\n        ripple_unzip(args.input, args.output, args.log)\n    except Exception as error:\n        log(args.log, Colors.ERROR, error)\n        exit(1)\n\n    # Get the end time of the script and calculate the elapsed time\n    end_time = time.time()\n    log(None, Colors.INFO, 'Tool has completed')\n    log(None, Colors.INFO, f'Elapsed time: {end_time - start_time:.2f} seconds')\n</code></pre>"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.recursive_unzip","title":"<code>recursive_unzip(input_path, output_path, original_input_path, log_path='')</code>","text":"<p>Recursively unzip .zip and .7z files in the input_path to the output_path.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input directory or compressed file.</p> required <code>output_path</code> <code>str</code> <p>Path to the output directory.</p> required <code>original_input_path</code> <code>str</code> <p>Path of the original input compress/directory.</p> required <code>log_path</code> <code>str</code> <p>Path to the log file. Defaults to ''.</p> <code>''</code> Source code in <code>twobilliontoolkit/RippleUnzipple/ripple_unzipple.py</code> <pre><code>def recursive_unzip(input_path: str, output_path: str, original_input_path: str, log_path: str = '') -&gt; None:   \n    \"\"\"\n    Recursively unzip .zip and .7z files in the input_path to the output_path.\n\n    Args:\n        input_path (str): Path to the input directory or compressed file.\n        output_path (str): Path to the output directory.\n        original_input_path (str): Path of the original input compress/directory.\n        log_path (str, optional): Path to the log file. Defaults to ''.\n    \"\"\"\n    # Create output_path if it doesn't exist\n    os.makedirs(output_path, exist_ok=True)\n\n    # Iterate through the directory and unzip any compressed folders\n    for root, dirs, files in os.walk(input_path):\n        for file in files:\n            # Get the file path of the input \n            file_path = os.path.join(root, file)\n\n            # Make sure that the original zip does not get touched\n            if file_path == original_input_path:\n                continue\n\n            file_to_remove = ''\n            if file.endswith((\".zip\", \".7z\")):\n                try:    \n                    with ZipFile(file_path, mode='r') if file.endswith(\".zip\") else SevenZipFile(file_path, mode='r') as archive_ref:\n                        # Get the path that the file will be extracted to\n                        extract_path = os.path.join(output_path, os.path.splitext(file_path)[0]) \n\n                        # unzip the file to the location\n                        archive_ref.extractall(extract_path)\n\n                        # Recursively call the function to check every file in the directory tree\n                        recursive_unzip(extract_path, extract_path, original_input_path, log_path)\n\n                        # Flag the compressed file to be removed\n                        file_to_remove = extract_path + '.zip' if file.endswith(\".zip\") else extract_path + '.7z'\n                except FileNotFoundError as error:\n                    error_message = f\"FileNotFoundError: {error.strerror} in ({file_path})\\n\\nA common cause for this issue may be that the MAX_PATH_LENGTH for your machine's directory is surpassed. The compressed directory will be placed in the folder for you to extract manually. Please read the Configuration section in the README to resolve this issue.\"\n                    log(log_path, Colors.ERROR, error_message)\n                    continue\n                except (BadZipFile, Bad7zFile) as error:\n                    error_message = f\"BadZipFile or Bad7ZFile: {error.strerror} with ({file_path})\\n\\nContinuing the tool and placing file for manual extracting.\"\n                    log(log_path, Colors.ERROR, error_message)\n                    continue\n\n                # Remove the original compressed file from the new output folder\n                if os.path.exists(file_to_remove):\n                    os.remove(file_to_remove)\n</code></pre>"},{"location":"pages/RippleUnzipple/#twobilliontoolkit.RippleUnzipple.ripple_unzipple.ripple_unzip","title":"<code>ripple_unzip(input_path, output_path, log_path='')</code>","text":"<p>Unzip .zip and .7z files either for a directory or a compressed file.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input directory or compressed file.</p> required <code>output_path</code> <code>str</code> <p>Path to the output directory.</p> required <code>log_path</code> <code>str</code> <p>Path to the log file. Defaults to ''.</p> <code>''</code> Source code in <code>twobilliontoolkit/RippleUnzipple/ripple_unzipple.py</code> <pre><code>def ripple_unzip(input_path: str, output_path: str, log_path: str = '') -&gt; None:\n    \"\"\"\n    Unzip .zip and .7z files either for a directory or a compressed file.\n\n    Args:\n        input_path (str): Path to the input directory or compressed file.\n        output_path (str): Path to the output directory.\n        log_path (str, optional): Path to the log file. Defaults to ''.\n    \"\"\"\n    try:\n        # Check if the provided path exists\n        if not os.path.exists(input_path):\n            raise ValueError(f\"ValueError: The specified path ({input_path}) does not exist\")\n\n        # Handle different input extensions\n        if os.path.isdir(input_path):\n            # First copy the directory to the new location \n            copy_tree(input_path, output_path)\n            recursive_unzip(output_path, output_path, log_path)\n\n        elif input_path.endswith((\".zip\", \".7z\")):\n            os.makedirs(output_path, exist_ok=True)\n\n            with ZipFile(input_path, mode='r') if input_path.endswith(\".zip\") else SevenZipFile(input_path, mode='r') as archive_ref:\n                archive_ref.extractall(output_path)\n                recursive_unzip(output_path, output_path, input_path, log_path)\n\n        else:\n            raise ValueError(\"ValueError: Unsupported input type. Please provide a directory or a compressed file.\")\n\n    except ValueError as error:\n        log(log_path, Colors.ERROR, error)\n        raise ValueError(error)   \n    except Exception as error:\n        log(log_path, Colors.ERROR, error)\n        raise Exception(error)  \n</code></pre>"},{"location":"pages/SpatialTransformer/Database/","title":"SpatialTransformer.Database","text":""},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database","title":"<code>Database</code>","text":"<p>A class for interacting with a PostgreSQL database.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>class Database:\n    \"\"\"A class for interacting with a PostgreSQL database.\"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initialize the Database instance.\"\"\"\n        self.connection = None\n        self.cursor = None\n        self.schema = None\n        self.table = None\n\n    def connect(self, params: dict[str, str]) -&gt; None:\n        \"\"\"\n        Connect to the database.\n\n        Args:\n            params (dict): Dictionary containing database connection parameters.\n        \"\"\"\n        try:\n            self.connection = psycopg2.connect(**params)\n            self.cursor = self.connection.cursor()\n            log(None, Colors.INFO, 'Opened a connection to the database...')\n        except Exception as error:\n            raise Exception(error)\n\n    def disconnect(self) -&gt; None:\n        \"\"\"Disconnect from the database.\"\"\"\n        try:\n            if self.connection is not None:\n                self.connection.close()\n                log(None, Colors.INFO, 'Database connection closed.')\n        except Exception as error:\n            raise Exception(error)\n\n    def get_params(self, config_path: str = None, section: str = 'postgresql') -&gt; dict[str, str]:\n        \"\"\"\n        Get database connection parameters from a configuration file.\n\n        Args:\n            config_path (str, optional): Path to the configuration file.\n            section (str, optional): Section in the configuration file.\n\n        Returns:\n            dict: Dictionary containing database connection parameters.\n        \"\"\"\n        # Get the directory of the current script\n        script_directory = os.path.dirname(os.path.abspath(__file__))\n\n        # Join the script directory with the relative path to database.ini\n        if config_path == None:\n            config_path = os.path.join(script_directory, 'database.ini')\n\n            if not os.path.exists(config_path):\n                raise FileExistsError(f'The database.ini file is not in the correct place/does not exist in {script_directory}')\n\n        # create a parser\n        parser = ConfigParser()\n\n        # read config file\n        parser.read(config_path)\n\n        # get section, default to postgresql\n        db = {}\n        if parser.has_section(section):\n            params = parser.items(section)\n            for param in params:\n                if not param[1]:\n                    raise ValueError(f'The [{param[0]}] field in {config_path} was not filled out.')\n\n                if param[0] == 'schema':\n                    self.schema = param[1]\n                    continue\n                elif param[0] == 'table':\n                    self.table = param[1]\n                    continue\n\n                db[param[0]] = param[1]\n        else:\n            raise Exception(f'Section {section} not found in the {config_path} file')\n\n        return db\n\n    def execute(self, query: str, values: list[str] = None) -&gt; None:\n        \"\"\"\n        Execute a SQL query.\n\n        Args:\n            query (str): SQL query string.\n            values (list, optional): List of parameter values for the query.\n        \"\"\"\n        self.cursor.execute(query, values)\n        self.connection.commit()\n\n    def get_columns(self, schema: str, table: str) -&gt; list[str]:\n        \"\"\"\n        Get the columns from the table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n\n        Returns:\n            list: List of strings that correspond to the table columns.\n        \"\"\"\n        query = f\"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = '{schema + '.' + table}'\"\n        self.execute(query) \n        return [row[0] for row in self.cursor.fetchall()]\n\n    def get_pkey(self, schema: str, table: str) -&gt; str:\n        \"\"\"\n        Get the primary key from the table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n\n        Returns:\n            str: The name of the primary key in the schema/table provided.\n        \"\"\"  \n        query = f\"SELECT * FROM information_schema.key_column_usage where table_schema = '{schema}' and constraint_name = CONCAT('{table}', '_pkey')\"\n        self.execute(query) \n        return self.cursor.fetchone()\n\n    def create(self, schema: str, table: str, columns: list[str], values: list[str]) -&gt; None:\n        \"\"\"\n        Insert data into a table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n            columns (list): List of column names.\n            values (list): List of values to be inserted.\n        \"\"\"\n        query = f\"INSERT INTO {schema + '.' + table} ({', '.join(columns)}) VALUES ({', '.join(['%s' for _ in values])})\"\n        self.execute(query, values)\n\n    def read(self, schema: str, table: str, columns: list[str] = None, condition: str = None) -&gt; list[tuple]:\n        \"\"\"\n        Retrieve data from a table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n            columns (list, optional): List of column names to retrieve. Defaults to None (all columns).\n            condition (str, optional): SQL condition to filter rows. Defaults to None.\n\n        Returns:\n            list: List of tuples containing the retrieved data.\n        \"\"\"\n        if columns is None:\n            columns = ['*']\n\n        query = f\"SELECT {', '.join(columns)} FROM {schema + '.' + table}\"\n\n        if condition is not None:\n            query += f\" WHERE {condition}\"\n\n        self.execute(query) \n\n        return self.cursor.fetchall()\n\n    def update(self, schema: str, table: str, values_dict: dict[str, str], condition: str) -&gt; None:\n        \"\"\"\n        Update data in a table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n            values_dict (dict): Dictionary of column-value pairs to be updated.\n            condition (str): SQL condition to filter rows to be updated.\n        \"\"\"        \n        query = f\"UPDATE {schema + '.' + table} SET {', '.join([f'{col}=%s' for col in values_dict.keys()])} WHERE {condition}\"\n\n        values = list(values_dict.values())\n\n        self.execute(query, values)\n\n    def delete(self, schema: str, table: str, condition: str) -&gt; None:\n        \"\"\"\n        Delete data from a table.\n\n        Args:\n            schema (str): Name of the database schema.\n            table (str): Name of the table.\n            condition (str): SQL condition to filter rows to be deleted.\n        \"\"\"\n        query = f\"DELETE FROM {schema + '.' + table} WHERE {condition}\"\n        self.execute(query)\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Database instance.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the Database instance.\"\"\"\n    self.connection = None\n    self.cursor = None\n    self.schema = None\n    self.table = None\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.connect","title":"<code>connect(params)</code>","text":"<p>Connect to the database.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Dictionary containing database connection parameters.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def connect(self, params: dict[str, str]) -&gt; None:\n    \"\"\"\n    Connect to the database.\n\n    Args:\n        params (dict): Dictionary containing database connection parameters.\n    \"\"\"\n    try:\n        self.connection = psycopg2.connect(**params)\n        self.cursor = self.connection.cursor()\n        log(None, Colors.INFO, 'Opened a connection to the database...')\n    except Exception as error:\n        raise Exception(error)\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.create","title":"<code>create(schema, table, columns, values)</code>","text":"<p>Insert data into a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <code>columns</code> <code>list</code> <p>List of column names.</p> required <code>values</code> <code>list</code> <p>List of values to be inserted.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def create(self, schema: str, table: str, columns: list[str], values: list[str]) -&gt; None:\n    \"\"\"\n    Insert data into a table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n        columns (list): List of column names.\n        values (list): List of values to be inserted.\n    \"\"\"\n    query = f\"INSERT INTO {schema + '.' + table} ({', '.join(columns)}) VALUES ({', '.join(['%s' for _ in values])})\"\n    self.execute(query, values)\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.delete","title":"<code>delete(schema, table, condition)</code>","text":"<p>Delete data from a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <code>condition</code> <code>str</code> <p>SQL condition to filter rows to be deleted.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def delete(self, schema: str, table: str, condition: str) -&gt; None:\n    \"\"\"\n    Delete data from a table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n        condition (str): SQL condition to filter rows to be deleted.\n    \"\"\"\n    query = f\"DELETE FROM {schema + '.' + table} WHERE {condition}\"\n    self.execute(query)\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.disconnect","title":"<code>disconnect()</code>","text":"<p>Disconnect from the database.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def disconnect(self) -&gt; None:\n    \"\"\"Disconnect from the database.\"\"\"\n    try:\n        if self.connection is not None:\n            self.connection.close()\n            log(None, Colors.INFO, 'Database connection closed.')\n    except Exception as error:\n        raise Exception(error)\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.execute","title":"<code>execute(query, values=None)</code>","text":"<p>Execute a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string.</p> required <code>values</code> <code>list</code> <p>List of parameter values for the query.</p> <code>None</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def execute(self, query: str, values: list[str] = None) -&gt; None:\n    \"\"\"\n    Execute a SQL query.\n\n    Args:\n        query (str): SQL query string.\n        values (list, optional): List of parameter values for the query.\n    \"\"\"\n    self.cursor.execute(query, values)\n    self.connection.commit()\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_columns","title":"<code>get_columns(schema, table)</code>","text":"<p>Get the columns from the table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list[str]</code> <p>List of strings that correspond to the table columns.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def get_columns(self, schema: str, table: str) -&gt; list[str]:\n    \"\"\"\n    Get the columns from the table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n\n    Returns:\n        list: List of strings that correspond to the table columns.\n    \"\"\"\n    query = f\"SELECT column_name FROM information_schema.columns where CONCAT(table_schema, '.', table_name) = '{schema + '.' + table}'\"\n    self.execute(query) \n    return [row[0] for row in self.cursor.fetchall()]\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_params","title":"<code>get_params(config_path=None, section='postgresql')</code>","text":"<p>Get database connection parameters from a configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to the configuration file.</p> <code>None</code> <code>section</code> <code>str</code> <p>Section in the configuration file.</p> <code>'postgresql'</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, str]</code> <p>Dictionary containing database connection parameters.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def get_params(self, config_path: str = None, section: str = 'postgresql') -&gt; dict[str, str]:\n    \"\"\"\n    Get database connection parameters from a configuration file.\n\n    Args:\n        config_path (str, optional): Path to the configuration file.\n        section (str, optional): Section in the configuration file.\n\n    Returns:\n        dict: Dictionary containing database connection parameters.\n    \"\"\"\n    # Get the directory of the current script\n    script_directory = os.path.dirname(os.path.abspath(__file__))\n\n    # Join the script directory with the relative path to database.ini\n    if config_path == None:\n        config_path = os.path.join(script_directory, 'database.ini')\n\n        if not os.path.exists(config_path):\n            raise FileExistsError(f'The database.ini file is not in the correct place/does not exist in {script_directory}')\n\n    # create a parser\n    parser = ConfigParser()\n\n    # read config file\n    parser.read(config_path)\n\n    # get section, default to postgresql\n    db = {}\n    if parser.has_section(section):\n        params = parser.items(section)\n        for param in params:\n            if not param[1]:\n                raise ValueError(f'The [{param[0]}] field in {config_path} was not filled out.')\n\n            if param[0] == 'schema':\n                self.schema = param[1]\n                continue\n            elif param[0] == 'table':\n                self.table = param[1]\n                continue\n\n            db[param[0]] = param[1]\n    else:\n        raise Exception(f'Section {section} not found in the {config_path} file')\n\n    return db\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.get_pkey","title":"<code>get_pkey(schema, table)</code>","text":"<p>Get the primary key from the table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the primary key in the schema/table provided.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def get_pkey(self, schema: str, table: str) -&gt; str:\n    \"\"\"\n    Get the primary key from the table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n\n    Returns:\n        str: The name of the primary key in the schema/table provided.\n    \"\"\"  \n    query = f\"SELECT * FROM information_schema.key_column_usage where table_schema = '{schema}' and constraint_name = CONCAT('{table}', '_pkey')\"\n    self.execute(query) \n    return self.cursor.fetchone()\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.read","title":"<code>read(schema, table, columns=None, condition=None)</code>","text":"<p>Retrieve data from a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <code>columns</code> <code>list</code> <p>List of column names to retrieve. Defaults to None (all columns).</p> <code>None</code> <code>condition</code> <code>str</code> <p>SQL condition to filter rows. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>list</code> <code>list[tuple]</code> <p>List of tuples containing the retrieved data.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def read(self, schema: str, table: str, columns: list[str] = None, condition: str = None) -&gt; list[tuple]:\n    \"\"\"\n    Retrieve data from a table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n        columns (list, optional): List of column names to retrieve. Defaults to None (all columns).\n        condition (str, optional): SQL condition to filter rows. Defaults to None.\n\n    Returns:\n        list: List of tuples containing the retrieved data.\n    \"\"\"\n    if columns is None:\n        columns = ['*']\n\n    query = f\"SELECT {', '.join(columns)} FROM {schema + '.' + table}\"\n\n    if condition is not None:\n        query += f\" WHERE {condition}\"\n\n    self.execute(query) \n\n    return self.cursor.fetchall()\n</code></pre>"},{"location":"pages/SpatialTransformer/Database/#twobilliontoolkit.SpatialTransformer.Database.Database.update","title":"<code>update(schema, table, values_dict, condition)</code>","text":"<p>Update data in a table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the database schema.</p> required <code>table</code> <code>str</code> <p>Name of the table.</p> required <code>values_dict</code> <code>dict</code> <p>Dictionary of column-value pairs to be updated.</p> required <code>condition</code> <code>str</code> <p>SQL condition to filter rows to be updated.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Database.py</code> <pre><code>def update(self, schema: str, table: str, values_dict: dict[str, str], condition: str) -&gt; None:\n    \"\"\"\n    Update data in a table.\n\n    Args:\n        schema (str): Name of the database schema.\n        table (str): Name of the table.\n        values_dict (dict): Dictionary of column-value pairs to be updated.\n        condition (str): SQL condition to filter rows to be updated.\n    \"\"\"        \n    query = f\"UPDATE {schema + '.' + table} SET {', '.join([f'{col}=%s' for col in values_dict.keys()])} WHERE {condition}\"\n\n    values = list(values_dict.values())\n\n    self.execute(query, values)\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/","title":"SpatialTransformer.Datatracker","text":""},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker","title":"<code>Datatracker</code>","text":"Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>class Datatracker:\n    def __init__(self, data_traker_path: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None) -&gt; None:\n        \"\"\"\n        Initializes the Datatracker class with input parameters to store data tracker information.\n\n        Args:\n            data_traker_path (str): Path to data tracker file.\n            load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'.\n            save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'.\n            log_path (str, optional): Path to log file for recording errors.\n        \"\"\"\n        self.data_dict = {}\n        self.datatracker = data_traker_path\n        self.load_from = load_from\n        self.save_to = save_to\n        self.log_path = log_path\n\n        if load_from == 'database' or save_to == 'database':\n            # Create database object\n            self.database_connection = Database()\n\n            # Read connection parameters from the configuration file\n            self.database_parameters = self.database_connection.get_params(config_path=database_config)\n            self.database_connection.connect(self.database_parameters)\n            self.database_pkey = self.database_connection.get_pkey(self.database_connection.schema, self.database_connection.table)\n            self.database_connection.disconnect()\n\n        self.load_data()\n\n    def add_data(self, key: str, **kwargs) -&gt; None:\n        \"\"\"\n        Adds project data to the data tracker.\n\n        Args:\n            key (str): Acts as key in dictionary.\n            **kwargs (any): Additional keyword arguments for project data.\n        \"\"\"\n        self.data_dict[key] = kwargs\n\n    def set_data(self, key: str, **kwargs) -&gt; None:\n        \"\"\"\n        Updates project data in the data tracker.\n\n        Args:\n            key (str): Acts as key in dictionary.\n            **kwargs (any): Keyword arguments for updating project data.\n        \"\"\"\n        # Update specified parameters as sets\n        project_data = self.data_dict.get(key, {})\n\n        for pkey, pvalue in kwargs.items():\n            if pvalue is not None:\n                project_data[pkey] = pvalue\n\n        self.data_dict[key] = project_data\n\n    def get_data(self, key: str) -&gt; dict:\n        \"\"\"\n        Gets an object of values given a project spatial id.\n\n        Args:\n            key (str): Acts as key in dictionary.\n\n        Returns:\n            dict: the values that correspond to the given key\n        \"\"\"\n        return self.data_dict[key]\n\n    def find_matching_data(self, **kwargs) -&gt; (str, dict): # type: ignore\n        \"\"\"\n        Search for a matching entry in the data based on given parameters.\n\n        Args:\n            **kwargs (any): Keyword arguments for finding a matching key.\n\n        Returns:\n            str: A tuple of matching (key, data) if  the parameters passed already exists in the dataframe, otherwise return None.\n        \"\"\"        \n        return next(\n            (\n                (key, data)\n                for key, data in self.data_dict.items()\n                if all(data.get(field) == value for field, value in kwargs.items())\n            ),\n            (None, None)\n        )\n\n    def count_occurances(self, field: str, value: str) -&gt; int:\n        \"\"\"\n        Count the occurrences of a specified field in the data object.\n\n        Args:\n            field (str): Name of the parameter to count occurrences.\n            value (str): Value of the parameter to count occurrences.\n\n        Returns:\n            int: Number of occurrences of the specified parameter.\n        \"\"\"\n        return sum(\n            1 for data in self.data_dict.values() if data.get(field) == value\n        )\n\n    def load_data(self) -&gt; None:\n        \"\"\"\n        Load data from an existing data tracker or a database connection into class.\n        \"\"\"\n        if self.load_from == 'database':\n            self.load_from_database()\n        else:\n            self.load_from_file()\n\n    def load_from_database(self) -&gt; None:\n        \"\"\"\n        Load data from a database connection into class.\n        \"\"\"\n        self.database_connection.connect(self.database_parameters)\n\n        columns = self.database_connection.get_columns(schema=self.database_connection.schema, table=self.database_connection.table)\n\n        rows = self.database_connection.read(schema=self.database_connection.schema, table=self.database_connection.table, columns=columns)\n\n        for fields in rows:\n            values = dict(zip(columns[1:], fields[1:]))\n            self.data_dict[self.database_pkey] = values\n\n        self.database_connection.disconnect()\n\n    def load_from_file(self) -&gt; None:\n        \"\"\"\n        Load data from a file into class.\n        \"\"\"\n        if not os.path.exists(self.datatracker):\n            return\n\n        data_df = pd.read_excel(self.datatracker)\n\n        for index, row in data_df.iterrows():\n            pkey = row.index[0]\n            data = row.drop(pkey).to_dict()\n            self.add_data(key=row[pkey], **data)\n\n    def save_data(self, update: bool = False) -&gt; None:\n        \"\"\"\n        Save data tracker information to data tracker or database connection.\n\n        Args:\n            update (bool): Flag to determine if there are some entries in the data object that will need updating.\n        \"\"\"\n        if self.save_to == 'database':\n            self.save_to_database(update)\n        else:\n            self.save_to_file()\n\n    def save_to_database(self, update: bool = False) -&gt; None:\n        \"\"\"\n        Save data tracker information to a database connection.\n\n        Args:\n            update (bool): Flag to determine if there are some entries in the data object that will need updating.\n        \"\"\"\n        self.database_connection.connect(self.database_parameters)\n\n        existing_keys = set(row[0] for row in self.database_connection.read(\n            schema=self.database_connection.schema, \n            table=self.database_connection.table, \n            columns=[self.database_pkey]\n        ))\n\n        for key, value in self.data_dict.items():\n            if key in existing_keys and update:\n                self.database_connection.update(\n                    schema=self.database_connection.schema, \n                    table=self.database_connection.table,\n                    values_dict={field: value[field] for field in value},\n                    condition=f\"{self.database_pkey}='{key}'\"\n                )\n            elif key not in existing_keys:\n                self.database_connection.create(\n                    schema=self.database_connection.schema, \n                    table=self.database_connection.table,\n                    columns=[self.database_pkey] + list(value.keys()),\n                    values=[key] + list(value.values())\n                )\n\n        self.database_connection.disconnect()\n\n    def save_to_file(self) -&gt; None:\n        \"\"\"\n        Save data tracker information to a file.\n        \"\"\"\n        df = pd.DataFrame(list(self.data_dict.values()))\n        df.insert(0, 'key', self.data_dict.keys())\n\n        if not df.empty:\n            df.to_excel(self.datatracker, index=False)\n            log(None, Colors.INFO, f'The data tracker \"{self.datatracker}\" has been created/updated successfully.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.__init__","title":"<code>__init__(data_traker_path, load_from='database', save_to='database', database_config=None, log_path=None)</code>","text":"<p>Initializes the Datatracker class with input parameters to store data tracker information.</p> <p>Parameters:</p> Name Type Description Default <code>data_traker_path</code> <code>str</code> <p>Path to data tracker file.</p> required <code>load_from</code> <code>str</code> <p>Source to load data from {'database', 'datatracker'}. Default: 'database'.</p> <code>'database'</code> <code>save_to</code> <code>str</code> <p>Destination to save data to {'database', 'datatracker'}. Default: 'database'.</p> <code>'database'</code> <code>log_path</code> <code>str</code> <p>Path to log file for recording errors.</p> <code>None</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def __init__(self, data_traker_path: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None) -&gt; None:\n    \"\"\"\n    Initializes the Datatracker class with input parameters to store data tracker information.\n\n    Args:\n        data_traker_path (str): Path to data tracker file.\n        load_from (str): Source to load data from {'database', 'datatracker'}. Default: 'database'.\n        save_to (str): Destination to save data to {'database', 'datatracker'}. Default: 'database'.\n        log_path (str, optional): Path to log file for recording errors.\n    \"\"\"\n    self.data_dict = {}\n    self.datatracker = data_traker_path\n    self.load_from = load_from\n    self.save_to = save_to\n    self.log_path = log_path\n\n    if load_from == 'database' or save_to == 'database':\n        # Create database object\n        self.database_connection = Database()\n\n        # Read connection parameters from the configuration file\n        self.database_parameters = self.database_connection.get_params(config_path=database_config)\n        self.database_connection.connect(self.database_parameters)\n        self.database_pkey = self.database_connection.get_pkey(self.database_connection.schema, self.database_connection.table)\n        self.database_connection.disconnect()\n\n    self.load_data()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.add_data","title":"<code>add_data(key, **kwargs)</code>","text":"<p>Adds project data to the data tracker.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Acts as key in dictionary.</p> required <code>**kwargs</code> <code>any</code> <p>Additional keyword arguments for project data.</p> <code>{}</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def add_data(self, key: str, **kwargs) -&gt; None:\n    \"\"\"\n    Adds project data to the data tracker.\n\n    Args:\n        key (str): Acts as key in dictionary.\n        **kwargs (any): Additional keyword arguments for project data.\n    \"\"\"\n    self.data_dict[key] = kwargs\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.count_occurances","title":"<code>count_occurances(field, value)</code>","text":"<p>Count the occurrences of a specified field in the data object.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>Name of the parameter to count occurrences.</p> required <code>value</code> <code>str</code> <p>Value of the parameter to count occurrences.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of occurrences of the specified parameter.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def count_occurances(self, field: str, value: str) -&gt; int:\n    \"\"\"\n    Count the occurrences of a specified field in the data object.\n\n    Args:\n        field (str): Name of the parameter to count occurrences.\n        value (str): Value of the parameter to count occurrences.\n\n    Returns:\n        int: Number of occurrences of the specified parameter.\n    \"\"\"\n    return sum(\n        1 for data in self.data_dict.values() if data.get(field) == value\n    )\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.find_matching_data","title":"<code>find_matching_data(**kwargs)</code>","text":"<p>Search for a matching entry in the data based on given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>any</code> <p>Keyword arguments for finding a matching key.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>(str, dict)</code> <p>A tuple of matching (key, data) if  the parameters passed already exists in the dataframe, otherwise return None.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def find_matching_data(self, **kwargs) -&gt; (str, dict): # type: ignore\n    \"\"\"\n    Search for a matching entry in the data based on given parameters.\n\n    Args:\n        **kwargs (any): Keyword arguments for finding a matching key.\n\n    Returns:\n        str: A tuple of matching (key, data) if  the parameters passed already exists in the dataframe, otherwise return None.\n    \"\"\"        \n    return next(\n        (\n            (key, data)\n            for key, data in self.data_dict.items()\n            if all(data.get(field) == value for field, value in kwargs.items())\n        ),\n        (None, None)\n    )\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.get_data","title":"<code>get_data(key)</code>","text":"<p>Gets an object of values given a project spatial id.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Acts as key in dictionary.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the values that correspond to the given key</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def get_data(self, key: str) -&gt; dict:\n    \"\"\"\n    Gets an object of values given a project spatial id.\n\n    Args:\n        key (str): Acts as key in dictionary.\n\n    Returns:\n        dict: the values that correspond to the given key\n    \"\"\"\n    return self.data_dict[key]\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_data","title":"<code>load_data()</code>","text":"<p>Load data from an existing data tracker or a database connection into class.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def load_data(self) -&gt; None:\n    \"\"\"\n    Load data from an existing data tracker or a database connection into class.\n    \"\"\"\n    if self.load_from == 'database':\n        self.load_from_database()\n    else:\n        self.load_from_file()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_from_database","title":"<code>load_from_database()</code>","text":"<p>Load data from a database connection into class.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def load_from_database(self) -&gt; None:\n    \"\"\"\n    Load data from a database connection into class.\n    \"\"\"\n    self.database_connection.connect(self.database_parameters)\n\n    columns = self.database_connection.get_columns(schema=self.database_connection.schema, table=self.database_connection.table)\n\n    rows = self.database_connection.read(schema=self.database_connection.schema, table=self.database_connection.table, columns=columns)\n\n    for fields in rows:\n        values = dict(zip(columns[1:], fields[1:]))\n        self.data_dict[self.database_pkey] = values\n\n    self.database_connection.disconnect()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.load_from_file","title":"<code>load_from_file()</code>","text":"<p>Load data from a file into class.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def load_from_file(self) -&gt; None:\n    \"\"\"\n    Load data from a file into class.\n    \"\"\"\n    if not os.path.exists(self.datatracker):\n        return\n\n    data_df = pd.read_excel(self.datatracker)\n\n    for index, row in data_df.iterrows():\n        pkey = row.index[0]\n        data = row.drop(pkey).to_dict()\n        self.add_data(key=row[pkey], **data)\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_data","title":"<code>save_data(update=False)</code>","text":"<p>Save data tracker information to data tracker or database connection.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>bool</code> <p>Flag to determine if there are some entries in the data object that will need updating.</p> <code>False</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def save_data(self, update: bool = False) -&gt; None:\n    \"\"\"\n    Save data tracker information to data tracker or database connection.\n\n    Args:\n        update (bool): Flag to determine if there are some entries in the data object that will need updating.\n    \"\"\"\n    if self.save_to == 'database':\n        self.save_to_database(update)\n    else:\n        self.save_to_file()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_to_database","title":"<code>save_to_database(update=False)</code>","text":"<p>Save data tracker information to a database connection.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>bool</code> <p>Flag to determine if there are some entries in the data object that will need updating.</p> <code>False</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def save_to_database(self, update: bool = False) -&gt; None:\n    \"\"\"\n    Save data tracker information to a database connection.\n\n    Args:\n        update (bool): Flag to determine if there are some entries in the data object that will need updating.\n    \"\"\"\n    self.database_connection.connect(self.database_parameters)\n\n    existing_keys = set(row[0] for row in self.database_connection.read(\n        schema=self.database_connection.schema, \n        table=self.database_connection.table, \n        columns=[self.database_pkey]\n    ))\n\n    for key, value in self.data_dict.items():\n        if key in existing_keys and update:\n            self.database_connection.update(\n                schema=self.database_connection.schema, \n                table=self.database_connection.table,\n                values_dict={field: value[field] for field in value},\n                condition=f\"{self.database_pkey}='{key}'\"\n            )\n        elif key not in existing_keys:\n            self.database_connection.create(\n                schema=self.database_connection.schema, \n                table=self.database_connection.table,\n                columns=[self.database_pkey] + list(value.keys()),\n                values=[key] + list(value.values())\n            )\n\n    self.database_connection.disconnect()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.save_to_file","title":"<code>save_to_file()</code>","text":"<p>Save data tracker information to a file.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def save_to_file(self) -&gt; None:\n    \"\"\"\n    Save data tracker information to a file.\n    \"\"\"\n    df = pd.DataFrame(list(self.data_dict.values()))\n    df.insert(0, 'key', self.data_dict.keys())\n\n    if not df.empty:\n        df.to_excel(self.datatracker, index=False)\n        log(None, Colors.INFO, f'The data tracker \"{self.datatracker}\" has been created/updated successfully.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker.set_data","title":"<code>set_data(key, **kwargs)</code>","text":"<p>Updates project data in the data tracker.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Acts as key in dictionary.</p> required <code>**kwargs</code> <code>any</code> <p>Keyword arguments for updating project data.</p> <code>{}</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def set_data(self, key: str, **kwargs) -&gt; None:\n    \"\"\"\n    Updates project data in the data tracker.\n\n    Args:\n        key (str): Acts as key in dictionary.\n        **kwargs (any): Keyword arguments for updating project data.\n    \"\"\"\n    # Update specified parameters as sets\n    project_data = self.data_dict.get(key, {})\n\n    for pkey, pvalue in kwargs.items():\n        if pvalue is not None:\n            project_data[pkey] = pvalue\n\n    self.data_dict[key] = project_data\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT","title":"<code>Datatracker2BT</code>","text":"<p>               Bases: <code>Datatracker</code></p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>class Datatracker2BT(Datatracker):\n    def __init__(self, data_traker_path: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None) -&gt; None:\n        \"\"\"\n        Initializes the Data class with input parameters. Used to store the data tracker information.\n\n        Args:\n            data_traker_path (str): Path to data tracker to load data if exists.\n            load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'.\n            save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'.\n            log_path (str, optional): The path to the log file if you wish to keep any errors that occur.\n        \"\"\"\n        super().__init__(data_traker_path, load_from, save_to, database_config, log_path)\n\n    def add_data(self, project_spatial_id: str, project_number: str, dropped: bool, raw_data_path: str, raw_gdb_path: str, absolute_file_path: str, in_raw_gdb: bool, contains_pdf: bool, contains_image: bool, extracted_attachments_path: str, editor_tracking_enabled: bool, processed: bool, entry_type: str) -&gt; None:\n        \"\"\"\n        Adds project data to the data tracker.\n\n        Args:\n            project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n            project_number (str): Project number.\n            dropped (bool): Indicates whether the entry is dropped, non-valid etc.\n            raw_data_path (str): Raw data path.\n            raw_gdb_path (str): Absolute path to the output geodatabase.\n            absolute_file_path (str): The full absolute file path.\n            in_raw_gdb (bool): Indicates whether data is in raw GDB.\n            contains_pdf (bool): Indicates whether data contains PDF files.\n            contains_image (bool): Indicates whether data contains image files.\n            extracted_attachments_path (str): The path to the extracted attachments if applicable.\n            editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb.\n            processed (bool): Indicates whether data has been processed yet.\n            entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry.\n        \"\"\"\n        self.data_dict[project_spatial_id] = {\n            'project_number': project_number,\n            'dropped': dropped,\n            'raw_data_path': raw_data_path,\n            'raw_gdb_path': raw_gdb_path,\n            'absolute_file_path': absolute_file_path,\n            'in_raw_gdb': in_raw_gdb,\n            'contains_pdf': contains_pdf,\n            'contains_image': contains_image,\n            'extracted_attachments_path': extracted_attachments_path,\n            'editor_tracking_enabled': editor_tracking_enabled,\n            'processed': processed,\n            'entry_type': entry_type\n        }\n\n    def set_data(self, project_spatial_id: str, project_number: str = None, dropped: bool = None, raw_data_path: str = None, absolute_file_path: str = None, in_raw_gdb: bool = None, contains_pdf: bool = None, contains_image: bool = None, extracted_attachments_path: str = None, editor_tracking_enabled: bool = None, processed: bool = None, entry_type: str = None) -&gt; None:\n        \"\"\"\n        Updates project data in the data tracker.\n\n        Args:\n            project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n            project_number (str): Project number (optional).\n            dropped (bool): Indicates whether the entry is dropped, non-valid etc.\n            raw_data_path (str): Raw data path (optional).\n            absolute_file_path (str): The full absolute file path.\n            in_raw_gdb (bool): Indicates whether data is in raw GDB (optional).\n            contains_pdf (bool): Indicates whether data contains PDF files (optional).\n            contains_image (bool): Indicates whether data contains image files (optional).\n            extracted_attachments_path (str): The path to the extracted attachments if applicable (optional).\n            editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional).\n            processed (bool): Indicates whether data has been processed yet (optional).            \n            entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional).\n        \"\"\"\n        # Update specified parameters as sets\n        project_data = self.data_dict.get(project_spatial_id, {})\n        if project_number is not None:\n            project_data['project_number'] = project_number\n        if dropped is not None:\n            project_data['dropped'] = dropped\n        if raw_data_path is not None:\n            project_data['raw_data_path'] = raw_data_path\n        if absolute_file_path is not None:\n            project_data['absolute_file_path'] = absolute_file_path\n        if in_raw_gdb is not None:\n            project_data['in_raw_gdb'] = in_raw_gdb\n        if contains_pdf is not None:\n            project_data['contains_pdf'] = contains_pdf\n        if contains_image is not None:\n            project_data['contains_image'] = contains_image\n        if extracted_attachments_path is not None:\n            project_data['extracted_attachments_path'] = extracted_attachments_path\n        if editor_tracking_enabled is not None:\n            project_data['editor_tracking_enabled'] = editor_tracking_enabled\n        if processed is not None:\n            project_data['processed'] = processed\n        if entry_type is not None:\n            project_data['entry_type'] = entry_type\n\n    def get_data(self, project_spatial_id: str) -&gt; dict:\n        \"\"\"\n        Gets an object of values given a project spatial id.\n\n        Args:\n            project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n\n        Returns:\n            dict: the values that correspond to the given key\n        \"\"\"\n        return self.data_dict[project_spatial_id]\n\n    def find_matching_spatial_id(self, absolute_file_path: str) -&gt; str:\n        \"\"\"\n        Search for a matching entry for the absolute data path.\n\n        Args:\n            absolute_file_path (str): The absolute path of the data.\n\n        Returns:\n            str: A matching project_spatial_id if it the absolute data path already exists in the dataframe, otherwise return None.\n        \"\"\"        \n        return next(\n            (\n                project_spatial_id\n                for project_spatial_id, project_data in self.data_dict.items()\n                if project_data.get('absolute_file_path') == absolute_file_path\n            ),\n            None\n        )\n\n    def get_highest_suffix(self, project_number: str) -&gt; int:\n        \"\"\"\n        Get the highest suffix for the given project number in the spatial_project_id field.\n\n        Args:\n            project_number (str): The project number to find the highest suffix for.\n\n        Returns:\n            int: The highest suffix found, or 0 if none are found.\n        \"\"\"\n        suffixes = [\n            int(key.split('_')[-1])\n            for key in self.data_dict.keys()\n            if self.data_dict[key].get('project_number') == project_number\n        ]\n        return max(suffixes, default=0)\n\n    def create_project_spatial_id(self, project_number: str) -&gt; str:\n        \"\"\"\n        Create the next project spatial id for the file\n\n        Args:\n            project_number (str): The formatted project number.\n\n        Returns:\n            str: The project spatial id next in line.\n        \"\"\"\n        # Get the next suffix from the project spatial ids from the data entries\n        results_next_id = self.get_highest_suffix(project_number) + 1\n\n        # Clean the project number and format to the correct project_spatial_id format\n        clean_project_number = project_number.replace('- ', '').replace(' ', '_')\n\n        return clean_project_number + '_' + str(results_next_id).zfill(2)\n\n    def load_from_database(self) -&gt; None:\n        \"\"\"\n        Load data from a database connection into class.\n        \"\"\"\n        self.database_connection.connect(self.database_parameters)\n\n        columns = ['project_spatial_id', 'project_number', 'dropped', 'raw_data_path','raw_gdb_path','absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image','extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type', 'created_at']\n\n        rows = self.database_connection.read(schema=self.database_connection.schema, table=self.database_connection.table, columns=columns)\n\n        for fields in rows:\n            project_spatial_id = fields[0]\n            values = dict(zip(columns[1:], fields[1:]))\n            self.data_dict[project_spatial_id] = values\n\n        self.database_connection.disconnect()\n\n    def load_from_file(self) -&gt; None:\n        \"\"\"\n        Load data from a file into class.\n        \"\"\"\n        if not os.path.exists(self.datatracker):\n            return\n\n        data_df = pd.read_excel(self.datatracker, dtype={\n            'project_spatial_id': object, \n            'project_number': object,\n            'dropped': bool, \n            'raw_data_path': object, \n            'raw_gdb_path': object,\n            'absolute_file_path': object,\n            'in_raw_gdb': bool, \n            'contains_pdf': bool,\n            'contains_image': bool, \n            'extracted_attachments_path': object,\n            'editor_tracking_enabled': bool, \n            'processed': bool, \n            'entry_type': str\n        }, index_col=None)\n\n        for index, row in data_df.iterrows():\n            self.add_data(\n                project_spatial_id=row['project_spatial_id'],\n                project_number=row['project_number'], \n                dropped=row['dropped'],\n                raw_data_path=row['raw_data_path'], \n                raw_gdb_path=row['raw_gdb_path'], \n                absolute_file_path=row['absolute_file_path'],\n                in_raw_gdb=row['in_raw_gdb'], \n                contains_pdf=row['contains_pdf'], \n                contains_image=row['contains_image'],\n                extracted_attachments_path=row['extracted_attachments_path'],\n                editor_tracking_enabled=row['editor_tracking_enabled'],\n                processed=row['processed'], \n                entry_type=row['entry_type']\n            )\n\n    def save_to_database(self, update: bool = False) -&gt; None:\n        \"\"\"\n        Save data tracker information to a database connection.\n\n        Args:\n            update (bool): Flag to determine if there are some entries in the data object that will need updating.\n        \"\"\"\n        self.database_connection.connect(self.database_parameters)\n\n        rows = self.database_connection.read(\n            schema=self.database_connection.schema, \n            table=self.database_connection.table,\n            columns=['project_spatial_id']\n        )\n        existing_ids = set(row[0] for row in rows)\n\n        for key, value in self.data_dict.items():\n            try:\n                if key in existing_ids and update:\n                    self.database_connection.update(\n                        schema=self.database_connection.schema, \n                        table=self.database_connection.table,\n                        values_dict={\n                            'dropped': value['dropped'],\n                            'in_raw_gdb': value['in_raw_gdb'], \n                            'contains_pdf': value['contains_pdf'], \n                            'contains_image': value['contains_image'],\n                            'editor_tracking_enabled': value['editor_tracking_enabled'], \n                            'processed': value['processed'], 'entry_type': value['entry_type']\n                        },\n                        condition=f\"project_spatial_id='{key}'\"\n                    )\n\n                elif key not in existing_ids:\n                    self.database_connection.create(\n                        schema=self.database_connection.schema, \n                        table=self.database_connection.table,\n                        columns=('project_spatial_id', 'project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type'),\n                        values=(\n                            key, \n                            value['project_number'], \n                            value['dropped'], \n                            value['raw_data_path'],\n                            value['raw_gdb_path'], \n                            value['absolute_file_path'], \n                            value['in_raw_gdb'], \n                            value['contains_pdf'], \n                            value['contains_image'],\n                            value['extracted_attachments_path'],\n                            value['editor_tracking_enabled'],\n                            value['processed'], \n                            value['entry_type']\n                        )\n                    ) \n\n            except psycopg2.errors.ForeignKeyViolation as error:\n                log(self.log_path, Colors.ERROR, error)\n\n        self.database_connection.disconnect()\n\n    def save_to_file(self) -&gt; None:\n        \"\"\"\n        Save data tracker information to a file.\n        \"\"\"\n        # TODO: Work on getting the save to file exactly like save to database\n\n        # Create a DataFrame and save it to Excel if the data tracker file doesn't exist\n        df = pd.DataFrame(list(self.data_dict.values()), columns=['project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type'])\n\n        # Add 'project_spatial_id' to the DataFrame\n        df['project_spatial_id'] = list(self.data_dict.keys())\n\n        # Reorder columns to have 'project_spatial_id' as the first column\n        df = df[['project_spatial_id', 'project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type']]\n\n        # Sort the rows by the project_spatial_id column\n        df = df.sort_values(by=['project_spatial_id'])\n\n        # Check if the directory exists, if not, create it\n        directory = os.path.dirname(self.datatracker)\n        if not os.path.exists(directory):\n            os.makedirs(directory)\n\n        # Convert dataframe to excel\n        df.to_excel(self.datatracker, index=False)\n\n        log(None, Colors.INFO, f'The data tracker \"{self.datatracker}\" has been created/updated successfully.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.__init__","title":"<code>__init__(data_traker_path, load_from='database', save_to='database', database_config=None, log_path=None)</code>","text":"<p>Initializes the Data class with input parameters. Used to store the data tracker information.</p> <p>Parameters:</p> Name Type Description Default <code>data_traker_path</code> <code>str</code> <p>Path to data tracker to load data if exists.</p> required <code>load_from</code> <code>str</code> <p>Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'.</p> <code>'database'</code> <code>save_to</code> <code>str</code> <p>Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'.</p> <code>'database'</code> <code>log_path</code> <code>str</code> <p>The path to the log file if you wish to keep any errors that occur.</p> <code>None</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def __init__(self, data_traker_path: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None) -&gt; None:\n    \"\"\"\n    Initializes the Data class with input parameters. Used to store the data tracker information.\n\n    Args:\n        data_traker_path (str): Path to data tracker to load data if exists.\n        load_from (str): Flag to determine if loading dataframe should be done from the {database, datatracker}. Default: 'database'.\n        save_to (str): Flag to determine if saving the dataframe should be done to the {database, datatracker}. Default: 'database'.\n        log_path (str, optional): The path to the log file if you wish to keep any errors that occur.\n    \"\"\"\n    super().__init__(data_traker_path, load_from, save_to, database_config, log_path)\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.add_data","title":"<code>add_data(project_spatial_id, project_number, dropped, raw_data_path, raw_gdb_path, absolute_file_path, in_raw_gdb, contains_pdf, contains_image, extracted_attachments_path, editor_tracking_enabled, processed, entry_type)</code>","text":"<p>Adds project data to the data tracker.</p> <p>Parameters:</p> Name Type Description Default <code>project_spatial_id</code> <code>str</code> <p>Project spatial ID. Acts as key in dictionary.</p> required <code>project_number</code> <code>str</code> <p>Project number.</p> required <code>dropped</code> <code>bool</code> <p>Indicates whether the entry is dropped, non-valid etc.</p> required <code>raw_data_path</code> <code>str</code> <p>Raw data path.</p> required <code>raw_gdb_path</code> <code>str</code> <p>Absolute path to the output geodatabase.</p> required <code>absolute_file_path</code> <code>str</code> <p>The full absolute file path.</p> required <code>in_raw_gdb</code> <code>bool</code> <p>Indicates whether data is in raw GDB.</p> required <code>contains_pdf</code> <code>bool</code> <p>Indicates whether data contains PDF files.</p> required <code>contains_image</code> <code>bool</code> <p>Indicates whether data contains image files.</p> required <code>extracted_attachments_path</code> <code>str</code> <p>The path to the extracted attachments if applicable.</p> required <code>editor_tracking_enabled</code> <code>bool</code> <p>Indicates whether the editor tracking has been enabled for the layer in the gdb.</p> required <code>processed</code> <code>bool</code> <p>Indicates whether data has been processed yet.</p> required <code>entry_type</code> <code>str</code> <p>Indicates wether the entry contains information for an aspatial or spatial entry.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def add_data(self, project_spatial_id: str, project_number: str, dropped: bool, raw_data_path: str, raw_gdb_path: str, absolute_file_path: str, in_raw_gdb: bool, contains_pdf: bool, contains_image: bool, extracted_attachments_path: str, editor_tracking_enabled: bool, processed: bool, entry_type: str) -&gt; None:\n    \"\"\"\n    Adds project data to the data tracker.\n\n    Args:\n        project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n        project_number (str): Project number.\n        dropped (bool): Indicates whether the entry is dropped, non-valid etc.\n        raw_data_path (str): Raw data path.\n        raw_gdb_path (str): Absolute path to the output geodatabase.\n        absolute_file_path (str): The full absolute file path.\n        in_raw_gdb (bool): Indicates whether data is in raw GDB.\n        contains_pdf (bool): Indicates whether data contains PDF files.\n        contains_image (bool): Indicates whether data contains image files.\n        extracted_attachments_path (str): The path to the extracted attachments if applicable.\n        editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb.\n        processed (bool): Indicates whether data has been processed yet.\n        entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry.\n    \"\"\"\n    self.data_dict[project_spatial_id] = {\n        'project_number': project_number,\n        'dropped': dropped,\n        'raw_data_path': raw_data_path,\n        'raw_gdb_path': raw_gdb_path,\n        'absolute_file_path': absolute_file_path,\n        'in_raw_gdb': in_raw_gdb,\n        'contains_pdf': contains_pdf,\n        'contains_image': contains_image,\n        'extracted_attachments_path': extracted_attachments_path,\n        'editor_tracking_enabled': editor_tracking_enabled,\n        'processed': processed,\n        'entry_type': entry_type\n    }\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.create_project_spatial_id","title":"<code>create_project_spatial_id(project_number)</code>","text":"<p>Create the next project spatial id for the file</p> <p>Parameters:</p> Name Type Description Default <code>project_number</code> <code>str</code> <p>The formatted project number.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The project spatial id next in line.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def create_project_spatial_id(self, project_number: str) -&gt; str:\n    \"\"\"\n    Create the next project spatial id for the file\n\n    Args:\n        project_number (str): The formatted project number.\n\n    Returns:\n        str: The project spatial id next in line.\n    \"\"\"\n    # Get the next suffix from the project spatial ids from the data entries\n    results_next_id = self.get_highest_suffix(project_number) + 1\n\n    # Clean the project number and format to the correct project_spatial_id format\n    clean_project_number = project_number.replace('- ', '').replace(' ', '_')\n\n    return clean_project_number + '_' + str(results_next_id).zfill(2)\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.find_matching_spatial_id","title":"<code>find_matching_spatial_id(absolute_file_path)</code>","text":"<p>Search for a matching entry for the absolute data path.</p> <p>Parameters:</p> Name Type Description Default <code>absolute_file_path</code> <code>str</code> <p>The absolute path of the data.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A matching project_spatial_id if it the absolute data path already exists in the dataframe, otherwise return None.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def find_matching_spatial_id(self, absolute_file_path: str) -&gt; str:\n    \"\"\"\n    Search for a matching entry for the absolute data path.\n\n    Args:\n        absolute_file_path (str): The absolute path of the data.\n\n    Returns:\n        str: A matching project_spatial_id if it the absolute data path already exists in the dataframe, otherwise return None.\n    \"\"\"        \n    return next(\n        (\n            project_spatial_id\n            for project_spatial_id, project_data in self.data_dict.items()\n            if project_data.get('absolute_file_path') == absolute_file_path\n        ),\n        None\n    )\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.get_data","title":"<code>get_data(project_spatial_id)</code>","text":"<p>Gets an object of values given a project spatial id.</p> <p>Parameters:</p> Name Type Description Default <code>project_spatial_id</code> <code>str</code> <p>Project spatial ID. Acts as key in dictionary.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>the values that correspond to the given key</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def get_data(self, project_spatial_id: str) -&gt; dict:\n    \"\"\"\n    Gets an object of values given a project spatial id.\n\n    Args:\n        project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n\n    Returns:\n        dict: the values that correspond to the given key\n    \"\"\"\n    return self.data_dict[project_spatial_id]\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.get_highest_suffix","title":"<code>get_highest_suffix(project_number)</code>","text":"<p>Get the highest suffix for the given project number in the spatial_project_id field.</p> <p>Parameters:</p> Name Type Description Default <code>project_number</code> <code>str</code> <p>The project number to find the highest suffix for.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The highest suffix found, or 0 if none are found.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def get_highest_suffix(self, project_number: str) -&gt; int:\n    \"\"\"\n    Get the highest suffix for the given project number in the spatial_project_id field.\n\n    Args:\n        project_number (str): The project number to find the highest suffix for.\n\n    Returns:\n        int: The highest suffix found, or 0 if none are found.\n    \"\"\"\n    suffixes = [\n        int(key.split('_')[-1])\n        for key in self.data_dict.keys()\n        if self.data_dict[key].get('project_number') == project_number\n    ]\n    return max(suffixes, default=0)\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.load_from_database","title":"<code>load_from_database()</code>","text":"<p>Load data from a database connection into class.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def load_from_database(self) -&gt; None:\n    \"\"\"\n    Load data from a database connection into class.\n    \"\"\"\n    self.database_connection.connect(self.database_parameters)\n\n    columns = ['project_spatial_id', 'project_number', 'dropped', 'raw_data_path','raw_gdb_path','absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image','extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type', 'created_at']\n\n    rows = self.database_connection.read(schema=self.database_connection.schema, table=self.database_connection.table, columns=columns)\n\n    for fields in rows:\n        project_spatial_id = fields[0]\n        values = dict(zip(columns[1:], fields[1:]))\n        self.data_dict[project_spatial_id] = values\n\n    self.database_connection.disconnect()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.load_from_file","title":"<code>load_from_file()</code>","text":"<p>Load data from a file into class.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def load_from_file(self) -&gt; None:\n    \"\"\"\n    Load data from a file into class.\n    \"\"\"\n    if not os.path.exists(self.datatracker):\n        return\n\n    data_df = pd.read_excel(self.datatracker, dtype={\n        'project_spatial_id': object, \n        'project_number': object,\n        'dropped': bool, \n        'raw_data_path': object, \n        'raw_gdb_path': object,\n        'absolute_file_path': object,\n        'in_raw_gdb': bool, \n        'contains_pdf': bool,\n        'contains_image': bool, \n        'extracted_attachments_path': object,\n        'editor_tracking_enabled': bool, \n        'processed': bool, \n        'entry_type': str\n    }, index_col=None)\n\n    for index, row in data_df.iterrows():\n        self.add_data(\n            project_spatial_id=row['project_spatial_id'],\n            project_number=row['project_number'], \n            dropped=row['dropped'],\n            raw_data_path=row['raw_data_path'], \n            raw_gdb_path=row['raw_gdb_path'], \n            absolute_file_path=row['absolute_file_path'],\n            in_raw_gdb=row['in_raw_gdb'], \n            contains_pdf=row['contains_pdf'], \n            contains_image=row['contains_image'],\n            extracted_attachments_path=row['extracted_attachments_path'],\n            editor_tracking_enabled=row['editor_tracking_enabled'],\n            processed=row['processed'], \n            entry_type=row['entry_type']\n        )\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.save_to_database","title":"<code>save_to_database(update=False)</code>","text":"<p>Save data tracker information to a database connection.</p> <p>Parameters:</p> Name Type Description Default <code>update</code> <code>bool</code> <p>Flag to determine if there are some entries in the data object that will need updating.</p> <code>False</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def save_to_database(self, update: bool = False) -&gt; None:\n    \"\"\"\n    Save data tracker information to a database connection.\n\n    Args:\n        update (bool): Flag to determine if there are some entries in the data object that will need updating.\n    \"\"\"\n    self.database_connection.connect(self.database_parameters)\n\n    rows = self.database_connection.read(\n        schema=self.database_connection.schema, \n        table=self.database_connection.table,\n        columns=['project_spatial_id']\n    )\n    existing_ids = set(row[0] for row in rows)\n\n    for key, value in self.data_dict.items():\n        try:\n            if key in existing_ids and update:\n                self.database_connection.update(\n                    schema=self.database_connection.schema, \n                    table=self.database_connection.table,\n                    values_dict={\n                        'dropped': value['dropped'],\n                        'in_raw_gdb': value['in_raw_gdb'], \n                        'contains_pdf': value['contains_pdf'], \n                        'contains_image': value['contains_image'],\n                        'editor_tracking_enabled': value['editor_tracking_enabled'], \n                        'processed': value['processed'], 'entry_type': value['entry_type']\n                    },\n                    condition=f\"project_spatial_id='{key}'\"\n                )\n\n            elif key not in existing_ids:\n                self.database_connection.create(\n                    schema=self.database_connection.schema, \n                    table=self.database_connection.table,\n                    columns=('project_spatial_id', 'project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type'),\n                    values=(\n                        key, \n                        value['project_number'], \n                        value['dropped'], \n                        value['raw_data_path'],\n                        value['raw_gdb_path'], \n                        value['absolute_file_path'], \n                        value['in_raw_gdb'], \n                        value['contains_pdf'], \n                        value['contains_image'],\n                        value['extracted_attachments_path'],\n                        value['editor_tracking_enabled'],\n                        value['processed'], \n                        value['entry_type']\n                    )\n                ) \n\n        except psycopg2.errors.ForeignKeyViolation as error:\n            log(self.log_path, Colors.ERROR, error)\n\n    self.database_connection.disconnect()\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.save_to_file","title":"<code>save_to_file()</code>","text":"<p>Save data tracker information to a file.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def save_to_file(self) -&gt; None:\n    \"\"\"\n    Save data tracker information to a file.\n    \"\"\"\n    # TODO: Work on getting the save to file exactly like save to database\n\n    # Create a DataFrame and save it to Excel if the data tracker file doesn't exist\n    df = pd.DataFrame(list(self.data_dict.values()), columns=['project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type'])\n\n    # Add 'project_spatial_id' to the DataFrame\n    df['project_spatial_id'] = list(self.data_dict.keys())\n\n    # Reorder columns to have 'project_spatial_id' as the first column\n    df = df[['project_spatial_id', 'project_number', 'dropped', 'raw_data_path', 'raw_gdb_path', 'absolute_file_path', 'in_raw_gdb', 'contains_pdf', 'contains_image', 'extracted_attachments_path', 'editor_tracking_enabled', 'processed', 'entry_type']]\n\n    # Sort the rows by the project_spatial_id column\n    df = df.sort_values(by=['project_spatial_id'])\n\n    # Check if the directory exists, if not, create it\n    directory = os.path.dirname(self.datatracker)\n    if not os.path.exists(directory):\n        os.makedirs(directory)\n\n    # Convert dataframe to excel\n    df.to_excel(self.datatracker, index=False)\n\n    log(None, Colors.INFO, f'The data tracker \"{self.datatracker}\" has been created/updated successfully.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Datatracker/#twobilliontoolkit.SpatialTransformer.Datatracker.Datatracker2BT.set_data","title":"<code>set_data(project_spatial_id, project_number=None, dropped=None, raw_data_path=None, absolute_file_path=None, in_raw_gdb=None, contains_pdf=None, contains_image=None, extracted_attachments_path=None, editor_tracking_enabled=None, processed=None, entry_type=None)</code>","text":"<p>Updates project data in the data tracker.</p> <p>Parameters:</p> Name Type Description Default <code>project_spatial_id</code> <code>str</code> <p>Project spatial ID. Acts as key in dictionary.</p> required <code>project_number</code> <code>str</code> <p>Project number (optional).</p> <code>None</code> <code>dropped</code> <code>bool</code> <p>Indicates whether the entry is dropped, non-valid etc.</p> <code>None</code> <code>raw_data_path</code> <code>str</code> <p>Raw data path (optional).</p> <code>None</code> <code>absolute_file_path</code> <code>str</code> <p>The full absolute file path.</p> <code>None</code> <code>in_raw_gdb</code> <code>bool</code> <p>Indicates whether data is in raw GDB (optional).</p> <code>None</code> <code>contains_pdf</code> <code>bool</code> <p>Indicates whether data contains PDF files (optional).</p> <code>None</code> <code>contains_image</code> <code>bool</code> <p>Indicates whether data contains image files (optional).</p> <code>None</code> <code>extracted_attachments_path</code> <code>str</code> <p>The path to the extracted attachments if applicable (optional).</p> <code>None</code> <code>editor_tracking_enabled</code> <code>bool</code> <p>Indicates whether the editor tracking has been enabled for the layer in the gdb (optional).</p> <code>None</code> <code>processed</code> <code>bool</code> <p>Indicates whether data has been processed yet (optional).            </p> <code>None</code> <code>entry_type</code> <code>str</code> <p>Indicates wether the entry contains information for an aspatial or spatial entry (optional).</p> <code>None</code> Source code in <code>twobilliontoolkit/SpatialTransformer/Datatracker.py</code> <pre><code>def set_data(self, project_spatial_id: str, project_number: str = None, dropped: bool = None, raw_data_path: str = None, absolute_file_path: str = None, in_raw_gdb: bool = None, contains_pdf: bool = None, contains_image: bool = None, extracted_attachments_path: str = None, editor_tracking_enabled: bool = None, processed: bool = None, entry_type: str = None) -&gt; None:\n    \"\"\"\n    Updates project data in the data tracker.\n\n    Args:\n        project_spatial_id (str): Project spatial ID. Acts as key in dictionary.\n        project_number (str): Project number (optional).\n        dropped (bool): Indicates whether the entry is dropped, non-valid etc.\n        raw_data_path (str): Raw data path (optional).\n        absolute_file_path (str): The full absolute file path.\n        in_raw_gdb (bool): Indicates whether data is in raw GDB (optional).\n        contains_pdf (bool): Indicates whether data contains PDF files (optional).\n        contains_image (bool): Indicates whether data contains image files (optional).\n        extracted_attachments_path (str): The path to the extracted attachments if applicable (optional).\n        editor_tracking_enabled (bool): Indicates whether the editor tracking has been enabled for the layer in the gdb (optional).\n        processed (bool): Indicates whether data has been processed yet (optional).            \n        entry_type (str): Indicates wether the entry contains information for an aspatial or spatial entry (optional).\n    \"\"\"\n    # Update specified parameters as sets\n    project_data = self.data_dict.get(project_spatial_id, {})\n    if project_number is not None:\n        project_data['project_number'] = project_number\n    if dropped is not None:\n        project_data['dropped'] = dropped\n    if raw_data_path is not None:\n        project_data['raw_data_path'] = raw_data_path\n    if absolute_file_path is not None:\n        project_data['absolute_file_path'] = absolute_file_path\n    if in_raw_gdb is not None:\n        project_data['in_raw_gdb'] = in_raw_gdb\n    if contains_pdf is not None:\n        project_data['contains_pdf'] = contains_pdf\n    if contains_image is not None:\n        project_data['contains_image'] = contains_image\n    if extracted_attachments_path is not None:\n        project_data['extracted_attachments_path'] = extracted_attachments_path\n    if editor_tracking_enabled is not None:\n        project_data['editor_tracking_enabled'] = editor_tracking_enabled\n    if processed is not None:\n        project_data['processed'] = processed\n    if entry_type is not None:\n        project_data['entry_type'] = entry_type\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/","title":"SpatialTransformer.Parameters","text":""},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters","title":"<code>Parameters</code>","text":"Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>class Parameters:\n    def __init__(self, input_path: str, output_path: str, gdb_path: str, master_data_path: str, datatracker: str, attachments: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None, debug: bool = False, resume: bool = False, suppress: bool = False, ps_script: str = None) -&gt; None:\n        \"\"\"\n        Initializes the Parameters class with input parameters.\n\n        Args:\n        - input_path (str): Path to input data.\n        - output_path (str): Path to output data.\n        - gdb_path (str): Path to save the GeoDatabase.\n        - master_data_path (str): Path to the aspatial master data.\n        - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from.\n        - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to.\n        - datatracker (str): Datatracker file name.\n        - attachments (str): Attachment folder name.\n        - log_path (str, optional): Path to log file. Defaults to an empty string.\n        - debug (bool, optional): Determines if the program is in debug mode.\n        - resume (bool, optional): Determines if the program should resume from where a crash happened.\n        - suppress (bool, optional): Determines if the program will suppress warnings to the command line.\n        - ps_script (str, optional): The path location of the script to run spatial transformer.\n        \"\"\"\n        self.local_dir = r'C:\\LocalTwoBillionToolkit'\n\n        # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool.\n        if attachments == '':\n            attachments = os.path.basename(gdb_path).replace('.gdb', '_Attachments')\n\n        # Ensure that if a datatracker is specified for loading or saving, then a path must be passed\n        if load_from == 'datatracker' or save_to == 'datatracker':\n            if not datatracker:\n                raise argparse.ArgumentTypeError(\"If --load or --save is 'datatracker', --datatracker must be specified.\")\n            else:\n                self.validate_path('datatracker', datatracker, must_ends_with='.xlsx') \n\n        if load_from == 'datatracker':\n            if not master_data_path:\n                raise argparse.ArgumentTypeError(\"If --load is 'datatracker', --master_data_path must be specified.\")\n            else:\n                self.validate_path('master_data_path', master_data_path, must_exists=True, must_ends_with='.xlsx') \n\n        # Validate and set paths\n        self.validate_path('input_path', input_path, must_exists=True)\n        self.validate_path('output_network_path', output_path)\n        self.validate_path('gdb_path', gdb_path, must_ends_with='.gdb')\n\n        # Build the paths\n        datatracker = os.path.join(self.local_dir, datatracker)\n        attachments = os.path.join(self.local_dir, attachments)\n\n        self.input = input_path\n        self.output = output_path\n        self.gdb_path = gdb_path\n        self.local_gdb_path = os.path.join(self.local_dir, os.path.basename(self.gdb_path))\n        self.load_from = load_from\n        self.save_to = save_to\n        self.datatracker = datatracker\n        self.attachments = attachments\n        self.database_config = database_config\n        self.log = log_path\n        self.debug = debug\n        self.resume = resume\n        self.suppress = suppress\n        self.ps_script = ps_script\n\n        self.project_numbers = self.get_project_numbers(master_data_path)\n\n    def validate_path(self, argument: str, param: str, must_exists: bool = False, must_ends_with: bool = None) -&gt; None:\n        \"\"\"\n        Validates the given path.\n\n        Args:\n        - argument (str): The key value of the param passed.\n        - param (str): Path to be validated.\n        - must_exists (bool, optional): If True, the path must exist. Defaults to False.\n        - must_ends_with (str, optional): If provided, the path must end with this string.\n\n        Raises:\n        - ValueError: If the path is not valid according to the specified conditions.\n        \"\"\"\n        if not isinstance(param, str) or not param.strip():\n            raise ValueError(f'{argument}: {param} must be a non-empty string.')\n\n        if must_ends_with is not None and not param.endswith(must_ends_with):\n            raise ValueError(f'{argument}: {param} must be of type {must_ends_with}.')\n\n        if must_exists and not os.path.exists(param):\n            raise ValueError(f'{argument}: {param} path does not exist.')\n\n    def handle_unzip(self) -&gt; None:\n        \"\"\"\n        Handles the unzipping process using ripple_unzip.\n\n        Calls the ripple_unzip function with input, output, and log paths.\n        \"\"\"\n        # If the resume after crash flag was specified, skip\n        if self.resume:\n            return\n\n        ripple_unzip(self.input, self.output, self.log)\n\n    def create_gdb(self) -&gt; None:\n        \"\"\"\n        Creates a geodatabase file if it does not already exist.\n\n        If the specified geodatabase file does not exist, it attempts to create one.\n        \"\"\"\n        # If the resume after crash flag was specified, skip\n        if self.resume:\n            return\n\n        # Create the .gdb if it does not already exists\n        if not arcpy.Exists(self.local_gdb_path):\n            try:\n                # Create the directory if it does not exist\n                directory_path = os.path.dirname(self.local_gdb_path)\n                if not os.path.exists(directory_path):\n                    os.makedirs(directory_path)\n\n                # Create the file geodatabase\n                file = os.path.basename(self.local_gdb_path)\n                arcpy.management.CreateFileGDB(directory_path, file)\n\n                log(None, Colors.INFO, f'Geodatabase: {file} created successfully')\n            except arcpy.ExecuteError:\n                log(self.log, Colors.ERROR, arcpy.GetMessages(2), ps_script=self.ps_script)\n\n    def get_project_numbers(self, master_datasheet: str = None) -&gt; list[str]:\n        \"\"\"\n        Get a list of project numbers from either the database or the master datasheet\n\n        Returns:\n            list[str]: A list of project numbers\n        \"\"\"\n        if self.load_from == 'datatracker':                      \n            masterdata = pd.read_excel(master_datasheet)\n\n            # Extra validation on master data to check it has project number column\n            if 'BT_Legacy_Project_ID__c' not in masterdata.columns:\n                raise ValueError(f\"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\")\n\n            # Convert masterdata to a list of strings\n            return masterdata['BT_Legacy_Project_ID__c'].unique().tolist()\n\n        # Create database object\n        database_connection = Database()\n\n        # Read connection parameters from the configuration file\n        database_parameters = database_connection.get_params(config_path=self.database_config)\n        database_connection.connect(database_parameters)\n        self.project_numbers = database_connection.read(\n            database_connection.schema,\n            table='project_number'\n        )\n        database_connection.disconnect()\n\n        return [str(num[0]) for num in self.project_numbers]\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.__init__","title":"<code>__init__(input_path, output_path, gdb_path, master_data_path, datatracker, attachments, load_from='database', save_to='database', database_config=None, log_path=None, debug=False, resume=False, suppress=False, ps_script=None)</code>","text":"<p>Initializes the Parameters class with input parameters.</p> <p>Args: - input_path (str): Path to input data. - output_path (str): Path to output data. - gdb_path (str): Path to save the GeoDatabase. - master_data_path (str): Path to the aspatial master data. - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from. - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to. - datatracker (str): Datatracker file name. - attachments (str): Attachment folder name. - log_path (str, optional): Path to log file. Defaults to an empty string. - debug (bool, optional): Determines if the program is in debug mode. - resume (bool, optional): Determines if the program should resume from where a crash happened. - suppress (bool, optional): Determines if the program will suppress warnings to the command line. - ps_script (str, optional): The path location of the script to run spatial transformer.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>def __init__(self, input_path: str, output_path: str, gdb_path: str, master_data_path: str, datatracker: str, attachments: str, load_from: str = 'database', save_to: str = 'database', database_config: str = None, log_path: str = None, debug: bool = False, resume: bool = False, suppress: bool = False, ps_script: str = None) -&gt; None:\n    \"\"\"\n    Initializes the Parameters class with input parameters.\n\n    Args:\n    - input_path (str): Path to input data.\n    - output_path (str): Path to output data.\n    - gdb_path (str): Path to save the GeoDatabase.\n    - master_data_path (str): Path to the aspatial master data.\n    - load_from (str): Either 'database' or 'datatracker' to determine what to load the data from.\n    - save_to (str): Either 'database' or 'datatracker' to determine what to save the data to.\n    - datatracker (str): Datatracker file name.\n    - attachments (str): Attachment folder name.\n    - log_path (str, optional): Path to log file. Defaults to an empty string.\n    - debug (bool, optional): Determines if the program is in debug mode.\n    - resume (bool, optional): Determines if the program should resume from where a crash happened.\n    - suppress (bool, optional): Determines if the program will suppress warnings to the command line.\n    - ps_script (str, optional): The path location of the script to run spatial transformer.\n    \"\"\"\n    self.local_dir = r'C:\\LocalTwoBillionToolkit'\n\n    # If nothing was specified for the attachments path, set it to the same place as the output of the ripple unzipple tool.\n    if attachments == '':\n        attachments = os.path.basename(gdb_path).replace('.gdb', '_Attachments')\n\n    # Ensure that if a datatracker is specified for loading or saving, then a path must be passed\n    if load_from == 'datatracker' or save_to == 'datatracker':\n        if not datatracker:\n            raise argparse.ArgumentTypeError(\"If --load or --save is 'datatracker', --datatracker must be specified.\")\n        else:\n            self.validate_path('datatracker', datatracker, must_ends_with='.xlsx') \n\n    if load_from == 'datatracker':\n        if not master_data_path:\n            raise argparse.ArgumentTypeError(\"If --load is 'datatracker', --master_data_path must be specified.\")\n        else:\n            self.validate_path('master_data_path', master_data_path, must_exists=True, must_ends_with='.xlsx') \n\n    # Validate and set paths\n    self.validate_path('input_path', input_path, must_exists=True)\n    self.validate_path('output_network_path', output_path)\n    self.validate_path('gdb_path', gdb_path, must_ends_with='.gdb')\n\n    # Build the paths\n    datatracker = os.path.join(self.local_dir, datatracker)\n    attachments = os.path.join(self.local_dir, attachments)\n\n    self.input = input_path\n    self.output = output_path\n    self.gdb_path = gdb_path\n    self.local_gdb_path = os.path.join(self.local_dir, os.path.basename(self.gdb_path))\n    self.load_from = load_from\n    self.save_to = save_to\n    self.datatracker = datatracker\n    self.attachments = attachments\n    self.database_config = database_config\n    self.log = log_path\n    self.debug = debug\n    self.resume = resume\n    self.suppress = suppress\n    self.ps_script = ps_script\n\n    self.project_numbers = self.get_project_numbers(master_data_path)\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.create_gdb","title":"<code>create_gdb()</code>","text":"<p>Creates a geodatabase file if it does not already exist.</p> <p>If the specified geodatabase file does not exist, it attempts to create one.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>def create_gdb(self) -&gt; None:\n    \"\"\"\n    Creates a geodatabase file if it does not already exist.\n\n    If the specified geodatabase file does not exist, it attempts to create one.\n    \"\"\"\n    # If the resume after crash flag was specified, skip\n    if self.resume:\n        return\n\n    # Create the .gdb if it does not already exists\n    if not arcpy.Exists(self.local_gdb_path):\n        try:\n            # Create the directory if it does not exist\n            directory_path = os.path.dirname(self.local_gdb_path)\n            if not os.path.exists(directory_path):\n                os.makedirs(directory_path)\n\n            # Create the file geodatabase\n            file = os.path.basename(self.local_gdb_path)\n            arcpy.management.CreateFileGDB(directory_path, file)\n\n            log(None, Colors.INFO, f'Geodatabase: {file} created successfully')\n        except arcpy.ExecuteError:\n            log(self.log, Colors.ERROR, arcpy.GetMessages(2), ps_script=self.ps_script)\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.get_project_numbers","title":"<code>get_project_numbers(master_datasheet=None)</code>","text":"<p>Get a list of project numbers from either the database or the master datasheet</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: A list of project numbers</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>def get_project_numbers(self, master_datasheet: str = None) -&gt; list[str]:\n    \"\"\"\n    Get a list of project numbers from either the database or the master datasheet\n\n    Returns:\n        list[str]: A list of project numbers\n    \"\"\"\n    if self.load_from == 'datatracker':                      \n        masterdata = pd.read_excel(master_datasheet)\n\n        # Extra validation on master data to check it has project number column\n        if 'BT_Legacy_Project_ID__c' not in masterdata.columns:\n            raise ValueError(f\"The column 'BT_Legacy_Project_ID__c' does not exist in the master data.\")\n\n        # Convert masterdata to a list of strings\n        return masterdata['BT_Legacy_Project_ID__c'].unique().tolist()\n\n    # Create database object\n    database_connection = Database()\n\n    # Read connection parameters from the configuration file\n    database_parameters = database_connection.get_params(config_path=self.database_config)\n    database_connection.connect(database_parameters)\n    self.project_numbers = database_connection.read(\n        database_connection.schema,\n        table='project_number'\n    )\n    database_connection.disconnect()\n\n    return [str(num[0]) for num in self.project_numbers]\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.handle_unzip","title":"<code>handle_unzip()</code>","text":"<p>Handles the unzipping process using ripple_unzip.</p> <p>Calls the ripple_unzip function with input, output, and log paths.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>def handle_unzip(self) -&gt; None:\n    \"\"\"\n    Handles the unzipping process using ripple_unzip.\n\n    Calls the ripple_unzip function with input, output, and log paths.\n    \"\"\"\n    # If the resume after crash flag was specified, skip\n    if self.resume:\n        return\n\n    ripple_unzip(self.input, self.output, self.log)\n</code></pre>"},{"location":"pages/SpatialTransformer/Parameters/#twobilliontoolkit.SpatialTransformer.Parameters.Parameters.validate_path","title":"<code>validate_path(argument, param, must_exists=False, must_ends_with=None)</code>","text":"<p>Validates the given path.</p> <p>Args: - argument (str): The key value of the param passed. - param (str): Path to be validated. - must_exists (bool, optional): If True, the path must exist. Defaults to False. - must_ends_with (str, optional): If provided, the path must end with this string.</p> <p>Raises: - ValueError: If the path is not valid according to the specified conditions.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Parameters.py</code> <pre><code>def validate_path(self, argument: str, param: str, must_exists: bool = False, must_ends_with: bool = None) -&gt; None:\n    \"\"\"\n    Validates the given path.\n\n    Args:\n    - argument (str): The key value of the param passed.\n    - param (str): Path to be validated.\n    - must_exists (bool, optional): If True, the path must exist. Defaults to False.\n    - must_ends_with (str, optional): If provided, the path must end with this string.\n\n    Raises:\n    - ValueError: If the path is not valid according to the specified conditions.\n    \"\"\"\n    if not isinstance(param, str) or not param.strip():\n        raise ValueError(f'{argument}: {param} must be a non-empty string.')\n\n    if must_ends_with is not None and not param.endswith(must_ends_with):\n        raise ValueError(f'{argument}: {param} must be of type {must_ends_with}.')\n\n    if must_exists and not os.path.exists(param):\n        raise ValueError(f'{argument}: {param} path does not exist.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/","title":"SpatialTransformer.Processor","text":""},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor","title":"<code>Processor</code>","text":"Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>class Processor:\n    def __init__(self, params: Parameters) -&gt; None:\n        \"\"\"\n        Initializes the Processor class with input parameters.\n\n        Args:\n            params (Parameters): Instance of the Parameters class.\n        \"\"\"\n        self.params = params\n\n        # Create the Data class to hold any data tracker information\n        self.data = Datatracker2BT(params.datatracker, params.load_from, params.save_to, params.database_config)\n\n    def del_gdb(self) -&gt; None:\n        \"\"\"\n        Removes the local Geodatabase and folder after the processing has been completed.\n        \"\"\"\n        arcpy.Delete_management(self.params.local_gdb_path)\n        arcpy.Delete_management(self.params.local_dir)\n\n    def create_datatracker_entries(self) -&gt; None:\n        \"\"\"\n        Creates data tracker entries by processing files and directories within the output path.\n\n        This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files,\n        GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker.\n        \"\"\"  \n        # Step through unzip output path\n        for root, dirs, files in os.walk(self.params.output):\n            for dir in dirs:\n                # Built full directory path\n                directory_path = f\"{root}\\{dir}\"\n\n                # Skip over entiries if in resume mode\n                if self.params.resume:\n                    (_, data_entry) = self.data.find_matching_data(absolute_file_path=convert_drive_path(directory_path))\n                    if data_entry:\n                        continue\n\n                # Skip over the gdb if tool is running a second time if it is somehow in the folder\n                if dir is self.params.gdb_path:\n                    continue\n\n                if dir.endswith('.gdb'):\n                    # Set the workspace to the specified .gdb\n                    arcpy.env.workspace = directory_path\n\n                    # Iterate through the feature classes and tables\n                    feature_tables = arcpy.ListTables()\n                    feature_classes = arcpy.ListFeatureClasses()\n                    for feature in feature_classes + feature_tables:\n                        if arcpy.Exists(feature) and arcpy.Describe(feature).dataType == 'FeatureClass':\n                            project_spatial_id = self.create_entry(directory_path, f\"{directory_path}\\{feature}\")\n\n                    # Remove the gdb from the dirs list so it doesnt walk through    \n                    dirs.remove(dir)\n\n            for file in files:\n                # Built full file path\n                file_path = f\"{root}\\{file}\"\n\n                # Ignore specified file extensions\n                lowercase_file = file.lower()\n                if lowercase_file.endswith(IGNORE_EXTENSIONS):\n                    continue\n\n                # Skip over entiries if in resume mode\n                if self.params.resume:\n                    (_, data_entry) = self.data.find_matching_data(absolute_file_path=convert_drive_path(file_path))\n                    if data_entry:\n                        continue\n\n                if lowercase_file.endswith(LAYOUT_FILE_EXTENSIONS):\n                    project_spatial_id = self.create_entry(file_path, file_path, entry_type='Aspatial', processed=True)\n\n                    # Log it\n                    log(self.params.log, Colors.WARNING, f'Layout file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)    \n\n                elif lowercase_file.endswith(DATA_SHEET_EXTENSIONS):\n                    project_spatial_id = self.create_entry(file_path, file_path, entry_type='Aspatial', processed=True)\n\n                    # Log it\n                    log(self.params.log, Colors.WARNING, f'Datasheet: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n                elif lowercase_file.endswith(IMAGE_FILE_EXTENSIONS):\n                    if lowercase_file.endswith('.pdf'):\n                        project_spatial_id = self.create_entry(file_path, file_path, contains_pdf=True, entry_type='Aspatial', processed=True)                \n                    else:\n                        project_spatial_id = self.create_entry(file_path, file_path, contains_image=True, entry_type='Aspatial', processed=True)\n\n                    if self.params.debug:                  \n                        # Log it\n                        log(self.params.log, Colors.WARNING, f'Image/PDF file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n                elif lowercase_file.endswith('.shp'):\n                    project_spatial_id = self.create_entry(file_path, file_path)\n\n                elif lowercase_file.endswith(('.kml', '.kmz')):\n                    try:\n                        contain_point = False\n                        contain_polygon = False\n                        contain_linestring = False\n                        layers = fiona.listlayers(file_path) \n\n                        # Iterate through layers and check their geometry type\n                        for layer in layers:\n                            with fiona.open(file_path, 'r', driver='LIBKML', layer=layer) as src:\n                                for feat in src:\n                                    if contain_point and contain_polygon and contain_linestring:\n                                        break\n\n                                    geom_type = feat.geometry.type\n                                    if geom_type == 'Point':\n                                        contain_point = True\n                                    elif geom_type == 'Polygon':\n                                        contain_polygon = True\n                                    elif geom_type == 'LineString':\n                                        contain_linestring = True\n\n                        if contain_point:\n                            project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Points\")\n                        if contain_polygon:\n                            project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Polygons\")\n                        if contain_linestring:\n                            project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Lines\")\n                    except Exception as error:\n                        log(self.params.log, Colors.ERROR, f'KML/KMZ file: {file_path} has encountered an error when making a datatracker entry. {error}')\n\n                elif lowercase_file.endswith('.geojson'):\n                    project_spatial_id = self.create_entry(file_path, file_path)\n\n                elif lowercase_file.endswith(('.gpkg', '.sqlite')):\n                    project_spatial_id = self.create_entry(file_path, file_path, processed=True)\n\n                    # Log it\n                    log(self.params.log, Colors.WARNING, f'GeoPackage/SQLite file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n                else:                    \n                    # Log it\n                    log(self.params.log, Colors.WARNING, f'Unsupported Filetype: {file_path} has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.', self.params.suppress, ps_script=self.params.ps_script)\n\n    def create_entry(self, absolute_path: str, feature_path: str, in_raw_gdb: bool = False, contains_pdf: bool = False, contains_image: bool = False, entry_type: str = 'Spatial', processed: bool = False) -&gt; str:\n        \"\"\"\n        Creates a new entry in the data dictionary for spatial data processing.\n\n        This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes.\n\n        Args:\n            absolute_path (str): The absolute path of the input file.\n            feature_path (str): The path to the feature data.\n            in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False.\n            contains_pdf (bool): Indicates if the entry contains a PDF. Default is False.\n            contains_image (bool): Indicates if the entry contains an image. Default is False.\n            entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'.\n            processed (bool): Indicates if the entry has been processed. Default is False.\n\n        Returns:\n            str: The formatted project spatial ID.\n        \"\"\"\n        # Check project numbers and format the result\n        formatted_result = self.check_project_numbers(feature_path)\n        formatted_result = formatted_result.upper()\n\n        # Create a unique identifier for the project spatial ID\n        formatted_project_spatial_id = self.data.create_project_spatial_id(formatted_result)\n\n        # Print file information if debugging is enabled\n        if self.params.debug:\n            log(None, Colors.INFO, feature_path)\n            log(None, Colors.INFO, formatted_project_spatial_id)\n\n        # Convert the raw data path to a relative path           \n        raw_data_path = os.path.relpath(feature_path, self.params.output)\n\n        # Convert the absolute path to the correct drive path format\n        absolute_file_path = convert_drive_path(absolute_path)\n\n        # Call a method to process raw data matching\n        self.call_raw_data_match(formatted_project_spatial_id, absolute_file_path)\n\n        # Add data to the data class \n        self.data.add_data(\n            project_spatial_id=formatted_project_spatial_id,\n            project_number=formatted_result, \n            dropped=False,\n            raw_data_path=raw_data_path, \n            raw_gdb_path=convert_drive_path(self.params.gdb_path),\n            absolute_file_path=absolute_file_path,\n            in_raw_gdb=in_raw_gdb, \n            contains_pdf=contains_pdf, \n            contains_image=contains_image,\n            extracted_attachments_path=None,\n            editor_tracking_enabled=False,\n            processed=processed, \n            entry_type=entry_type\n        )\n\n        return formatted_project_spatial_id\n\n    def process_entries(self) -&gt; None:\n        \"\"\"\n        Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking.\n\n        The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status.\n        \"\"\"\n        # Iterate over each entry in the data dictionary\n        for index, entry in enumerate(self.data.data_dict):\n            # Get the absolute path of the entry file\n            entry_absolute_path:str = self.data.data_dict[entry].get('absolute_file_path')\n            try:\n                # Check if the current entry has already been processed\n                if self.data.data_dict[entry].get('processed'):\n                    continue\n\n                # Check if the created_by date of the entry is within the same day as now            \n                created_at = self.data.data_dict[entry].get('created_at')\n                if self.params.load_from == 'database' and created_at:\n                    now = datetime.datetime.now()\n                    if created_at.date() != now.date():\n                        continue\n\n                # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase \n                gdb_entry_name = f\"proj_{entry}\"\n                feature_gdb_path = os.path.join(self.params.local_gdb_path, gdb_entry_name)\n\n                # Check the file type and export features accordingly\n                if entry_absolute_path.endswith('.gdb'):\n                    # Export features from one geodatabase to the output geodatabase\n                    arcpy.conversion.ExportFeatures(\n                        os.path.join(self.params.output, self.data.data_dict[entry].get('raw_data_path')),\n                        feature_gdb_path\n                    )\n\n                elif entry_absolute_path.endswith('.shp'):\n                    # Export features from shapefile to the output geodatabase\n                    arcpy.conversion.ExportFeatures(\n                        entry_absolute_path,\n                        feature_gdb_path\n                    )\n\n                elif entry_absolute_path.endswith(('.kml', '.kmz')):\n                    # Initialize an empty GeoDataFrame\n                    data = gpd.GeoDataFrame()\n\n                    # List all layers in the KML/KMZ file\n                    layers = fiona.listlayers(entry_absolute_path)\n                    for layer in layers:\n                        # Read each layer into a temporary GeoDataFrame\n                        layer_gdb = gpd.read_file(entry_absolute_path, driver='LIBKML', layer=layer)\n\n                        # Check if the temporary GeoDataFrame is empty before concatenating\n                        if not layer_gdb.empty:\n                            data = pd.concat([data, layer_gdb], ignore_index=True)\n\n                    # Rename the 'OBJECTID' column if it exists\n                    if 'OBJECTID' in data.columns:\n                        data.rename(columns={'OBJECTID': 'OBJECTID_STRING'}, inplace=True)\n\n                    # Convert all datetime columns to string with specified utc=True\n                    for col in data.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns:\n                        data[col] = pd.to_datetime(data[col], utc=True, errors='coerce').dt.strftime('%Y-%m-%d')\n\n                    # Check if 'timestamp' column exists\n                    if 'timestamp' in data.columns:\n                        data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True, errors='coerce').dt.strftime('%Y-%m-%d')\n\n                    # Determine the path for raw data and entry basename\n                    raw_data_path = self.data.data_dict[entry].get('raw_data_path')\n                    entry_data_basename = os.path.basename(raw_data_path)\n\n                    # Export the GeoDataFrame to the appropriate feature class based on geometry type\n                    if entry_data_basename == 'Points':\n                        points_gdf = data[data.geometry.type == 'Point']\n                        points_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)\n                    elif entry_data_basename == 'Polygons':\n                        polygons_gdf = data[data.geometry.type == 'Polygon']\n                        polygons_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)\n                    elif entry_data_basename == 'Lines':\n                        lines_gdf = data[data.geometry.type == 'LineString']\n                        lines_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)     \n\n                elif entry_absolute_path.endswith('.geojson'):\n                    # Export features from GeoJSON to the output geodatabase\n                    arcpy.conversion.JSONToFeatures(\n                        entry_absolute_path,\n                        feature_gdb_path\n                    )\n\n                # Update the entry status to indicate it has been processed and exists in raw geodatabase format\n                self.data.set_data(\n                    project_spatial_id=entry, \n                    in_raw_gdb=True,\n                    processed=True\n                )\n\n                # Enable the version control of the layer in the geodatabase\n                self.enable_version_control(feature_gdb_path)\n\n                # Update the entry status to indicate that editor tracking is enabled\n                self.data.set_data(\n                    project_spatial_id=entry, \n                    editor_tracking_enabled=True\n                )\n\n            except (arcpy.ExecuteError, arcgisscripting.ExecuteError) as error:\n                log(self.params.log, Colors.ERROR, f'An error occurred when processing the layer for {entry_absolute_path}, you can fix or remove it from the datatracker/database, then run the command again with --resume\\n{error}', ps_script=self.params.ps_script, project_id=entry)\n                # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on\n                # raise Exception(error) \n            except Exception as error:\n                log(self.params.log, Colors.ERROR, f'An uncaught error occurred when processing the layer for {entry_absolute_path}', ps_script=self.params.ps_script, project_id=entry)\n                raise Exception(error)\n\n    def check_project_numbers(self, file_path: str) -&gt; str:\n        \"\"\"\n        Check project numbers against a master data sheet.\n\n        Args:\n            file_path (str): Filepath to check.            \n\n        Returns:\n            str: the formatted result for the project number of the spatial file path.\n        \"\"\"\n        # Define a regular expression pattern to extract project numbers and search for pattern in file path\n        pattern = r'(\\d{4})[\\s_\u2013-]*([a-zA-Z]{3})[\\s_\u2013-]*(\\d{3})'\n        search = re.search(pattern, file_path)\n\n        # If no match is found, log a warning and assign an arbitrary project number\n        if not search:\n            log(self.params.log, Colors.WARNING, f'Could not find a project number for: {file_path} - giving it an arbitrary project number \"0000 XXX - 000\"', self.params.suppress, ps_script=self.params.ps_script)\n            formatted_result = '0000 XXX - 000'\n        else:\n            # Format the result using the matched groups\n            formatted_result = '{} {} - {}'.format(search.group(1), search.group(2), search.group(3))\n\n            # Check if the project number is in the project numbers list\n            project_found = None\n            for project_number in self.params.project_numbers:\n                if project_number.lower().replace(' ', '') == formatted_result.lower().replace(' ', ''):\n                    # Exit the loop as soon as the first occurrence is found\n                    project_found = project_number\n                    break\n\n            if not project_found:\n                log(self.params.log, Colors.WARNING, f'The project number {formatted_result} does not match a project number in the provided source - giving it an arbitrary project number \"0000 XXX - 000\"', self.params.suppress, ps_script=self.params.ps_script)\n                formatted_result = '0000 XXX - 000'\n\n        return formatted_result\n\n    def call_raw_data_match(self, current_spatial_id: str, absolute_file_path: str) -&gt; None:\n        \"\"\"\n        Call the method that finds a matching raw data path and returns the project spatial id.\n\n        Args:\n            current_spatial_id (str): The current spatial project id being checked.\n            absolute_file_path (str): The absolute data path to be searched in the dictionary.\n        \"\"\"\n        # Find a corresponding project spatial ID in the data dictionary based on the raw data path\n        (found_match, _) = self.data.find_matching_data(absolute_file_path=absolute_file_path)\n        if found_match is not None:\n            log(self.params.log, Colors.WARNING, f'Absolute Path: {absolute_file_path} already exists in the data tracker! -  Current Spatial ID: {current_spatial_id} Matching Spatial ID: {found_match}', self.params.suppress, ps_script=self.params.ps_script, project_id=current_spatial_id)    \n\n    def extract_attachments(self) -&gt; None:\n        \"\"\"\n        Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB.\n        \"\"\"        \n        # Find and process attachments from the gdb\n        attachment_dict = find_attachments(self.params.local_gdb_path, self.params.attachments)\n\n        # Print file information if debugging is enabled\n        if self.params.debug:\n            log(None, Colors.INFO, attachment_dict)\n\n        # Iterating through key-value pairs using items()\n        for key, value in attachment_dict.items():\n            # Update the entry of the Geodatabase feature class\n            self.data.set_data(\n                project_spatial_id=key.replace('proj_', ''), \n                # extracted_attachments_path=value\n                extracted_attachments_path=os.path.join(self.params.gdb_path, value.replace(f\"C:\\LocalTwoBillionToolkit\\\\\", ''))\n            )\n\n        # Log completion of this task\n        log(None, Colors.INFO, 'All attachments have been extracted from the result Geodatabase.')\n\n    def enable_version_control(self, feature_class) -&gt; None:\n        \"\"\"\n        Enable the editor tracking version control for a feature class in the Geodatabase.\n        \"\"\"\n        try:\n            # Set the arc environement to the resulting GDB\n            arcpy.env.workspace = self.params.local_gdb_path\n\n            # Add a site id for mapping in a later tool\n            arcpy.management.AddField(\n                feature_class, \n                'bt_site_id',\n                'SHORT'               \n            )\n\n            # Enable the 4 fields for editor tracking\n            arcpy.EnableEditorTracking_management(feature_class, \"bt_created_by\", \"bt_date_created\", \"bt_last_edited_by\", \"bt_date_edited\", \"ADD_FIELDS\", \"UTC\")\n\n            # Set flag in data object for editor tracking to True\n            self.data.set_data(\n                project_spatial_id=os.path.basename(feature_class).replace('proj_', ''),\n                editor_tracking_enabled=True\n            )\n\n        except Exception as error:\n            log(self.params.log, Colors.ERROR, f'An error has been caught while trying to enable editor tracking for {feature_class} in resulting gdb, {error}', ps_script=self.params.ps_script)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.__init__","title":"<code>__init__(params)</code>","text":"<p>Initializes the Processor class with input parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Parameters</code> <p>Instance of the Parameters class.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def __init__(self, params: Parameters) -&gt; None:\n    \"\"\"\n    Initializes the Processor class with input parameters.\n\n    Args:\n        params (Parameters): Instance of the Parameters class.\n    \"\"\"\n    self.params = params\n\n    # Create the Data class to hold any data tracker information\n    self.data = Datatracker2BT(params.datatracker, params.load_from, params.save_to, params.database_config)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.call_raw_data_match","title":"<code>call_raw_data_match(current_spatial_id, absolute_file_path)</code>","text":"<p>Call the method that finds a matching raw data path and returns the project spatial id.</p> <p>Parameters:</p> Name Type Description Default <code>current_spatial_id</code> <code>str</code> <p>The current spatial project id being checked.</p> required <code>absolute_file_path</code> <code>str</code> <p>The absolute data path to be searched in the dictionary.</p> required Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def call_raw_data_match(self, current_spatial_id: str, absolute_file_path: str) -&gt; None:\n    \"\"\"\n    Call the method that finds a matching raw data path and returns the project spatial id.\n\n    Args:\n        current_spatial_id (str): The current spatial project id being checked.\n        absolute_file_path (str): The absolute data path to be searched in the dictionary.\n    \"\"\"\n    # Find a corresponding project spatial ID in the data dictionary based on the raw data path\n    (found_match, _) = self.data.find_matching_data(absolute_file_path=absolute_file_path)\n    if found_match is not None:\n        log(self.params.log, Colors.WARNING, f'Absolute Path: {absolute_file_path} already exists in the data tracker! -  Current Spatial ID: {current_spatial_id} Matching Spatial ID: {found_match}', self.params.suppress, ps_script=self.params.ps_script, project_id=current_spatial_id)    \n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.check_project_numbers","title":"<code>check_project_numbers(file_path)</code>","text":"<p>Check project numbers against a master data sheet.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Filepath to check.            </p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>the formatted result for the project number of the spatial file path.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def check_project_numbers(self, file_path: str) -&gt; str:\n    \"\"\"\n    Check project numbers against a master data sheet.\n\n    Args:\n        file_path (str): Filepath to check.            \n\n    Returns:\n        str: the formatted result for the project number of the spatial file path.\n    \"\"\"\n    # Define a regular expression pattern to extract project numbers and search for pattern in file path\n    pattern = r'(\\d{4})[\\s_\u2013-]*([a-zA-Z]{3})[\\s_\u2013-]*(\\d{3})'\n    search = re.search(pattern, file_path)\n\n    # If no match is found, log a warning and assign an arbitrary project number\n    if not search:\n        log(self.params.log, Colors.WARNING, f'Could not find a project number for: {file_path} - giving it an arbitrary project number \"0000 XXX - 000\"', self.params.suppress, ps_script=self.params.ps_script)\n        formatted_result = '0000 XXX - 000'\n    else:\n        # Format the result using the matched groups\n        formatted_result = '{} {} - {}'.format(search.group(1), search.group(2), search.group(3))\n\n        # Check if the project number is in the project numbers list\n        project_found = None\n        for project_number in self.params.project_numbers:\n            if project_number.lower().replace(' ', '') == formatted_result.lower().replace(' ', ''):\n                # Exit the loop as soon as the first occurrence is found\n                project_found = project_number\n                break\n\n        if not project_found:\n            log(self.params.log, Colors.WARNING, f'The project number {formatted_result} does not match a project number in the provided source - giving it an arbitrary project number \"0000 XXX - 000\"', self.params.suppress, ps_script=self.params.ps_script)\n            formatted_result = '0000 XXX - 000'\n\n    return formatted_result\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.create_datatracker_entries","title":"<code>create_datatracker_entries()</code>","text":"<p>Creates data tracker entries by processing files and directories within the output path.</p> <p>This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files, GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def create_datatracker_entries(self) -&gt; None:\n    \"\"\"\n    Creates data tracker entries by processing files and directories within the output path.\n\n    This function walks through the specified output directory, processes different file types, and creates entries in the data tracker. It handles geodatabases, shapefiles, KML/KMZ files,\n    GeoJSON files, GeoPackages, and other file types, ensuring that they are correctly added to the data tracker.\n    \"\"\"  \n    # Step through unzip output path\n    for root, dirs, files in os.walk(self.params.output):\n        for dir in dirs:\n            # Built full directory path\n            directory_path = f\"{root}\\{dir}\"\n\n            # Skip over entiries if in resume mode\n            if self.params.resume:\n                (_, data_entry) = self.data.find_matching_data(absolute_file_path=convert_drive_path(directory_path))\n                if data_entry:\n                    continue\n\n            # Skip over the gdb if tool is running a second time if it is somehow in the folder\n            if dir is self.params.gdb_path:\n                continue\n\n            if dir.endswith('.gdb'):\n                # Set the workspace to the specified .gdb\n                arcpy.env.workspace = directory_path\n\n                # Iterate through the feature classes and tables\n                feature_tables = arcpy.ListTables()\n                feature_classes = arcpy.ListFeatureClasses()\n                for feature in feature_classes + feature_tables:\n                    if arcpy.Exists(feature) and arcpy.Describe(feature).dataType == 'FeatureClass':\n                        project_spatial_id = self.create_entry(directory_path, f\"{directory_path}\\{feature}\")\n\n                # Remove the gdb from the dirs list so it doesnt walk through    \n                dirs.remove(dir)\n\n        for file in files:\n            # Built full file path\n            file_path = f\"{root}\\{file}\"\n\n            # Ignore specified file extensions\n            lowercase_file = file.lower()\n            if lowercase_file.endswith(IGNORE_EXTENSIONS):\n                continue\n\n            # Skip over entiries if in resume mode\n            if self.params.resume:\n                (_, data_entry) = self.data.find_matching_data(absolute_file_path=convert_drive_path(file_path))\n                if data_entry:\n                    continue\n\n            if lowercase_file.endswith(LAYOUT_FILE_EXTENSIONS):\n                project_spatial_id = self.create_entry(file_path, file_path, entry_type='Aspatial', processed=True)\n\n                # Log it\n                log(self.params.log, Colors.WARNING, f'Layout file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)    \n\n            elif lowercase_file.endswith(DATA_SHEET_EXTENSIONS):\n                project_spatial_id = self.create_entry(file_path, file_path, entry_type='Aspatial', processed=True)\n\n                # Log it\n                log(self.params.log, Colors.WARNING, f'Datasheet: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n            elif lowercase_file.endswith(IMAGE_FILE_EXTENSIONS):\n                if lowercase_file.endswith('.pdf'):\n                    project_spatial_id = self.create_entry(file_path, file_path, contains_pdf=True, entry_type='Aspatial', processed=True)                \n                else:\n                    project_spatial_id = self.create_entry(file_path, file_path, contains_image=True, entry_type='Aspatial', processed=True)\n\n                if self.params.debug:                  \n                    # Log it\n                    log(self.params.log, Colors.WARNING, f'Image/PDF file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n            elif lowercase_file.endswith('.shp'):\n                project_spatial_id = self.create_entry(file_path, file_path)\n\n            elif lowercase_file.endswith(('.kml', '.kmz')):\n                try:\n                    contain_point = False\n                    contain_polygon = False\n                    contain_linestring = False\n                    layers = fiona.listlayers(file_path) \n\n                    # Iterate through layers and check their geometry type\n                    for layer in layers:\n                        with fiona.open(file_path, 'r', driver='LIBKML', layer=layer) as src:\n                            for feat in src:\n                                if contain_point and contain_polygon and contain_linestring:\n                                    break\n\n                                geom_type = feat.geometry.type\n                                if geom_type == 'Point':\n                                    contain_point = True\n                                elif geom_type == 'Polygon':\n                                    contain_polygon = True\n                                elif geom_type == 'LineString':\n                                    contain_linestring = True\n\n                    if contain_point:\n                        project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Points\")\n                    if contain_polygon:\n                        project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Polygons\")\n                    if contain_linestring:\n                        project_spatial_id = self.create_entry(file_path, f\"{file_path}\\Lines\")\n                except Exception as error:\n                    log(self.params.log, Colors.ERROR, f'KML/KMZ file: {file_path} has encountered an error when making a datatracker entry. {error}')\n\n            elif lowercase_file.endswith('.geojson'):\n                project_spatial_id = self.create_entry(file_path, file_path)\n\n            elif lowercase_file.endswith(('.gpkg', '.sqlite')):\n                project_spatial_id = self.create_entry(file_path, file_path, processed=True)\n\n                # Log it\n                log(self.params.log, Colors.WARNING, f'GeoPackage/SQLite file: {file_path} will be added to data tracker but not resulting gdb.', self.params.suppress, ps_script=self.params.ps_script, project_id=project_spatial_id)\n\n            else:                    \n                # Log it\n                log(self.params.log, Colors.WARNING, f'Unsupported Filetype: {file_path} has been found and logged but not added to the datatracker or the geodatabase because it is not implemented or supported.', self.params.suppress, ps_script=self.params.ps_script)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.create_entry","title":"<code>create_entry(absolute_path, feature_path, in_raw_gdb=False, contains_pdf=False, contains_image=False, entry_type='Spatial', processed=False)</code>","text":"<p>Creates a new entry in the data dictionary for spatial data processing.</p> <p>This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes.</p> <p>Parameters:</p> Name Type Description Default <code>absolute_path</code> <code>str</code> <p>The absolute path of the input file.</p> required <code>feature_path</code> <code>str</code> <p>The path to the feature data.</p> required <code>in_raw_gdb</code> <code>bool</code> <p>Indicates if the data is in the raw geodatabase format. Default is False.</p> <code>False</code> <code>contains_pdf</code> <code>bool</code> <p>Indicates if the entry contains a PDF. Default is False.</p> <code>False</code> <code>contains_image</code> <code>bool</code> <p>Indicates if the entry contains an image. Default is False.</p> <code>False</code> <code>entry_type</code> <code>str</code> <p>The type of entry (e.g., 'Spatial'). Default is 'Spatial'.</p> <code>'Spatial'</code> <code>processed</code> <code>bool</code> <p>Indicates if the entry has been processed. Default is False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted project spatial ID.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def create_entry(self, absolute_path: str, feature_path: str, in_raw_gdb: bool = False, contains_pdf: bool = False, contains_image: bool = False, entry_type: str = 'Spatial', processed: bool = False) -&gt; str:\n    \"\"\"\n    Creates a new entry in the data dictionary for spatial data processing.\n\n    This function generates a unique project spatial ID, formats the paths, processes raw data matching, and adds the entry to the data dictionary with the given attributes.\n\n    Args:\n        absolute_path (str): The absolute path of the input file.\n        feature_path (str): The path to the feature data.\n        in_raw_gdb (bool): Indicates if the data is in the raw geodatabase format. Default is False.\n        contains_pdf (bool): Indicates if the entry contains a PDF. Default is False.\n        contains_image (bool): Indicates if the entry contains an image. Default is False.\n        entry_type (str): The type of entry (e.g., 'Spatial'). Default is 'Spatial'.\n        processed (bool): Indicates if the entry has been processed. Default is False.\n\n    Returns:\n        str: The formatted project spatial ID.\n    \"\"\"\n    # Check project numbers and format the result\n    formatted_result = self.check_project_numbers(feature_path)\n    formatted_result = formatted_result.upper()\n\n    # Create a unique identifier for the project spatial ID\n    formatted_project_spatial_id = self.data.create_project_spatial_id(formatted_result)\n\n    # Print file information if debugging is enabled\n    if self.params.debug:\n        log(None, Colors.INFO, feature_path)\n        log(None, Colors.INFO, formatted_project_spatial_id)\n\n    # Convert the raw data path to a relative path           \n    raw_data_path = os.path.relpath(feature_path, self.params.output)\n\n    # Convert the absolute path to the correct drive path format\n    absolute_file_path = convert_drive_path(absolute_path)\n\n    # Call a method to process raw data matching\n    self.call_raw_data_match(formatted_project_spatial_id, absolute_file_path)\n\n    # Add data to the data class \n    self.data.add_data(\n        project_spatial_id=formatted_project_spatial_id,\n        project_number=formatted_result, \n        dropped=False,\n        raw_data_path=raw_data_path, \n        raw_gdb_path=convert_drive_path(self.params.gdb_path),\n        absolute_file_path=absolute_file_path,\n        in_raw_gdb=in_raw_gdb, \n        contains_pdf=contains_pdf, \n        contains_image=contains_image,\n        extracted_attachments_path=None,\n        editor_tracking_enabled=False,\n        processed=processed, \n        entry_type=entry_type\n    )\n\n    return formatted_project_spatial_id\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.del_gdb","title":"<code>del_gdb()</code>","text":"<p>Removes the local Geodatabase and folder after the processing has been completed.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def del_gdb(self) -&gt; None:\n    \"\"\"\n    Removes the local Geodatabase and folder after the processing has been completed.\n    \"\"\"\n    arcpy.Delete_management(self.params.local_gdb_path)\n    arcpy.Delete_management(self.params.local_dir)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.enable_version_control","title":"<code>enable_version_control(feature_class)</code>","text":"<p>Enable the editor tracking version control for a feature class in the Geodatabase.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def enable_version_control(self, feature_class) -&gt; None:\n    \"\"\"\n    Enable the editor tracking version control for a feature class in the Geodatabase.\n    \"\"\"\n    try:\n        # Set the arc environement to the resulting GDB\n        arcpy.env.workspace = self.params.local_gdb_path\n\n        # Add a site id for mapping in a later tool\n        arcpy.management.AddField(\n            feature_class, \n            'bt_site_id',\n            'SHORT'               \n        )\n\n        # Enable the 4 fields for editor tracking\n        arcpy.EnableEditorTracking_management(feature_class, \"bt_created_by\", \"bt_date_created\", \"bt_last_edited_by\", \"bt_date_edited\", \"ADD_FIELDS\", \"UTC\")\n\n        # Set flag in data object for editor tracking to True\n        self.data.set_data(\n            project_spatial_id=os.path.basename(feature_class).replace('proj_', ''),\n            editor_tracking_enabled=True\n        )\n\n    except Exception as error:\n        log(self.params.log, Colors.ERROR, f'An error has been caught while trying to enable editor tracking for {feature_class} in resulting gdb, {error}', ps_script=self.params.ps_script)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.extract_attachments","title":"<code>extract_attachments()</code>","text":"<p>Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def extract_attachments(self) -&gt; None:\n    \"\"\"\n    Call the GeoAttachmentSeeker module function to find, extract and note down any attachments in the result GDB.\n    \"\"\"        \n    # Find and process attachments from the gdb\n    attachment_dict = find_attachments(self.params.local_gdb_path, self.params.attachments)\n\n    # Print file information if debugging is enabled\n    if self.params.debug:\n        log(None, Colors.INFO, attachment_dict)\n\n    # Iterating through key-value pairs using items()\n    for key, value in attachment_dict.items():\n        # Update the entry of the Geodatabase feature class\n        self.data.set_data(\n            project_spatial_id=key.replace('proj_', ''), \n            # extracted_attachments_path=value\n            extracted_attachments_path=os.path.join(self.params.gdb_path, value.replace(f\"C:\\LocalTwoBillionToolkit\\\\\", ''))\n        )\n\n    # Log completion of this task\n    log(None, Colors.INFO, 'All attachments have been extracted from the result Geodatabase.')\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.Processor.process_entries","title":"<code>process_entries()</code>","text":"<p>Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking.</p> <p>The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def process_entries(self) -&gt; None:\n    \"\"\"\n    Processes spatial data entries from a dictionary, converts them into a geodatabase format, and enables version control and editor tracking.\n\n    The function iterates over the entries in the data dictionary, checks their file types, converts them to a geodatabase feature class, and updates their processing status.\n    \"\"\"\n    # Iterate over each entry in the data dictionary\n    for index, entry in enumerate(self.data.data_dict):\n        # Get the absolute path of the entry file\n        entry_absolute_path:str = self.data.data_dict[entry].get('absolute_file_path')\n        try:\n            # Check if the current entry has already been processed\n            if self.data.data_dict[entry].get('processed'):\n                continue\n\n            # Check if the created_by date of the entry is within the same day as now            \n            created_at = self.data.data_dict[entry].get('created_at')\n            if self.params.load_from == 'database' and created_at:\n                now = datetime.datetime.now()\n                if created_at.date() != now.date():\n                    continue\n\n            # Define the name of the geodatabase entry and build path for the feature class in the local geodatabase \n            gdb_entry_name = f\"proj_{entry}\"\n            feature_gdb_path = os.path.join(self.params.local_gdb_path, gdb_entry_name)\n\n            # Check the file type and export features accordingly\n            if entry_absolute_path.endswith('.gdb'):\n                # Export features from one geodatabase to the output geodatabase\n                arcpy.conversion.ExportFeatures(\n                    os.path.join(self.params.output, self.data.data_dict[entry].get('raw_data_path')),\n                    feature_gdb_path\n                )\n\n            elif entry_absolute_path.endswith('.shp'):\n                # Export features from shapefile to the output geodatabase\n                arcpy.conversion.ExportFeatures(\n                    entry_absolute_path,\n                    feature_gdb_path\n                )\n\n            elif entry_absolute_path.endswith(('.kml', '.kmz')):\n                # Initialize an empty GeoDataFrame\n                data = gpd.GeoDataFrame()\n\n                # List all layers in the KML/KMZ file\n                layers = fiona.listlayers(entry_absolute_path)\n                for layer in layers:\n                    # Read each layer into a temporary GeoDataFrame\n                    layer_gdb = gpd.read_file(entry_absolute_path, driver='LIBKML', layer=layer)\n\n                    # Check if the temporary GeoDataFrame is empty before concatenating\n                    if not layer_gdb.empty:\n                        data = pd.concat([data, layer_gdb], ignore_index=True)\n\n                # Rename the 'OBJECTID' column if it exists\n                if 'OBJECTID' in data.columns:\n                    data.rename(columns={'OBJECTID': 'OBJECTID_STRING'}, inplace=True)\n\n                # Convert all datetime columns to string with specified utc=True\n                for col in data.select_dtypes(include=['datetime64[ns]', 'datetime64[ns, UTC]']).columns:\n                    data[col] = pd.to_datetime(data[col], utc=True, errors='coerce').dt.strftime('%Y-%m-%d')\n\n                # Check if 'timestamp' column exists\n                if 'timestamp' in data.columns:\n                    data['timestamp'] = pd.to_datetime(data['timestamp'], utc=True, errors='coerce').dt.strftime('%Y-%m-%d')\n\n                # Determine the path for raw data and entry basename\n                raw_data_path = self.data.data_dict[entry].get('raw_data_path')\n                entry_data_basename = os.path.basename(raw_data_path)\n\n                # Export the GeoDataFrame to the appropriate feature class based on geometry type\n                if entry_data_basename == 'Points':\n                    points_gdf = data[data.geometry.type == 'Point']\n                    points_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)\n                elif entry_data_basename == 'Polygons':\n                    polygons_gdf = data[data.geometry.type == 'Polygon']\n                    polygons_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)\n                elif entry_data_basename == 'Lines':\n                    lines_gdf = data[data.geometry.type == 'LineString']\n                    lines_gdf.to_file(self.params.local_gdb_path, driver='OpenFileGDB', layer=gdb_entry_name)     \n\n            elif entry_absolute_path.endswith('.geojson'):\n                # Export features from GeoJSON to the output geodatabase\n                arcpy.conversion.JSONToFeatures(\n                    entry_absolute_path,\n                    feature_gdb_path\n                )\n\n            # Update the entry status to indicate it has been processed and exists in raw geodatabase format\n            self.data.set_data(\n                project_spatial_id=entry, \n                in_raw_gdb=True,\n                processed=True\n            )\n\n            # Enable the version control of the layer in the geodatabase\n            self.enable_version_control(feature_gdb_path)\n\n            # Update the entry status to indicate that editor tracking is enabled\n            self.data.set_data(\n                project_spatial_id=entry, \n                editor_tracking_enabled=True\n            )\n\n        except (arcpy.ExecuteError, arcgisscripting.ExecuteError) as error:\n            log(self.params.log, Colors.ERROR, f'An error occurred when processing the layer for {entry_absolute_path}, you can fix or remove it from the datatracker/database, then run the command again with --resume\\n{error}', ps_script=self.params.ps_script, project_id=entry)\n            # Can remove the comment from below when being shipped so the tool stops when a excetption is caught instead of continue on\n            # raise Exception(error) \n        except Exception as error:\n            log(self.params.log, Colors.ERROR, f'An uncaught error occurred when processing the layer for {entry_absolute_path}', ps_script=self.params.ps_script, project_id=entry)\n            raise Exception(error)\n</code></pre>"},{"location":"pages/SpatialTransformer/Processor/#twobilliontoolkit.SpatialTransformer.Processor.convert_drive_path","title":"<code>convert_drive_path(file_path)</code>","text":"<p>Converts a path with a mapped drive (ie. M:\\, V:) to the actual network drive name.</p> Source code in <code>twobilliontoolkit/SpatialTransformer/Processor.py</code> <pre><code>def convert_drive_path(file_path):\n    \"\"\"\n    Converts a path with a mapped drive (ie. M:\\, V:\\) to the actual network drive name.\n    \"\"\"\n    abs_file_path = os.path.abspath(file_path)\n    actual_drive_path = abs_file_path\n\n    try:\n        # Extract drive letter from absolute file path\n        drive_letter, _ = os.path.splitdrive(abs_file_path)\n\n        # Check if the path contains a drive letter\n        if re.match(r\"[A-Za-z]{1}:{1}\", drive_letter):\n            # Convert mapped drive path to UNC path\n            actual_drive_path = win32wnet.WNetGetUniversalName(actual_drive_path, 1)\n    except Exception as e:\n        print(f\"Error: {e}\")\n\n    return actual_drive_path\n</code></pre>"},{"location":"pages/SpatialTransformer/SpatialTransformer/","title":"SpatialTransformer","text":"<p>File: twobilliontoolkit/SpatialTransformer/spatial_transformer.py Created By:       Anthony Rodway Email:            anthony.rodway@nrcan-rncan.gc.ca Creation Date:    Wed November 15 10:30:00 PST 2023 Organization:     Natural Resources of Canada Team:             Carbon Accounting Team</p> Description <p>The spatial_transformer.py script is a Python tool for processing spatial data. It handles tasks like geodatabase creation, file validation, and checking project numbers against a master data sheet. </p> Usage <p>python path/to/spatial_transformer.py [-h] --input_path input_path --output_path output_path --gdb_path gdb_path --master master_data_path --load {datatracker,database} --save {datatracker,database} --datatracker datatracker_path --debug [--resume]</p>"},{"location":"pages/SpatialTransformer/SpatialTransformer/#twobilliontoolkit.SpatialTransformer.spatial_transformer.main","title":"<code>main()</code>","text":"<p>The main function of the spatial_transformer.py script</p> Source code in <code>twobilliontoolkit/SpatialTransformer/spatial_transformer.py</code> <pre><code>def main():\n    \"\"\" The main function of the spatial_transformer.py script \"\"\"\n    # Get the start time of the script\n    start_time = time.time()\n    log(None, Colors.INFO, f'Tool is starting... Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n    # Initialize the argument parse\n    parser = argparse.ArgumentParser(description='Spatial Transformer Tool')\n\n    # Define command-line arguments\n    parser.add_argument('--input_path', required=True, help='Directory or Compressed file location that will be handed to Ripple Unzipple.')\n    parser.add_argument('--output_path', required=True, help='Where the final output of Ripple Unzipple will extract to.')\n    parser.add_argument('--load', choices=['datatracker', 'database'], required=True, default='database', help='Specify what to load from (datatracker or database).')\n    parser.add_argument('--save', choices=['datatracker', 'database'], required=True, default='database', help='Specify what to save to (datatracker or database).')\n    parser.add_argument('--gdb_path', required=True, default='', help='Path of where the geodatabase will be saved, if it does not already exist, it will be created.')\n    parser.add_argument('--datatracker', default='', help='Name of the datatracker file that will be saved adjacent to the geodatabase if provided.')\n    parser.add_argument('--attachments', default='', help='Name of the attachments folder that will be saved adjacent to the geodatabase.')\n    parser.add_argument('--master', default='', help='The location of the master aspatial datasheet.')\n    parser.add_argument('--ini', default='', help='Path to the database initilization file. If not provided, then it will use the one provided in the repository.')\n    parser.add_argument('--debug', action='store_true', default=False, help='Enable debug mode.')\n    parser.add_argument('--resume', action='store_true', default=False, help='Resume from where a crash happened.')\n    parser.add_argument('--suppress', action='store_true', default=False, help='Suppress Warnings in the command-line and only show Errors.')\n    parser.add_argument('--ps_script', default='', help='The location of the script to run commands if used.')\n\n    # Parse the command-line arguments\n    args = parser.parse_args()\n\n    # Call the entry function\n    spatial_transformer(args.input_path, args.output_path, args.load, args.save, args.gdb_path, args.datatracker, args.attachments, args.master, args.ini, args.debug, args.resume, args.suppress, args.ps_script)\n\n    # Get the end time of the script and calculate the elapsed time\n    end_time = time.time()\n    log(None, Colors.INFO, f'Tool has completed. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n    log(None, Colors.INFO, f'Elapsed time: {end_time - start_time:.2f} seconds')\n</code></pre>"},{"location":"pages/SpatialTransformer/SpatialTransformer/#twobilliontoolkit.SpatialTransformer.spatial_transformer.spatial_transformer","title":"<code>spatial_transformer(input_path, output_path, load_from, save_to, gdb_path, datatracker, attachments, master_data_path, database_config=None, debug=False, resume=False, suppress=False, ps_script=None)</code>","text":"<p>The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input directory or compressed file.</p> required <code>output_path</code> <code>str</code> <p>Path to output data of Ripple Unzipple.</p> required <code>load_from</code> <code>str</code> <p>Either 'database' or 'datatracker' to determine what to load the data from.</p> required <code>save_to</code> <code>str</code> <p>Either 'database' or 'datatracker' to determine what to save the data to.</p> required <code>gdb_path</code> <code>str</code> <p>Path to the Geodatabase.</p> required <code>datatracker</code> <code>str</code> <p>Datatracker file name.</p> required <code>attachments</code> <code>str</code> <p>Attachment folder name.</p> required <code>master_data_path</code> <code>str</code> <p>Path to the aspatial master data.</p> required <code>debug</code> <code>bool</code> <p>Determines if the program is in debug mode. Defaults False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Determines if the program should resume from where a crash happened. Defaults False.</p> <code>False</code> <code>suppress</code> <code>bool</code> <p>Determines if the program will suppress Warning Messages to the command line while running.</p> <code>False</code> <code>ps_script</code> <code>str</code> <p>The path location of the script to run spatial transformer.</p> <code>None</code> Source code in <code>twobilliontoolkit/SpatialTransformer/spatial_transformer.py</code> <pre><code>def spatial_transformer(input_path: str, output_path: str, load_from: str, save_to: str, gdb_path: str, datatracker: str, attachments: str, master_data_path: str, database_config: str = None, debug: bool = False, resume: bool = False, suppress: bool = False, ps_script: str = None) -&gt; None:\n    \"\"\"\n    The spatial_transformer function serves as the main entry point for the spatial transformation script. Its primary purpose is to handle various tasks related to spatial data processing, such as starting the ripple_unzipple tool and geodatabase creation.\n\n    Args:\n        input_path (str): Path to the input directory or compressed file.\n        output_path (str): Path to output data of Ripple Unzipple.\n        load_from (str): Either 'database' or 'datatracker' to determine what to load the data from.\n        save_to (str): Either 'database' or 'datatracker' to determine what to save the data to.\n        gdb_path (str): Path to the Geodatabase.\n        datatracker (str): Datatracker file name.\n        attachments (str): Attachment folder name.\n        master_data_path (str): Path to the aspatial master data.\n        debug (bool, optional): Determines if the program is in debug mode. Defaults False.\n        resume (bool, optional): Determines if the program should resume from where a crash happened. Defaults False.\n        suppress (bool, optional): Determines if the program will suppress Warning Messages to the command line while running.\n        ps_script (str, optional): The path location of the script to run spatial transformer.\n    \"\"\"\n    # Create the logfile path\n    log_file = os.path.basename(gdb_path).replace('.gdb', f\"_Log_{datetime.datetime.now().strftime('%Y-%m-%d')}.txt\")\n\n    # Initialize a variable for the processor in case an error occurs beforehand\n    spatial_processor = None\n\n    try:       \n        # Initialize Parameters class\n        setup_parameters = Parameters(input_path, output_path, gdb_path, master_data_path, datatracker, attachments, load_from, save_to, database_config, log_file, debug, resume, suppress, ps_script)\n\n        # Start the unzip tool \n        setup_parameters.handle_unzip()\n        log(None, Colors.INFO, f'Ripple Unzipple has completed extracted the files. Now starting to create the datatracker entries from the files. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Create the GDB\n        setup_parameters.create_gdb()\n\n        # Initialize the SpatialData class\n        spatial_processor = Processor(setup_parameters)\n\n        # Search for any spatial data and create an entry in the datatracker for each one\n        spatial_processor.create_datatracker_entries()\n        log(None, Colors.INFO, f'All entries have been created in the datatracker for the aspatial and spatial files. Now starting to process those found spatial files. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Go through the dictionary of entries and process them into the output geodatabase\n        spatial_processor.process_entries()\n        log(None, Colors.INFO, f'The Processor has completed processing the files into the Geodatabase. Now starting to extract attachments from the Geodatabase. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Extract attachments from the Geodatabase\n        spatial_processor.extract_attachments()\n        log(None, Colors.INFO, f'The Attachments Seeker has completed extracting the attachments from the geodatabase. Now starting to transfer over the files from the local directory to the specified output. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Move the local files to the specified output\n        success = transfer(\n            local_path=spatial_processor.params.local_dir,\n            network_path=os.path.dirname(spatial_processor.params.gdb_path),\n            list_files=[os.path.basename(spatial_processor.params.gdb_path), os.path.basename(spatial_processor.params.datatracker), os.path.basename(spatial_processor.params.attachments), spatial_processor.params.log[:-4] + '_WARNING.txt', spatial_processor.params.log[:-4] + '_ERROR.txt'],\n            datatracker_2bt=spatial_processor.data,\n            log_path=spatial_processor.params.log\n        )\n        log(None, Colors.INFO, f'The Network Transfer has completed moving the files from local to the network. Now saving the data. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Save the data tracker before returning\n        spatial_processor.data.save_data(True if resume else False)\n        log(None, Colors.INFO, f'The changes have successfully been saved to the specified datatracker. Now opening Record Reviser. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n        # Open the record reviser\n        filter = {'created_at': datetime.datetime.now()}\n        call_record_reviser(spatial_processor.data, spatial_processor.params.gdb_path, filter)\n        log(None, Colors.INFO, 'The Record Reviser has completed editing any entries and is closing.')\n\n        if not debug and success:\n            # Remove the local contents\n            spatial_processor.del_gdb()\n            os.mkdir(setup_parameters.local_dir)\n            log(None, Colors.INFO, f'Removing contents from the local directory completed. Time: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n\n    except (ValueError, Exception) as error:        \n        # Log the error\n        log(log_file, Colors.ERROR, traceback.format_exc(), ps_script=ps_script)\n\n        # Save the data to the datatracker in case of crashing\n        if spatial_processor:\n            spatial_processor.data.save_data(True if resume else False)\n            log(None, Colors.INFO, 'A checkpoint has been made at the point of failure.')\n\n        exit(1)\n</code></pre>"}]}